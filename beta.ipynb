{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e95def91",
   "metadata": {},
   "source": [
    "<h1><center>Projeto 1\n",
    "\n",
    "\n",
    "MC934B/MO436A\n",
    "\n",
    "2s 2023\n",
    "                 \n",
    "Reinforcement Learning  \n",
    "\n",
    "Israel Silva | Jorge Frasson | Juliano Soares | Mario Costa\n",
    "</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Especificação dos problemas</center></h1>\n",
    "<p align=\"justify\"> As técnicas de aprendizado de máquina estão cada vez mais em uso nos dias de hoje, tais técnicas vem corroborando com a evolução de diversas áreas como: robótica, medicina, engenharia, entre outras; Podemos citar um exemplo na medicina atráves do reconhecimento de cancer de pele por meio de técnicas de aprendizado de máquina. Dentro da área de aprendizado uma área que vem sendo utilizada é a técnica de aprendizado de máquina por reforço. Que é o foco do nosso estou para esta atividade.</p>\n",
    "<p align=\"justify\">A técnica de aprendizado por reforço consiste no treinamento de modelos de aprendizado de máquina que poderiam parecer com treinamento supervisionado -- ser humano interage informando se a decisão tomada está certa ou errada; Porém o que faz a máquina decidir é como você programa as recompensas que ela irá ganhar em cada acerto ou erro. Esse modelo faz com que a máquina tome a sua própria decisão tentando maximizar os ganhos.</p>\n",
    "<p align=\"justify\">Este modelo tem sido utilizado para ambientes não determinísticos em que exite uma alta complexidade nas ações tomadas, como por exemplo, a robótica, que possui um grau de extrema dificuldade em determinar o resultado de um único passo de um robo, quem dirá prever todas as ações externas que ele pode estar sofrendo como por exemplo uma ventania.   </p>\n",
    "<p align=\"justify\">Para aplicar as técnicas propostas pelo trabalho, criamos um desafio que consiste em, um ambiente com um espaço limitado a X por Y (pode ser configurado), que permite a inserção de barreiras que não podem ser transpostas. O ambiente também vai possuir uma origem e um destino, esta distância (origem/destino). Este traçado deverá ser percorrido por um determinado bloco ,que vamos chamar de Bob, que deverá aprender o caminho, porém o Bob deverá percorrer de maneira deterministica que implica em um treinamento e execução no mesmo ambiente, assim como, no estocástico, treinamento em um ambiente mas execução em um ambiente desconhecido. Desta forma a melhor política irá levar Bob para casa no menor número de passos.</p>\n",
    "<p align=\"justify\">O trabalho proposto irá explorar 3 técnicas: Monte Carlo, Qlearning, Sarsa e montar uma avaliação entre as técnicas utilizadas tendo como base de descoberta o ambiente descrito acima.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1d68aa",
   "metadata": {},
   "source": [
    "<h1><center>Entrada de dados</center></h1>\n",
    "<p align=\"justify\">A solução apresentada para executar as atividades propostas pelo trabalho foram: Python e algumas bibliotécas apresentadas na importação das bibliotécas. O sistema está arquitetado de forma que para execução basta que o usuário apenas siga a sequência execução das funções propostas.</p>   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b40ec44",
   "metadata": {},
   "source": [
    "## 1) Código comum para todos os algorítmos de RL utilizados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b91f02",
   "metadata": {},
   "source": [
    "### 1.1) Instalação das Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gymnasium\n",
    "# !pip install numpy\n",
    "# !pip install pygame\n",
    "# !pip install random\n",
    "# !pip install tqdm\n",
    "# !pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2) Importação das Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "934042a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import pygame\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys\n",
    "import math\n",
    "import pandas as pd\n",
    "import os\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda373c8",
   "metadata": {},
   "source": [
    "### 1.3) Ambiente gráfico do labirinto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "12d95224",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MazeGameEnv(gym.Env):\n",
    "    def __init__(self, maze):\n",
    "        super(MazeGameEnv, self).__init__()\n",
    "        self.maze = np.array(maze)\n",
    "        self.start_pos = tuple(np.argwhere(self.maze == 'S')[0])\n",
    "        self.goal_pos = tuple(np.argwhere(self.maze == 'G')[0])\n",
    "        self.current_pos = self.start_pos\n",
    "        self.num_rows, self.num_cols = self.maze.shape\n",
    "\n",
    "        # Define ação como Discrete com 4 ações (cima, baixo, esquerda, direita)\n",
    "        self.action_space = gym.spaces.Discrete(4)\n",
    "\n",
    "        # Defina o espaço de observação como uma tupla com o número de linhas e colunas\n",
    "        self.observation_space = gym.spaces.Tuple((gym.spaces.Discrete(self.num_rows), gym.spaces.Discrete(self.num_cols)))\n",
    "    \n",
    "        # Define atraso de renderização (em ms)\n",
    "        self.delay = 0\n",
    "\n",
    "        # Defina cores\n",
    "        self.WHITE = (255, 255, 255)\n",
    "        self.GREEN = (0, 255, 0)\n",
    "        self.RED = (255, 0, 0)\n",
    "        self.BLACK = (0, 0, 0)\n",
    "        self.PURPLE = (255, 0, 255)\n",
    "        self.BLUE = (0, 0, 255)\n",
    "        self.GOLD = (255, 215, 0)\n",
    "        self.GRAY = (128, 128, 128)\n",
    "\n",
    "    def InitializePygame(self):\n",
    "        # Inicialize o ambiente Pygame\n",
    "        pygame.init()\n",
    "\n",
    "        # Defina o tamanho da célula e a largura e altura da tela\n",
    "        self.cell_size = 50\n",
    "        self.screen_width = self.num_cols * self.cell_size\n",
    "        self.screen_height = self.num_rows * self.cell_size\n",
    "        self.screen = pygame.display.set_mode((self.screen_width, self.screen_height))\n",
    "\n",
    "    def reset(self, num_obstacles = 0, fixed_start_pos = None):\n",
    "        self.reset_obstacles(num_obstacles)\n",
    "        if fixed_start_pos is not None:\n",
    "            row, col = fixed_start_pos\n",
    "        else:\n",
    "            row, col = np.random.randint(0, self.num_rows), np.random.randint(0, self.num_cols)\n",
    "            while self.maze[row, col] in ['#', 'G']:  # Garante que a posição inicial não é um obstáculo ou a posição final\n",
    "                row, col = np.random.randint(0, self.num_rows), np.random.randint(0, self.num_cols)\n",
    "        self.maze[self.start_pos] = '.'  # Limpa a posição inicial antiga\n",
    "        self.start_pos = (row, col)\n",
    "        self.maze[row, col] = 'S'  # Define a nova posição inicial\n",
    "        self.current_pos = self.start_pos\n",
    "        return self.current_pos\n",
    "\n",
    "    def reset_obstacles(self, num_obstacles = 0):\n",
    "        for row in range(self.num_rows):\n",
    "            for col in range(self.num_cols):\n",
    "                if self.maze[row, col] == 'R':\n",
    "                    self.maze[row, col] = '.'\n",
    "\n",
    "        for _ in range(num_obstacles):\n",
    "            row, col = np.random.randint(0, self.num_rows), np.random.randint(0, self.num_cols)\n",
    "            while self.maze[row, col] in ['S', 'G', '#','R']:\n",
    "                row, col = np.random.randint(0, self.num_rows), np.random.randint(0, self.num_cols)\n",
    "            self.maze[row, col] = 'R'\n",
    "\n",
    "    def step(self, action, reward = 1):\n",
    "        old_pos = self.current_pos\n",
    "                \n",
    "        if action == 0:  # Cima\n",
    "            self.move('up')\n",
    "        elif action == 1:  # Baixo\n",
    "            self.move('down')\n",
    "        elif action == 2:  # Esquerda\n",
    "            self.move('left')\n",
    "        elif action == 3:  # Direita\n",
    "            self.move('right')\n",
    "\n",
    "        obs = self.current_pos\n",
    "        #\n",
    "        # reward = 1 if self.current_pos == self.goal_pos else 0\n",
    "        # distance_to_goal = abs(self.current_pos[0] - self.goal_pos[0]) + abs(self.current_pos[1] - self.goal_pos[1])\n",
    "        # reward = 1 / (distance_to_goal + 1)\n",
    "        # done = self.current_pos == self.goal_pos\n",
    "        #\n",
    "        # max_distance = self.num_rows + self.num_cols - 2\n",
    "        # if self.current_pos == old_pos:\n",
    "        #     reward = -max_distance  # Penalidade por ação inválida\n",
    "        #     done = False\n",
    "        # else:\n",
    "        #     if self.current_pos == self.goal_pos:\n",
    "        #         reward = max_distance\n",
    "        #         done = True\n",
    "        #     else:\n",
    "        #         distance_to_goal = abs(self.current_pos[0] - self.goal_pos[0]) + abs(self.current_pos[1] - self.goal_pos[1])\n",
    "        #         reward = -1 + (1 - distance_to_goal / max_distance)\n",
    "        #         # reward = -distance_to_goal\n",
    "        #         done = False\n",
    "\n",
    "        new_pos = self.current_pos\n",
    "\n",
    "        # Recompensas e penalidades\n",
    "        if new_pos == self.goal_pos:\n",
    "            reward = 100  # Recompensa por atingir o objetivo\n",
    "            done = True\n",
    "        elif new_pos == old_pos:\n",
    "            reward = -10  # Penalidade por bater em um obstáculo ou parede\n",
    "            done = False\n",
    "        else:\n",
    "            reward = -1  # Penalidade por cada movimento\n",
    "            done = False\n",
    "\n",
    "        info = {}\n",
    "\n",
    "        return new_pos, reward, done, info\n",
    "\n",
    "    def move(self, action):\n",
    "        new_pos = list(self.current_pos)\n",
    "\n",
    "        if action == 'up':\n",
    "            new_pos[0] -= 1\n",
    "        elif action == 'down':\n",
    "            new_pos[0] += 1\n",
    "        elif action == 'left':\n",
    "            new_pos[1] -= 1\n",
    "        elif action == 'right':\n",
    "            new_pos[1] += 1\n",
    "\n",
    "        new_pos = tuple(new_pos)\n",
    "\n",
    "        if self.is_valid_position(new_pos[0], new_pos[1]):\n",
    "            self.current_pos = new_pos\n",
    "\n",
    "    def is_valid_position(self, row, col):\n",
    "        return 0 <= row < self.num_rows and 0 <= col < self.num_cols and self.maze[row, col] != '#' and self.maze[row, col] != 'R'\n",
    "\n",
    "\n",
    "    def render(self, path=None):\n",
    "        self.screen.fill(self.WHITE)\n",
    "        self.draw_maze(path)\n",
    "        pygame.display.update()\n",
    "        if self.delay > 0:\n",
    "            pygame.time.wait(self.delay)\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "            elif event.type == pygame.KEYDOWN:\n",
    "                if event.key == pygame.K_ESCAPE:\n",
    "                    pygame.quit()\n",
    "                    sys.exit()\n",
    "\n",
    "    def play_path(self, path, delay=0.5):\n",
    "        for position in path:\n",
    "            self.current_pos = position\n",
    "            self.render(path = path)\n",
    "            time.sleep(delay)\n",
    "            \n",
    "    def draw_star(self, surface, color, center, points, outer_radius, inner_radius):\n",
    "        def get_point(index):\n",
    "            angle_offset = math.pi / 2  # Ajuste de orientação\n",
    "            radius = outer_radius if index % 2 == 0 else inner_radius\n",
    "            angle = index * (2 * math.pi / (2 * points)) - angle_offset\n",
    "            return center[0] + radius * math.sin(angle), center[1] - radius * math.cos(angle)\n",
    "\n",
    "        star_points = [get_point(i) for i in range(2 * points)]\n",
    "        pygame.draw.polygon(surface, color, star_points)\n",
    "\n",
    "    def draw_maze(self, path = None):\n",
    "        for row in range(self.num_rows):\n",
    "            for col in range(self.num_cols):\n",
    "                cell_left = col * self.cell_size\n",
    "                cell_top = row * self.cell_size\n",
    "\n",
    "                # Desenha o quadrado branco com borda preta\n",
    "                pygame.draw.rect(self.screen, self.BLACK, (cell_left, cell_top, self.cell_size, self.cell_size), 1)\n",
    "\n",
    "                if self.maze[row, col] == '#' or self.maze[row, col] == 'R':\n",
    "                    pygame.draw.rect(self.screen, self.BLACK, (cell_left, cell_top, self.cell_size, self.cell_size))\n",
    "                elif self.maze[row, col] == 'S':\n",
    "                    pygame.draw.circle(self.screen, self.GREEN, (cell_left + self.cell_size // 2, cell_top + self.cell_size // 2), self.cell_size // 2 - 5)\n",
    "                elif self.maze[row, col] == 'G':\n",
    "                    self.draw_star(self.screen, self.GOLD, (cell_left + self.cell_size // 2, cell_top + self.cell_size // 2), 5, self.cell_size // 2, self.cell_size // 4)\n",
    "\n",
    "        if path:\n",
    "            for i in range(len(path)-1):\n",
    "                start = path[i]\n",
    "                end = path[i+1]\n",
    "                self.draw_line(start, end)\n",
    "\n",
    "        current_row, current_col = self.current_pos\n",
    "        pygame.draw.rect(self.screen, self.PURPLE, (current_col * self.cell_size + 5, current_row * self.cell_size + 5, self.cell_size - 10, self.cell_size - 10))\n",
    "\n",
    "    def draw_line(self, start, end):\n",
    "        start_x = start[1] * self.cell_size + self.cell_size // 2\n",
    "        start_y = start[0] * self.cell_size + self.cell_size // 2\n",
    "        end_x = end[1] * self.cell_size + self.cell_size // 2\n",
    "        end_y = end[0] * self.cell_size + self.cell_size // 2\n",
    "\n",
    "        pygame.draw.line(self.screen, self.BLUE, (start_x, start_y), (end_x, end_y), 2)\n",
    "\n",
    "    def draw_arrows(self, Q):\n",
    "        arrow_image = pygame.image.load('assets/arrow.png')  # Carrega a imagem da seta\n",
    "        arrow_image = pygame.transform.scale(arrow_image, (self.cell_size // 2, self.cell_size // 2))  # Redimensiona para caber na célula\n",
    "\n",
    "        for row in range(self.num_rows):\n",
    "            for col in range(self.num_cols):\n",
    "                state = (row, col)\n",
    "                if self.maze[row][col] in ['.', 'R', 'S']:  # Não desenhar em '#', 'S' ou 'G'\n",
    "                    best_action = max(Q[state], key=Q[state].get)\n",
    "                    center = (col * self.cell_size + (self.cell_size // 2 // 2), row * self.cell_size + (self.cell_size // 2 // 2))\n",
    "\n",
    "                    if best_action == 0:  # Cima\n",
    "                        rotated_image = pygame.transform.rotate(arrow_image, 90)\n",
    "                    elif best_action == 1:  # Baixo\n",
    "                        rotated_image = pygame.transform.rotate(arrow_image, -90)\n",
    "                    elif best_action == 2:  # Esquerda\n",
    "                        rotated_image = pygame.transform.rotate(arrow_image, 180)\n",
    "                    elif best_action == 3:  # Direita\n",
    "                        rotated_image = arrow_image  # Não precisa rotacionar\n",
    "\n",
    "                    self.screen.blit(rotated_image, center)  # Desenha a imagem rotacionada\n",
    "\n",
    "    def close(self):\n",
    "        pygame.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328ec43e",
   "metadata": {},
   "source": [
    "### 1.5) Função para criação do espaço labiríntico a ser analisado\n",
    "\n",
    "Obs: Execute apenas o labirinto que deseja analisar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b230b97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mazes = [\n",
    "    [\n",
    "    ['S', '.', '.', '.', '.', '.', '.', '.', '.', '.'],\n",
    "    ['.', '.', '.', '.', '#', '.', '#', '.', '.', '.'],\n",
    "    ['.', '.', '.', '.', '#', '.', '.', '.', '.', '.'],\n",
    "    ['.', '.', '.', '.', '#', '.', '.', '.', '#', '.'],\n",
    "    ['.', '.', '.', '.', '#', '.', '.', '.', '.', '.'],\n",
    "    ['.', '#', '#', '#', '#', '.', '.', '.', '.', '.'],\n",
    "    ['.', '.', '.', '.', '.', '.', '.', '.', '.', '.'],\n",
    "    ['.', '.', '.', '.', '.', '#', '.', '.', '.', '.'],\n",
    "    ['.', '.', '.', '.', '.', '.', '.', '.', '.', '.'],\n",
    "    ['.', '#', '.', '.', '.', '.', '.', '.', '.', 'G'],\n",
    "    ],\n",
    "    [\n",
    "    ['S','.','.','.','.','.'],\n",
    "    ['.','.','.','#','.','.'],\n",
    "    ['.','.','.','#','.','.'],\n",
    "    ['.','.','.','#','.','.'],\n",
    "    ['.','#','#','#','.','.'],\n",
    "    ['.','.','.','.','.','G']\n",
    "    ],\n",
    "    [\n",
    "    ['S','.','#','.','#','.'],\n",
    "    ['.','.','.','.','#','.'],\n",
    "    ['.','#','#','.','#','.'],\n",
    "    ['.','#','#','.','#','.'],\n",
    "    ['.','.','.','.','#','.'],\n",
    "    ['#','.','.','.','.','G']\n",
    "    ],\n",
    "    [\n",
    "    ['S','.','.','.','.','.'],\n",
    "    ['.','.','.','.','.','.'],\n",
    "    ['.','.','#','#','.','.'],\n",
    "    ['.','.','#','#','.','.'],\n",
    "    ['.','.','.','.','.','.'],\n",
    "    ['.','.','.','.','.','G']\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6) Funções Utilitarías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(episodes_data):\n",
    "    rewards = [info['reward'] for episode, info in episodes_data.items()]\n",
    "    steps = [info['steps'] for episode, info in episodes_data.items()]\n",
    "    return rewards, steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dea4fe",
   "metadata": {},
   "source": [
    "## 2) Função Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7dd9f9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonteCarloAgent:\n",
    "    def __init__(self, env, alpha = 0.05, gamma=0.6, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01, random_initialization=True):\n",
    "        self.env = env\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.Q = {}\n",
    "        self.returns = {}\n",
    "        self.success_count = 0\n",
    "        self.variable_alpha_values = []\n",
    "        self.exploration_rate = []  # Inicializa a lista para armazenar a taxa de exploração por episódio\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.random_initialization = random_initialization\n",
    "        self.state_visits = {}\n",
    "        self.initialize_Q_and_returns()\n",
    "\n",
    "    def initialize_Q_and_returns(self):\n",
    "        for row in range(self.env.num_rows):\n",
    "            for col in range(self.env.num_cols):\n",
    "                self.Q[(row, col)] = {}\n",
    "                self.returns[(row, col)] = {}\n",
    "                self.state_visits[(row, col)] = 0  # Inicializa as visitas como zero para cada estado\n",
    "                for a in range(self.env.action_space.n):\n",
    "                    if self.random_initialization:\n",
    "                        self.Q[(row, col)][a] = np.random.uniform(low=-1.0, high=1.0)\n",
    "                    else:\n",
    "                        self.Q[(row, col)][a] = 0\n",
    "                    self.returns[(row, col)][a] = []\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.uniform(0, 1) < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            return max(self.Q[state], key=self.Q[state].get)\n",
    "\n",
    "    \n",
    "    #MC padrao sem alpha (taxa de aprendizado)\n",
    "    #-----------------------------------------\n",
    "    #def learn(self, episode):\n",
    "    #    G = 0\n",
    "    #    for state, action, reward in reversed(episode):\n",
    "    #        G = self.gamma * G + reward\n",
    "    #        \n",
    "    #        if state is not None and action is not None:\n",
    "    #            self.returns[state][action].append(G)\n",
    "    #            self.Q[state][action] = np.mean(self.returns[state][action])\n",
    "    #-------------------------------------------------------------------------            \n",
    "    \n",
    "    #MC com Alpha Variável conforme pedido no Project 1\n",
    "    #--------------------------------------------------\n",
    "    def learn(self, episode):\n",
    "        G = 0\n",
    "        # Dicionário para armazenar os valores de alpha para cada estado-ação\n",
    "        alphas = {}\n",
    "\n",
    "        # Primeiro, calcule os valores de alpha para cada estado-ação na sequência\n",
    "        for state, action, _ in episode:\n",
    "            if (state, action) not in alphas:\n",
    "                N_st_at = len(self.returns[state][action])\n",
    "                alphas[(state, action)] = 1.0 / N_st_at if N_st_at > 0 else 0.0\n",
    "\n",
    "        # Agora, percorra a sequência novamente para atualizar Q-values\n",
    "        for state, action, reward in reversed(episode):\n",
    "            G = self.gamma * G + reward\n",
    "            if state is not None and action is not None:\n",
    "                self.returns[state][action].append(G)\n",
    "\n",
    "                # Use o valor de alpha pré-calculado\n",
    "                alpha = alphas[(state, action)]\n",
    "\n",
    "                # Atualizar o valor Q utilizando a fórmula com alpha variável\n",
    "                current_Q = self.Q[state][action]\n",
    "                updated_Q = current_Q + alpha * (G - current_Q)\n",
    "                self.Q[state][action] = updated_Q\n",
    "    \n",
    "    #--------------------------------------------------\n",
    "    #Fim do MC com Alpha Variável            \n",
    "                \n",
    "    def update_epsilon(self):\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon_decay * self.epsilon)\n",
    "\n",
    "    def train(self, episodes, num_obstacles=0, render=False, max_steps=100):\n",
    "        if render:\n",
    "            self.env.InitializePygame()\n",
    "            \n",
    "        episodes_data = {}\n",
    "        max_penalty = -50\n",
    "\n",
    "        for episode_num in range(episodes):\n",
    "            episode = []\n",
    "            state = self.env.reset(num_obstacles)\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            step_count = 0\n",
    "            self.exploration_rate.append(self.epsilon)  # Adiciona o valor atual de epsilon à lista\n",
    "\n",
    "            while not done and step_count < max_steps:\n",
    "                action = self.choose_action(state)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                # reward = -1 que isso ???? sempre o reward é -1\n",
    "                episode.append((state, action, reward))\n",
    "                state = next_state\n",
    "                self.state_visits[state] += 1  # Atualiza a contagem de visitas para o estado\n",
    "                total_reward += reward\n",
    "                step_count += 1\n",
    "                if render:\n",
    "                    self.env.render()\n",
    "                    pygame.display.set_caption('Monte Carlo' )\n",
    "\n",
    "            if step_count >= max_steps and not done:\n",
    "                episode[-1] = (episode[-1][0], episode[-1][1], total_reward)\n",
    "                total_reward += max_penalty\n",
    "            else:\n",
    "                self.success_count += 1\n",
    "\n",
    "            self.learn(episode)  # Aprendizado é feito após o término do episódio\n",
    "            # self.update_epsilon()  # Atualiza epsilon após cada episódio\n",
    "            \n",
    "            episodes_data[episode_num] = {\n",
    "                'reward': total_reward,\n",
    "                'steps': step_count,\n",
    "                'path': [x[0] for x in episode]\n",
    "            }\n",
    "\n",
    "        # if render:\n",
    "        #    self.env.close()\n",
    "\n",
    "        return episodes_data\n",
    "    \n",
    "    def execute_optimal_policy(self, max_steps=100, start_pos = None):\n",
    "        state = self.env.reset(num_obstacles=0, fixed_start_pos = start_pos)\n",
    "        path = [state]  # Iniciar o caminho com o estado inicial\n",
    "        self.env.render(path=path)  # Passa o caminho atual para a função render\n",
    "        step_count = 0\n",
    "        done = False\n",
    "        while not done and step_count < max_steps:\n",
    "            action = max(self.Q[state], key=self.Q[state].get)\n",
    "            next_state, reward, done, _ = self.env.step(action)\n",
    "            path.append(next_state)  # Adicionar o novo estado ao caminho\n",
    "            self.env.render(path=path)  # Renderizar novamente com o caminho atualizado\n",
    "            state = next_state\n",
    "            step_count += 1\n",
    "            time.sleep(0.2)\n",
    "            if done:\n",
    "                # self.env.close()\n",
    "                break\n",
    "\n",
    "    def showQ(self):\n",
    "        running = True\n",
    "        while running:\n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.KEYDOWN:\n",
    "                    if event.key == pygame.K_ESCAPE:  # Tecla ESC\n",
    "                        running = False\n",
    "\n",
    "            if running:  # Desenha apenas se a janela ainda estiver aberta\n",
    "                self.env.draw_arrows(self.Q)\n",
    "                pygame.display.update()\n",
    "            else:\n",
    "                self.env.close()\n",
    "                break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1) Função Monte Carlo com Aproximação de Função Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5727cca",
   "metadata": {},
   "source": [
    "## 3) Função QLearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a0f19059",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self, env, alpha=0.1, gamma=0.99, epsilon=0.1, epsilon_decay=0.995, epsilon_min=0.01, random_initialization=True):\n",
    "        self.env = env\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.random_initialization = random_initialization\n",
    "        self.Q = {}\n",
    "        self.success_count = 0\n",
    "        self.state_visits = {}\n",
    "        self.exploration_rate = []  # Inicializa a lista para armazenar a taxa de exploração por episódio\n",
    "        self.initialize_Q_and_returns()\n",
    "\n",
    "    def initialize_Q_and_returns(self):\n",
    "        for row in range(self.env.num_rows):\n",
    "            for col in range(self.env.num_cols):\n",
    "                self.Q[(row, col)] = {}\n",
    "                self.state_visits[(row, col)] = 0\n",
    "                for a in range(self.env.action_space.n):\n",
    "                    if self.random_initialization:\n",
    "                        self.Q[(row, col)][a] = np.random.uniform(low=-1.0, high=1.0)\n",
    "                    else:\n",
    "                        self.Q[(row, col)][a] = 0\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.uniform(0, 1) < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            return max(self.Q[state], key=self.Q[state].get)\n",
    "\n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        predict = self.Q[state][action]\n",
    "        target = reward\n",
    "        if next_state:\n",
    "            target = reward + self.gamma * max(self.Q[next_state].values())\n",
    "\n",
    "        self.Q[state][action] += self.alpha * (target - predict)\n",
    "\n",
    "    def update_epsilon(self):\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon_decay * self.epsilon)\n",
    "\n",
    "    def train(self, episodes, num_obstacles=0, render=False, max_steps=100):\n",
    "        if render:\n",
    "            self.env.InitializePygame()\n",
    "    \n",
    "        episodes_data = {}\n",
    "\n",
    "        for episode in range(episodes):\n",
    "            total_reward = 0\n",
    "            state = self.env.reset(num_obstacles)\n",
    "            path = [state]\n",
    "            done = False\n",
    "            step_count = 0\n",
    "            self.exploration_rate.append(self.epsilon)\n",
    "\n",
    "            while not done and step_count < max_steps:\n",
    "                action = self.choose_action(state)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                path.append(next_state)  # Adicione o estado ao caminho\n",
    "                self.learn(state, action, reward, None if done else next_state)\n",
    "                state = next_state\n",
    "                self.state_visits[state] += 1  # Atualiza a contagem de visitas para o estado\n",
    "                total_reward += reward\n",
    "                step_count += 1  # Incremente o contador de passos\n",
    "                if render:\n",
    "                    self.env.render()\n",
    "\n",
    "            if done and step_count < max_steps:\n",
    "                self.success_count += 1\n",
    "            \n",
    "            self.update_epsilon()  # Atualiza epsilon após cada episódio\n",
    "                \n",
    "            episodes_data[episode] = {\n",
    "                'reward': total_reward,\n",
    "                'steps': step_count,\n",
    "                'path': path\n",
    "            }\n",
    "\n",
    "            # if render:\n",
    "            #    self.env.close()\n",
    "\n",
    "        return episodes_data\n",
    "\n",
    "\n",
    "    def execute_optimal_policy(self, max_steps=100, start_pos = None):\n",
    "        state = self.env.reset(num_obstacles=0, fixed_start_pos = start_pos)\n",
    "        path = [state]  # Iniciar o caminho com o estado inicial\n",
    "        self.env.render(path=path)  # Passa o caminho atual para a função render\n",
    "        step_count = 0\n",
    "        done = False\n",
    "        while not done and step_count < max_steps:\n",
    "            action = max(self.Q[state], key=self.Q[state].get)\n",
    "            next_state, reward, done, _ = self.env.step(action)\n",
    "            path.append(next_state)  # Adicionar o novo estado ao caminho\n",
    "            self.env.render(path=path)  # Renderizar novamente com o caminho atualizado\n",
    "            state = next_state\n",
    "            step_count += 1\n",
    "            time.sleep(0.2)\n",
    "            if done:\n",
    "                # self.env.close()\n",
    "                break\n",
    "\n",
    "    def showQ(self):\n",
    "        running = True\n",
    "        while running:\n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.KEYDOWN:\n",
    "                    if event.key == pygame.K_ESCAPE:  # Tecla ESC\n",
    "                        running = False\n",
    "\n",
    "            if running:  # Desenha apenas se a janela ainda estiver aberta\n",
    "                self.env.draw_arrows(self.Q)\n",
    "                pygame.display.update()\n",
    "            else:\n",
    "                self.env.close()\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1) Função QLearning com Aproximação de Função Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8c25d22",
   "metadata": {},
   "source": [
    "## 4) Função Sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ee48896f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSAgent:\n",
    "    def __init__(self, env, alpha=0.1, gamma=0.99, epsilon=0.1, lambd=0.5, epsilon_decay=0.995, epsilon_min=0.01, random_initialization=True):\n",
    "        self.env = env\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.lambd = lambd  # Parâmetro lambda para traços de elegibilidade\n",
    "        self.Q = {}\n",
    "        self.E = {}  # Dicionário para traços de elegibilidade\n",
    "        self.success_count = 0\n",
    "        self.exploration_rate = []  # Inicializa a lista para armazenar a taxa de exploração por episódio\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.random_initialization = random_initialization\n",
    "        self.state_visits = {}\n",
    "        self.initialize_Q_and_returns()\n",
    "\n",
    "    def initialize_Q_and_returns(self):\n",
    "        for row in range(self.env.num_rows):\n",
    "            for col in range(self.env.num_cols):\n",
    "                self.Q[(row, col)] = {}\n",
    "                self.E[(row, col)] = {}  # Inicializar traços de elegibilidade\n",
    "                self.state_visits[(row, col)] = 0\n",
    "                for a in range(self.env.action_space.n):\n",
    "                    self.E[(row, col)][a] = 0  # Inicializar traço de elegibilidade a zero\n",
    "                    if self.random_initialization:\n",
    "                        self.Q[(row, col)][a] = np.random.uniform(low=-1.0, high=1.0)\n",
    "                    else:\n",
    "                        self.Q[(row, col)][a] = 0\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.uniform(0, 1) < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            return max(self.Q[state], key=self.Q[state].get)\n",
    "\n",
    "    def learn(self, state, action, reward, next_state, next_action):\n",
    "        predict = self.Q[state][action]\n",
    "        target = reward\n",
    "        if next_state is not None and next_action is not None:\n",
    "            target += self.gamma * self.Q[next_state][next_action]\n",
    "\n",
    "        # Atualiza traços de elegibilidade para o estado-ação atual\n",
    "        for s in self.Q.keys():\n",
    "            for a in self.Q[s].keys():\n",
    "                self.E[s][a] *= self.gamma * self.lambd\n",
    "                if s == state and a == action:\n",
    "                    self.E[s][a] += 1\n",
    "\n",
    "        # Atualiza todos os valores Q e traços de elegibilidade\n",
    "        for s in self.Q.keys():\n",
    "            for a in self.Q[s].keys():\n",
    "                delta = self.alpha * (target - predict) * self.E[s][a]\n",
    "                self.Q[s][a] += delta\n",
    "                self.E[s][a] *= self.gamma * self.lambd\n",
    "\n",
    "    def update_epsilon(self):\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon_decay * self.epsilon)\n",
    "\n",
    "    def train(self, episodes, num_obstacles=0, render=False, max_steps=100):\n",
    "        episodes_data = {}\n",
    "\n",
    "        for episode in range(episodes):\n",
    "            total_reward = 0\n",
    "            state = self.env.reset(num_obstacles)\n",
    "            action = self.choose_action(state)\n",
    "            path = [state]\n",
    "            done = False\n",
    "            step_count = 0\n",
    "            self.exploration_rate.append(self.epsilon)\n",
    "\n",
    "            while not done and step_count < max_steps:\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                path.append(next_state)\n",
    "                next_action = None\n",
    "                if not done:\n",
    "                    next_action = self.choose_action(next_state)\n",
    "                self.learn(state, action, reward, next_state, next_action)\n",
    "                state = next_state\n",
    "                if not done:\n",
    "                    self.state_visits[state] += 1\n",
    "                    action = next_action\n",
    "                total_reward += reward\n",
    "                step_count += 1\n",
    "                if render:\n",
    "                    self.env.render()\n",
    "\n",
    "            if done and step_count < max_steps:\n",
    "                self.success_count += 1\n",
    "\n",
    "            self.update_epsilon()  # Atualiza epsilon após cada episódio\n",
    "            \n",
    "            episodes_data[episode] = {\n",
    "                'reward': total_reward,\n",
    "                'steps': step_count,\n",
    "                'path': path\n",
    "            }\n",
    "\n",
    "            # if render:\n",
    "            #    self.env.close()\n",
    "\n",
    "        return episodes_data\n",
    "\n",
    "\n",
    "    def execute_optimal_policy(self, max_steps=100, start_pos = None):\n",
    "        state = self.env.reset(num_obstacles=0, fixed_start_pos = start_pos)\n",
    "        path = [state]  # Iniciar o caminho com o estado inicial\n",
    "        self.env.render(path=path)  # Passa o caminho atual para a função render\n",
    "        step_count = 0\n",
    "        done = False\n",
    "        while not done and step_count < max_steps:\n",
    "            action = max(self.Q[state], key=self.Q[state].get)\n",
    "            next_state, reward, done, _ = self.env.step(action)\n",
    "            path.append(next_state)  # Adicionar o novo estado ao caminho\n",
    "            self.env.render(path=path)  # Renderizar novamente com o caminho atualizado\n",
    "            state = next_state\n",
    "            step_count += 1\n",
    "            time.sleep(0.2)\n",
    "            if done:\n",
    "                # self.env.close()\n",
    "                break\n",
    "\n",
    "    def showQ(self):\n",
    "        running = True\n",
    "        while running:\n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.KEYDOWN:\n",
    "                    if event.key == pygame.K_ESCAPE:  # Tecla ESC\n",
    "                        running = False\n",
    "\n",
    "            if running:  # Desenha apenas se a janela ainda estiver aberta\n",
    "                self.env.draw_arrows(self.Q)\n",
    "                pygame.display.update()\n",
    "            else:\n",
    "                self.env.close()\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1) Função SARSA com Aproximação de Função Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots e Análises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Curva de Aprendizado (Learning Curve):\n",
    "\n",
    "Mostrar a evolução da recompensa total por episódio.\n",
    "Indicar a convergência do agente para uma política estável."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(rewards, agent, show_plots, save_plots, num_obstacles):\n",
    "    \"\"\"\n",
    "    Plota a curva de aprendizado.\n",
    "\n",
    "    Args:\n",
    "    - rewards (list): Lista de recompensas totais por episódio.\n",
    "    - title (str): Título do gráfico.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(rewards, label='Total Reward per Episode')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.title(f'Learning Curve - {agent.__class__.__name__}')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    \n",
    "    if save_plots:\n",
    "        plt.savefig(f'results/{agent.__class__.__name__}/learning_curve_obs_{num_obstacles}_eps_{agent.epsilon}_gamma_{agent.gamma}_alpha_{agent.alpha}.png')\n",
    "\n",
    "    if show_plots:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Número de Passos por Episódio:\n",
    "\n",
    "Mostrar como a eficiência do agente melhora ao longo do tempo (isto é, se ele está aprendendo a alcançar o objetivo com menos passos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_steps_per_episode(steps, agent, show_plots, save_plots, num_obstacles):\n",
    "    \"\"\"\n",
    "    Plota o número de passos por episódio.\n",
    "\n",
    "    Args:\n",
    "    - steps (list): Lista do número de passos por episódio.\n",
    "    - title (str): Título do gráfico.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(steps, label='Steps per Episode')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Number of Steps')\n",
    "    plt.title(f'Steps per Episode - {agent.__class__.__name__}')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    \n",
    "    if save_plots:\n",
    "        plt.savefig(f'results/{agent.__class__.__name__}/steps_per_episode_obs_{num_obstacles}_eps_{agent.epsilon}_gamma_{agent.gamma}_alpha_{agent.alpha}.png')\n",
    "    \n",
    "    if show_plots:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapa de Calor dos Valores Q (Heatmap of Q Values):\n",
    "Para visualizar quais estados têm valores Q mais altos, indicando caminhos preferidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_q_value_heatmap(agent, show_plots, save_plots, num_obstacles):\n",
    "    \"\"\"\n",
    "    Plota o mapa de calor dos valores Q.\n",
    "\n",
    "    Args:\n",
    "    - Q (dict): Dicionário dos valores Q, onde as chaves são tuplas (estado) e os valores são dicionários de ações.\n",
    "    - title (str): Título do gráfico.\n",
    "    \"\"\"\n",
    "    Q = agent.Q\n",
    "    # Assume que os estados são tuplas (row, col) e as ações são números inteiros\n",
    "    num_rows = max([state[0] for state in Q.keys()]) + 1\n",
    "    num_cols = max([state[1] for state in Q.keys()]) + 1\n",
    "    num_actions = max([max(actions.keys()) for actions in Q.values()]) + 1\n",
    "\n",
    "    # Inicializa a matriz de valores Q\n",
    "    q_values_matrix = np.zeros((num_rows, num_cols, num_actions))\n",
    "\n",
    "    # Preenche a matriz com os valores Q\n",
    "    for state, actions in Q.items():\n",
    "        row, col = state\n",
    "        for action, value in actions.items():\n",
    "            q_values_matrix[row, col, action] = value\n",
    "\n",
    "    # Calcula o valor máximo de Q para cada estado\n",
    "    max_q_values = np.max(q_values_matrix, axis=2)\n",
    "\n",
    "    # Plota o mapa de calor\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    im = plt.imshow(max_q_values, cmap='hot', interpolation='nearest')\n",
    "    plt.colorbar(im, label='Max Q value')\n",
    "    plt.xlabel('Column')\n",
    "    plt.ylabel('Row')\n",
    "    plt.title(f'Max Q Values - {agent.__class__.__name__}')\n",
    "\n",
    "    # Adiciona anotações de valores Q\n",
    "    for row in range(num_rows):\n",
    "        for col in range(num_cols):\n",
    "            value = max_q_values[row, col]\n",
    "            plt.text(col, row, f'{value:.2f}', ha='center', va='center', color='blue')\n",
    "\n",
    "    # Salvamento e exibição do gráfico\n",
    "    if save_plots:\n",
    "        plt.savefig(f'results/{agent.__class__.__name__}/q_values_heatmap_obs_{num_obstacles}_eps_{agent.epsilon}_gamma_{agent.gamma}_alpha_{agent.alpha}.png')\n",
    "\n",
    "    if show_plots:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribuição de Visitas aos Estados:\n",
    "\n",
    "Matriz de visitas aos estados, indicando quais estados o agente visitou com mais frequência."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_state_visitation_heatmap(agent, env, show_plots, save_plots, num_obstacles):\n",
    "    \"\"\"\n",
    "    Plota o mapa de calor da visita aos estados com contagem numérica.\n",
    "\n",
    "    Args:\n",
    "    - visits (dict): Dicionário de contagem de visitas, onde as chaves são estados (tuplas) e os valores são contagens.\n",
    "    - num_rows (int): Número de linhas no ambiente.\n",
    "    - num_cols (int): Número de colunas no ambiente.\n",
    "    - title (str): Título do gráfico.\n",
    "    \"\"\"\n",
    "    visit_matrix = np.zeros((env.num_rows, env.num_cols))\n",
    "\n",
    "    for state, count in agent.state_visits.items():\n",
    "        row, col = state\n",
    "        visit_matrix[row, col] = count\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    heatmap = plt.imshow(visit_matrix, cmap='viridis', interpolation='nearest')\n",
    "    plt.colorbar(heatmap, label='Number of Visits')\n",
    "    plt.xlabel('Column')\n",
    "    plt.ylabel('Row')\n",
    "    plt.title(f'State Visitation - {agent.__class__.__name__}')\n",
    "\n",
    "    # Adiciona anotações de texto\n",
    "    for i in range(env.num_rows):\n",
    "        for j in range(env.num_cols):\n",
    "            text = plt.text(j, i, int(visit_matrix[i, j]), ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "    if save_plots:\n",
    "        plt.savefig(f'results/{agent.__class__.__name__}/state_visitation_heatmap_obs_{num_obstacles}_eps_{agent.epsilon}_gamma_{agent.gamma}_alpha_{agent.alpha}.png')\n",
    "\n",
    "    if show_plots:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gráfico de Barras de Sucesso vs. Falhas:\n",
    "\n",
    "Mostrar quantas vezes o agente alcançou o estado terminal com sucesso em comparação com falhas (por exemplo, atingir o número máximo de passos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_success_vs_failures(episodes, agent, show_plots, save_plots, num_obstacles):\n",
    "    \"\"\"\n",
    "    Plota um gráfico de barras comparando o número de sucessos e falhas, com anotações numéricas.\n",
    "\n",
    "    Args:\n",
    "    - success_count (int): Número total de sucessos.\n",
    "    - failure_count (int): Número total de falhas.\n",
    "    - title (str): Título do gráfico.\n",
    "    \"\"\"\n",
    "    # Categorias\n",
    "    categories = ['Sucesso', 'Falha']\n",
    "\n",
    "    # Valores\n",
    "    success_count = agent.success_count\n",
    "    failure_count = episodes - agent.success_count\n",
    "    values = [success_count, failure_count]\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    bars = plt.bar(categories, values, color=['green', 'red'])\n",
    "\n",
    "    # Adiciona anotações de texto com o número de sucessos/falhas em cima de cada barra\n",
    "    for bar in bars:\n",
    "        yval = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, yval, int(yval), ha='center', va='bottom')\n",
    "\n",
    "    plt.xlabel('Categoria')\n",
    "    plt.ylabel('Número de Episódios')\n",
    "    plt.title(f'Sucesso vs. Falha - {agent.__class__.__name__}')\n",
    "    plt.tight_layout()  # Ajusta o layout para evitar sobreposição de elementos\n",
    "    \n",
    "    if save_plots:\n",
    "        plt.savefig(f'results/{agent.__class__.__name__}/success_vs_failures_obs_{num_obstacles}_eps_{agent.epsilon}_gamma_{agent.gamma}_alpha_{agent.alpha}.png')\n",
    "\n",
    "    if show_plots:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gráfico de Desempenho por Episódio:\n",
    "\n",
    "Combinar várias métricas (por exemplo, recompensa, número de passos, etc.) em um gráfico para mostrar a performance por episódio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_performance_over_episodes(rewards, steps, agent, show_plots, save_plots, num_obstacles):\n",
    "    \"\"\"\n",
    "    Plota um gráfico de desempenho combinando várias métricas por episódio, com recompensa total por coluna de passos.\n",
    "\n",
    "    Args:\n",
    "    - rewards (list): Lista de recompensas totais por episódio.\n",
    "    - steps (list): Lista do número de passos por episódio.\n",
    "    - exploration_rate (list): Lista da taxa de exploração (epsilon) por episódio.\n",
    "    - title (str): Título do gráfico.\n",
    "    \"\"\"\n",
    "    episodes = range(len(rewards))\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    # Plotando o número de passos\n",
    "    color = 'tab:blue'\n",
    "    ax1.set_xlabel('Episódios')\n",
    "    ax1.set_ylabel('Número de Passos', color=color)\n",
    "    ax1.plot(episodes, steps, color=color, label='Número de Passos')\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    # Plotando a recompensa total para cada valor de passos\n",
    "    ax2 = ax1.twinx()\n",
    "    color = 'tab:green'\n",
    "    ax2.set_ylabel('Recompensa Total', color=color)\n",
    "    ax2.scatter(episodes, rewards, color=color, label='Recompensa Total')\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    # Plotando a taxa de exploração\n",
    "    ax3 = ax1.twinx()\n",
    "    ax3.spines['right'].set_position(('outward', 60))  # Desloca o terceiro eixo para a direita\n",
    "    color = 'tab:red'\n",
    "    ax3.set_ylabel('Taxa de Exploração (ε)', color=color)\n",
    "    ax3.plot(episodes, agent.exploration_rate, color=color, label='Taxa de Exploração (ε)')\n",
    "    ax3.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    plt.title(f'Desempenho por Episódio - {agent.__class__.__name__}')\n",
    "    fig.tight_layout()  # Ajusta o layout\n",
    "    \n",
    "    if save_plots:\n",
    "        plt.savefig(f'results/{agent.__class__.__name__}/performance_over_episodes_obs_{num_obstacles}_eps_{agent.epsilon}_gamma_{agent.gamma}_alpha_{agent.alpha}.png')\n",
    "\n",
    "    if show_plots:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparação entre Ambientes Determinístico e Estocástico:\n",
    "\n",
    "Gráficos lado a lado ou sobrepostos para comparar o desempenho nos dois tipos de ambientes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_deterministic_vs_stochastic(deterministic_rewards, stochastic_rewards, method, show_plots, save_plots):\n",
    "    \"\"\"\n",
    "    Plota gráficos lado a lado para comparar o desempenho em ambientes determinísticos e estocásticos.\n",
    "\n",
    "    Args:\n",
    "    - deterministic_rewards (list): Lista de recompensas totais por episódio no ambiente determinístico.\n",
    "    - stochastic_rewards (list): Lista de recompensas totais por episódio no ambiente estocástico.\n",
    "    - title (str): Título do gráfico.\n",
    "    \"\"\"\n",
    "    episodes = range(len(deterministic_rewards))\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    # Gráfico para ambiente determinístico\n",
    "    ax1.plot(episodes, deterministic_rewards, color='tab:blue')\n",
    "    ax1.set_title('Ambiente Determinístico')\n",
    "    ax1.set_xlabel('Episódios')\n",
    "    ax1.set_ylabel('Recompensa Total')\n",
    "    ax1.grid(True)\n",
    "\n",
    "    # Gráfico para ambiente estocástico\n",
    "    ax2.plot(episodes, stochastic_rewards, color='tab:green')\n",
    "    ax2.set_title('Ambiente Estocástico')\n",
    "    ax2.set_xlabel('Episódios')\n",
    "    ax2.set_ylabel('Recompensa Total')\n",
    "    ax2.grid(True)\n",
    "\n",
    "    plt.suptitle(f'Comparação entre Ambientes Determinístico e Estocástico - {method}')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_plots:\n",
    "        plt.savefig(f'results/{method}/deterministic_vs_stochastic.png')\n",
    "    \n",
    "    if show_plots:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Análise de Sensibilidade:\n",
    "\n",
    "Gráficos que mostram como a variação de um único parâmetro enquanto os outros permanecem constantes afeta a performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sensitivity_analysis(parameter_values, performance_metrics, parameter_name, method, metric_name, title, show_plots, save_plots):\n",
    "    \"\"\"\n",
    "    Plota um gráfico de sensibilidade para analisar o impacto de um parâmetro no desempenho do agente.\n",
    "\n",
    "    Args:\n",
    "    - parameter_values (list): Lista de valores do parâmetro.\n",
    "    - performance_metrics (list): Lista de métricas de desempenho.\n",
    "    - parameter_name (str): Nome do parâmetro.\n",
    "    - method (str): Nome do método.\n",
    "    - metric_name (str): Nome da métrica.\n",
    "    - title (str): Título do gráfico.\n",
    "    - show_plots (bool): Se True, exibe o gráfico.\n",
    "    - save_plots (bool): Se True, salva o gráfico.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(parameter_values, performance_metrics, marker='o')\n",
    "    plt.xlabel(parameter_name)\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "\n",
    "    if save_plots:\n",
    "        plt.savefig(f'results/{method}/sensitivity_analysis_{parameter_name}.png')\n",
    "\n",
    "    if show_plots:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tabelas de Desempenho:\n",
    "\n",
    "Resumo das estatísticas de desempenho, como média e desvio padrão das recompensas, número de passos, e taxa de sucesso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_performance_table(rewards, steps, success_count, total_episodes):\n",
    "    \"\"\"\n",
    "    Cria uma tabela de desempenho com estatísticas de desempenho do agente.\n",
    "\n",
    "    Args:\n",
    "    - rewards (list): Lista das recompensas totais por episódio.\n",
    "    - steps (list): Lista do número de passos por episódio.\n",
    "    - success_count (int): Número total de episódios bem-sucedidos.\n",
    "    - total_episodes (int): Número total de episódios.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Tabela de desempenho.\n",
    "    \"\"\"\n",
    "    data = {\n",
    "        'Média das Recompensas': [pd.Series(rewards).mean()],\n",
    "        'Desvio Padrão das Recompensas': [pd.Series(rewards).std()],\n",
    "        'Média de Passos': [pd.Series(steps).mean()],\n",
    "        'Desvio Padrão dos Passos': [pd.Series(steps).std()],\n",
    "        'Taxa de Sucesso': [success_count / total_episodes]\n",
    "    }\n",
    "\n",
    "    performance_table = pd.DataFrame(data)\n",
    "    return performance_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heatmap, learning curve and steps per episode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap_learn_steps(rewards, steps, env, agent,  show_plots, save_plots, num_obstacles):\n",
    "    fig = plt.figure(figsize=(20, 6))  # Largura, Altura\n",
    "    fig.suptitle(f'Heatmap of Q Values, Learning Curve, and Number of Steps for {agent.__class__.__name__}', fontsize=16)\n",
    "\n",
    "    q_values = np.zeros((env.num_rows, env.num_cols))\n",
    "    for row in range(env.num_rows):\n",
    "        for col in range(env.num_cols):\n",
    "            state = (row, col)\n",
    "            if state in agent.Q:\n",
    "                # Pega o máximo valor Q para o estado\n",
    "                q_values[row, col] = max(agent.Q[state].values())\n",
    "\n",
    "    # Heatmap dos valores Q\n",
    "    ax1 = fig.add_subplot(1, 3, 1)  # 1 linha, 3 colunas, posição 1\n",
    "    im = ax1.imshow(q_values, cmap='hot', interpolation='nearest')\n",
    "    fig.colorbar(im, ax=ax1, label='Q value')\n",
    "    ax1.set_title(f'Heatmap of Q Values')\n",
    "\n",
    "    # Curva de aprendizado\n",
    "    ax2 = fig.add_subplot(1, 3, 2)  # 1 linha, 3 colunas, posição 2\n",
    "    ax2.plot(rewards)\n",
    "    ax2.set_xlabel('Episode')\n",
    "    ax2.set_ylabel('Reward')\n",
    "    ax2.set_title('Learning Curve')\n",
    "\n",
    "    # Número de passos\n",
    "    ax3 = fig.add_subplot(1, 3, 3)  # 1 linha, 3 colunas, posição 3\n",
    "    ax3.plot(steps)\n",
    "    ax3.set_xlabel('Episode')\n",
    "    ax3.set_ylabel('Steps')\n",
    "    ax3.set_title('Number of Steps')\n",
    "\n",
    "    # Ajusta o layout para que não haja sobreposição\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_plots:\n",
    "        plt.savefig(f'results/{agent.__class__.__name__}/heatmap_learn_steps_obs_{num_obstacles}_eps_{agent.epsilon}_gamma_{agent.gamma}_alpha_{agent.alpha}.png')\n",
    "\n",
    "    if show_plots:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance Summary Table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "def performance_summary(agent, num_obstacles, episodes, execution_time, metrics, performance_table):\n",
    "    # Detalhes do Treinamento\n",
    "    training_details = [\n",
    "        [\"Agent\", agent.__class__.__name__],\n",
    "        [\"Obstacles\", num_obstacles],\n",
    "        [\"Epsilon\", agent.epsilon],\n",
    "        [\"Gamma\", agent.gamma],\n",
    "        [\"Alpha\", agent.alpha],\n",
    "        [\"Episodes\", episodes],\n",
    "        [\"Execution Time (seconds)\", f\"{execution_time:.2f}\"],\n",
    "        [\"Success Count\", agent.success_count]\n",
    "    ]\n",
    "\n",
    "    # Métricas de Desempenho\n",
    "    metrics_summary = []\n",
    "    for metric, values in metrics.items():\n",
    "        metrics_summary.append([f\"Max {metric}\", f\"Episode {values.index(max(values))}\", f\"{metric} = {max(values)}\"])\n",
    "        metrics_summary.append([f\"Min {metric}\", f\"Episode {values.index(min(values))}\", f\"{metric} = {min(values)}\"])\n",
    "\n",
    "    print(f\"{agent.__class__.__name__} - {num_obstacles} Obstacles - {agent.epsilon} Epsilon - {agent.gamma} Gamma - {agent.alpha} Alpha\\n\")\n",
    "    print(\"Training Details:\")\n",
    "    print(tabulate(training_details, headers=[\"Parameter\", \"Value\"], tablefmt=\"pretty\"))\n",
    "    print(\"\\nPerformance Metrics:\")\n",
    "    print(tabulate(metrics_summary, headers=[\"Metric\", \"Episode\", \"Value\"], tablefmt=\"pretty\"))\n",
    "    print(\"\\nPerformance Table:\")\n",
    "    print(tabulate(performance_table, headers=\"keys\", tablefmt=\"pretty\", showindex=False))\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execução de todas as análises:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(agent_class, maze, episodes, num_obstacles_list, param_grid,  save_plots, show_plots, render=False, max_steps=100):\n",
    "    results = {}\n",
    "    # Nome do arquivo CSV para salvar os resultados\n",
    "    csv_file = f'results/{agent_class.__name__}.csv'\n",
    "\n",
    "    # Iterate over each combination of parameters\n",
    "    for num_obstacles in num_obstacles_list:\n",
    "        for epsilon in param_grid['epsilon']:\n",
    "            for gamma in param_grid['gamma']:\n",
    "                for alpha in param_grid['alpha']:\n",
    "                    # Verifica se o arquivo já existe para decidir se precisa escrever o cabeçalho\n",
    "                    file_exists = os.path.isfile(csv_file)\n",
    "                    \n",
    "                    # Initialize the environment and agent with the current set of parameters\n",
    "                    env = MazeGameEnv(mazes[maze])\n",
    "                    agent = agent_class(env, alpha=alpha, gamma=gamma, epsilon=epsilon)\n",
    "                    \n",
    "                    # Run the training\n",
    "                    start_time = time.time()  \n",
    "                    agent_data = agent.train(episodes, render=render, num_obstacles=num_obstacles, max_steps=max_steps)\n",
    "                    end_time = time.time() \n",
    "\n",
    "                    execution_time = end_time - start_time\n",
    "\n",
    "                    # Extract and analyze data\n",
    "                    rewards, steps = extract_data(agent_data)\n",
    "                    metrics = {'reward': rewards, 'steps': steps}\n",
    "                    \n",
    "                    # Save the results\n",
    "                    results = {\n",
    "                        'execution_time': execution_time,\n",
    "                        'alpha': alpha,\n",
    "                        'gamma': gamma,\n",
    "                        'epsilon': epsilon,\n",
    "                        'num_obstacles': num_obstacles,\n",
    "                        'success_count': agent.success_count,\n",
    "                        'failure_count': episodes - agent.success_count,\n",
    "                        'rewards': [rewards],\n",
    "                        'steps': [steps],\n",
    "                        'visits': [agent.state_visits],\n",
    "                    }\n",
    "                    \n",
    "                    # Criando a tabela de desempenho\n",
    "                    performance_table = create_performance_table(rewards, steps, agent.success_count, episodes)\n",
    "                    # Exemplo de chamada da função\n",
    "                    performance_summary(agent, num_obstacles, episodes, execution_time, metrics, performance_table)\n",
    "\n",
    "\n",
    "                    if show_plots or save_plots:\n",
    "                        # Plot the heatmap, learning curve and number of steps\n",
    "                        heatmap_learn_steps(rewards, steps, env, agent, show_plots, save_plots, num_obstacles)\n",
    "                        \n",
    "                        # Chama a função para plotar a curva de aprendizado\n",
    "                        plot_learning_curve(rewards, agent, show_plots, save_plots, num_obstacles)\n",
    "\n",
    "                        # Chama a função para plotar o gráfico do número de passos por episódio\n",
    "                        plot_steps_per_episode(steps, agent, show_plots, save_plots, num_obstacles)\n",
    "\n",
    "                        # # Chama a função para plotar o mapa de calor dos valores Q\n",
    "                        plot_q_value_heatmap(agent, show_plots, save_plots, num_obstacles)\n",
    "\n",
    "                        # Chama a função para plotar o mapa de calor de visitação dos estados\n",
    "                        plot_state_visitation_heatmap(agent, env, show_plots, save_plots, num_obstacles)\n",
    "\n",
    "                        # Chama a função para plotar o gráfico de barras\n",
    "                        plot_success_vs_failures(episodes, agent, show_plots, save_plots, num_obstacles)\n",
    "\n",
    "                        # Chama a função para plotar o gráfico de desempenho\n",
    "                        plot_performance_over_episodes(rewards, steps, agent, show_plots, save_plots, num_obstacles)\n",
    "\n",
    "                    # Execução da política ótima\n",
    "                    if render:\n",
    "                        agent.execute_optimal_policy(max_steps=100, start_pos=(0, 0))\n",
    "                        agent.showQ()\n",
    "                    \n",
    "                    pd.DataFrame(results).to_csv(csv_file, mode='a', header=not file_exists, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Experimentos Monte Carlo\n",
    "\n",
    "**episódios** = 1000\n",
    "\n",
    "**reward =** -1 quando mover sem obstáculos e -50 quando mover em paredes ou obstáculos -150 se passar de 100 movimentos no episódio\n",
    "\n",
    "**alpha =** variável de acordo com a formula $\\alpha_{t} = 1/N_(s_t, s_a)$\n",
    "\n",
    "**gamma =** 0.1 , 0.6, 0.9, 0.99\n",
    "\n",
    "**epsilon =**  de acordo com a formula $\\epsilon_{t} = N0/(N0+N(st))$\n",
    "\n",
    "**Ambiente =** Matriz 6 x 6 com obstáculo fixo ao centro "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MonteCarloAgent - 0 Obstacles - 0.1 Epsilon - 0.1 Gamma - 0.1 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------------+\n",
      "|        Parameter         |      Value      |\n",
      "+--------------------------+-----------------+\n",
      "|          Agent           | MonteCarloAgent |\n",
      "|        Obstacles         |        0        |\n",
      "|         Epsilon          |       0.1       |\n",
      "|          Gamma           |       0.1       |\n",
      "|          Alpha           |       0.1       |\n",
      "|         Episodes         |      1000       |\n",
      "| Execution Time (seconds) |      0.53       |\n",
      "|      Success Count       |       683       |\n",
      "+--------------------------+-----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-----------+---------------+\n",
      "|   Metric   |  Episode  |     Value     |\n",
      "+------------+-----------+---------------+\n",
      "| Max reward | Episode 8 | reward = 100  |\n",
      "| Min reward | Episode 0 | reward = -996 |\n",
      "| Max steps  | Episode 0 |  steps = 100  |\n",
      "| Min steps  | Episode 8 |   steps = 1   |\n",
      "+------------+-----------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|         -7.93         |      154.00792687891604       |     42.208      |    43.38793898365362     |      0.683      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "MonteCarloAgent - 0 Obstacles - 0.1 Epsilon - 0.1 Gamma - 0.5 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------------+\n",
      "|        Parameter         |      Value      |\n",
      "+--------------------------+-----------------+\n",
      "|          Agent           | MonteCarloAgent |\n",
      "|        Obstacles         |        0        |\n",
      "|         Epsilon          |       0.1       |\n",
      "|          Gamma           |       0.1       |\n",
      "|          Alpha           |       0.5       |\n",
      "|         Episodes         |      1000       |\n",
      "| Execution Time (seconds) |      0.29       |\n",
      "|      Success Count       |       817       |\n",
      "+--------------------------+-----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 62 | reward = 100  |\n",
      "| Min reward | Episode 33 | reward = -969 |\n",
      "| Max steps  | Episode 0  |  steps = 100  |\n",
      "| Min steps  | Episode 62 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        26.138         |       139.9275694088934       |     30.534      |     37.5806485993676     |      0.817      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "MonteCarloAgent - 0 Obstacles - 0.1 Epsilon - 0.1 Gamma - 0.9 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------------+\n",
      "|        Parameter         |      Value      |\n",
      "+--------------------------+-----------------+\n",
      "|          Agent           | MonteCarloAgent |\n",
      "|        Obstacles         |        0        |\n",
      "|         Epsilon          |       0.1       |\n",
      "|          Gamma           |       0.1       |\n",
      "|          Alpha           |       0.9       |\n",
      "|         Episodes         |      1000       |\n",
      "| Execution Time (seconds) |      0.16       |\n",
      "|      Success Count       |       950       |\n",
      "+--------------------------+-----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 14 | reward = 100  |\n",
      "| Min reward | Episode 9  | reward = -960 |\n",
      "| Max steps  | Episode 0  |  steps = 100  |\n",
      "| Min steps  | Episode 14 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        68.385         |      116.64700295463359       |      12.69      |    22.63723358417155     |      0.95       |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "MonteCarloAgent - 0 Obstacles - 0.1 Epsilon - 0.6 Gamma - 0.1 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------------+\n",
      "|        Parameter         |      Value      |\n",
      "+--------------------------+-----------------+\n",
      "|          Agent           | MonteCarloAgent |\n",
      "|        Obstacles         |        0        |\n",
      "|         Epsilon          |       0.1       |\n",
      "|          Gamma           |       0.6       |\n",
      "|          Alpha           |       0.1       |\n",
      "|         Episodes         |      1000       |\n",
      "| Execution Time (seconds) |      0.12       |\n",
      "|      Success Count       |       970       |\n",
      "+--------------------------+-----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-----------+---------------+\n",
      "|   Metric   |  Episode  |     Value     |\n",
      "+------------+-----------+---------------+\n",
      "| Max reward | Episode 2 | reward = 100  |\n",
      "| Min reward | Episode 8 | reward = -996 |\n",
      "| Max steps  | Episode 0 |  steps = 100  |\n",
      "| Min steps  | Episode 2 |   steps = 1   |\n",
      "+------------+-----------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        74.733         |      102.74329124032816       |     10.892      |    18.472616857759316    |      0.97       |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "MonteCarloAgent - 0 Obstacles - 0.1 Epsilon - 0.6 Gamma - 0.5 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------------+\n",
      "|        Parameter         |      Value      |\n",
      "+--------------------------+-----------------+\n",
      "|          Agent           | MonteCarloAgent |\n",
      "|        Obstacles         |        0        |\n",
      "|         Epsilon          |       0.1       |\n",
      "|          Gamma           |       0.6       |\n",
      "|          Alpha           |       0.5       |\n",
      "|         Episodes         |      1000       |\n",
      "| Execution Time (seconds) |      0.15       |\n",
      "|      Success Count       |       971       |\n",
      "+--------------------------+-----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 23 | reward = 100  |\n",
      "| Min reward | Episode 6  | reward = -960 |\n",
      "| Max steps  | Episode 0  |  steps = 100  |\n",
      "| Min steps  | Episode 23 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        75.652         |      100.50897945433012       |     10.457      |    17.77194398463469     |      0.971      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "MonteCarloAgent - 0 Obstacles - 0.1 Epsilon - 0.6 Gamma - 0.9 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------------+\n",
      "|        Parameter         |      Value      |\n",
      "+--------------------------+-----------------+\n",
      "|          Agent           | MonteCarloAgent |\n",
      "|        Obstacles         |        0        |\n",
      "|         Epsilon          |       0.1       |\n",
      "|          Gamma           |       0.6       |\n",
      "|          Alpha           |       0.9       |\n",
      "|         Episodes         |      1000       |\n",
      "| Execution Time (seconds) |      0.36       |\n",
      "|      Success Count       |       665       |\n",
      "+--------------------------+-----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 30 | reward = 100  |\n",
      "| Min reward | Episode 11 | reward = -969 |\n",
      "| Max steps  | Episode 0  |  steps = 100  |\n",
      "| Min steps  | Episode 30 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        -4.839         |      154.00481343633894       |     38.847      |    44.13848430561245     |      0.665      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "MonteCarloAgent - 0 Obstacles - 0.1 Epsilon - 0.99 Gamma - 0.1 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------------+\n",
      "|        Parameter         |      Value      |\n",
      "+--------------------------+-----------------+\n",
      "|          Agent           | MonteCarloAgent |\n",
      "|        Obstacles         |        0        |\n",
      "|         Epsilon          |       0.1       |\n",
      "|          Gamma           |      0.99       |\n",
      "|          Alpha           |       0.1       |\n",
      "|         Episodes         |      1000       |\n",
      "| Execution Time (seconds) |      0.41       |\n",
      "|      Success Count       |       576       |\n",
      "+--------------------------+-----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+----------------+\n",
      "|   Metric   |  Episode   |     Value      |\n",
      "+------------+------------+----------------+\n",
      "| Max reward | Episode 45 |  reward = 100  |\n",
      "| Min reward | Episode 5  | reward = -1050 |\n",
      "| Max steps  | Episode 0  |  steps = 100   |\n",
      "| Min steps  | Episode 45 |   steps = 1    |\n",
      "+------------+------------+----------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        -45.587        |      192.79009946948378       |     48.255      |    46.099079167594006    |      0.576      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "MonteCarloAgent - 0 Obstacles - 0.1 Epsilon - 0.99 Gamma - 0.5 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------------+\n",
      "|        Parameter         |      Value      |\n",
      "+--------------------------+-----------------+\n",
      "|          Agent           | MonteCarloAgent |\n",
      "|        Obstacles         |        0        |\n",
      "|         Epsilon          |       0.1       |\n",
      "|          Gamma           |      0.99       |\n",
      "|          Alpha           |       0.5       |\n",
      "|         Episodes         |      1000       |\n",
      "| Execution Time (seconds) |      0.17       |\n",
      "|      Success Count       |       937       |\n",
      "+--------------------------+-----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 21 | reward = 100  |\n",
      "| Min reward | Episode 12 | reward = -978 |\n",
      "| Max steps  | Episode 0  |  steps = 100  |\n",
      "| Min steps  | Episode 21 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        57.498         |      136.88543245217008       |     15.206      |    25.232602882941904    |      0.937      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "MonteCarloAgent - 0 Obstacles - 0.1 Epsilon - 0.99 Gamma - 0.9 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------------+\n",
      "|        Parameter         |      Value      |\n",
      "+--------------------------+-----------------+\n",
      "|          Agent           | MonteCarloAgent |\n",
      "|        Obstacles         |        0        |\n",
      "|         Epsilon          |       0.1       |\n",
      "|          Gamma           |      0.99       |\n",
      "|          Alpha           |       0.9       |\n",
      "|         Episodes         |      1000       |\n",
      "| Execution Time (seconds) |      0.15       |\n",
      "|      Success Count       |       960       |\n",
      "+--------------------------+-----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-------------+----------------+\n",
      "|   Metric   |   Episode   |     Value      |\n",
      "+------------+-------------+----------------+\n",
      "| Max reward | Episode 29  |  reward = 100  |\n",
      "| Min reward | Episode 179 | reward = -1050 |\n",
      "| Max steps  |  Episode 0  |  steps = 100   |\n",
      "| Min steps  | Episode 29  |   steps = 1    |\n",
      "+------------+-------------+----------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        58.762         |      145.42575427346424       |     12.429      |    20.888017336939097    |      0.96       |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "MonteCarloAgent - 0 Obstacles - 0.5 Epsilon - 0.1 Gamma - 0.1 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------------+\n",
      "|        Parameter         |      Value      |\n",
      "+--------------------------+-----------------+\n",
      "|          Agent           | MonteCarloAgent |\n",
      "|        Obstacles         |        0        |\n",
      "|         Epsilon          |       0.5       |\n",
      "|          Gamma           |       0.1       |\n",
      "|          Alpha           |       0.1       |\n",
      "|         Episodes         |      1000       |\n",
      "| Execution Time (seconds) |      0.33       |\n",
      "|      Success Count       |       953       |\n",
      "+--------------------------+-----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 55 | reward = 100  |\n",
      "| Min reward | Episode 3  | reward = -645 |\n",
      "| Max steps  | Episode 0  |  steps = 100  |\n",
      "| Min steps  | Episode 55 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|         38.91         |       87.27070571546948       |     27.291      |    26.259129095628076    |      0.953      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "MonteCarloAgent - 0 Obstacles - 0.5 Epsilon - 0.1 Gamma - 0.5 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------------+\n",
      "|        Parameter         |      Value      |\n",
      "+--------------------------+-----------------+\n",
      "|          Agent           | MonteCarloAgent |\n",
      "|        Obstacles         |        0        |\n",
      "|         Epsilon          |       0.5       |\n",
      "|          Gamma           |       0.1       |\n",
      "|          Alpha           |       0.5       |\n",
      "|         Episodes         |      1000       |\n",
      "| Execution Time (seconds) |      0.38       |\n",
      "|      Success Count       |       906       |\n",
      "+--------------------------+-----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 20 | reward = 100  |\n",
      "| Min reward | Episode 5  | reward = -600 |\n",
      "| Max steps  | Episode 0  |  steps = 100  |\n",
      "| Min steps  | Episode 20 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        15.084         |      109.52391717078252       |      34.03      |    31.483333691022942    |      0.906      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "MonteCarloAgent - 0 Obstacles - 0.5 Epsilon - 0.1 Gamma - 0.9 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------------+\n",
      "|        Parameter         |      Value      |\n",
      "+--------------------------+-----------------+\n",
      "|          Agent           | MonteCarloAgent |\n",
      "|        Obstacles         |        0        |\n",
      "|         Epsilon          |       0.5       |\n",
      "|          Gamma           |       0.1       |\n",
      "|          Alpha           |       0.9       |\n",
      "|         Episodes         |      1000       |\n",
      "| Execution Time (seconds) |      0.30       |\n",
      "|      Success Count       |       951       |\n",
      "+--------------------------+-----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 10 | reward = 100  |\n",
      "| Min reward | Episode 3  | reward = -528 |\n",
      "| Max steps  | Episode 2  |  steps = 100  |\n",
      "| Min steps  | Episode 10 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|         40.95         |       84.92395173081371       |     25.948      |    26.657079658055114    |      0.951      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "MonteCarloAgent - 0 Obstacles - 0.5 Epsilon - 0.6 Gamma - 0.1 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------------+\n",
      "|        Parameter         |      Value      |\n",
      "+--------------------------+-----------------+\n",
      "|          Agent           | MonteCarloAgent |\n",
      "|        Obstacles         |        0        |\n",
      "|         Epsilon          |       0.5       |\n",
      "|          Gamma           |       0.6       |\n",
      "|          Alpha           |       0.1       |\n",
      "|         Episodes         |      1000       |\n",
      "| Execution Time (seconds) |      0.20       |\n",
      "|      Success Count       |       990       |\n",
      "+--------------------------+-----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 50 | reward = 100  |\n",
      "| Min reward | Episode 6  | reward = -600 |\n",
      "| Max steps  | Episode 0  |  steps = 100  |\n",
      "| Min steps  | Episode 50 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        69.108         |       53.15582637752778       |     14.596      |    14.148744501579317    |      0.99       |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "MonteCarloAgent - 0 Obstacles - 0.5 Epsilon - 0.6 Gamma - 0.5 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------------+\n",
      "|        Parameter         |      Value      |\n",
      "+--------------------------+-----------------+\n",
      "|          Agent           | MonteCarloAgent |\n",
      "|        Obstacles         |        0        |\n",
      "|         Epsilon          |       0.5       |\n",
      "|          Gamma           |       0.6       |\n",
      "|          Alpha           |       0.5       |\n",
      "|         Episodes         |      1000       |\n",
      "| Execution Time (seconds) |      0.23       |\n",
      "|      Success Count       |       977       |\n",
      "+--------------------------+-----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 73 | reward = 100  |\n",
      "| Min reward | Episode 1  | reward = -465 |\n",
      "| Max steps  | Episode 0  |  steps = 100  |\n",
      "| Min steps  | Episode 73 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        62.707         |        65.26060141538         |      16.82      |    19.26007659773321     |      0.977      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "MonteCarloAgent - 0 Obstacles - 0.5 Epsilon - 0.6 Gamma - 0.9 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------------+\n",
      "|        Parameter         |      Value      |\n",
      "+--------------------------+-----------------+\n",
      "|          Agent           | MonteCarloAgent |\n",
      "|        Obstacles         |        0        |\n",
      "|         Epsilon          |       0.5       |\n",
      "|          Gamma           |       0.6       |\n",
      "|          Alpha           |       0.9       |\n",
      "|         Episodes         |      1000       |\n",
      "| Execution Time (seconds) |      0.28       |\n",
      "|      Success Count       |       995       |\n",
      "+--------------------------+-----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 17 | reward = 100  |\n",
      "| Min reward | Episode 10 | reward = -582 |\n",
      "| Max steps  | Episode 0  |  steps = 100  |\n",
      "| Min steps  | Episode 17 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        72.711         |       47.61229262920879       |     12.882      |    11.924104170136232    |      0.995      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "MonteCarloAgent - 0 Obstacles - 0.5 Epsilon - 0.99 Gamma - 0.1 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------------+\n",
      "|        Parameter         |      Value      |\n",
      "+--------------------------+-----------------+\n",
      "|          Agent           | MonteCarloAgent |\n",
      "|        Obstacles         |        0        |\n",
      "|         Epsilon          |       0.5       |\n",
      "|          Gamma           |      0.99       |\n",
      "|          Alpha           |       0.1       |\n",
      "|         Episodes         |      1000       |\n",
      "| Execution Time (seconds) |      0.20       |\n",
      "|      Success Count       |       995       |\n",
      "+--------------------------+-----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-----------+---------------+\n",
      "|   Metric   |  Episode  |     Value     |\n",
      "+------------+-----------+---------------+\n",
      "| Max reward | Episode 7 | reward = 100  |\n",
      "| Min reward | Episode 1 | reward = -492 |\n",
      "| Max steps  | Episode 0 |  steps = 100  |\n",
      "| Min steps  | Episode 7 |   steps = 1   |\n",
      "+------------+-----------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        65.314         |      55.992629680865846       |      15.95      |    17.36061320607342     |      0.995      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "MonteCarloAgent - 0 Obstacles - 0.5 Epsilon - 0.99 Gamma - 0.5 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------------+\n",
      "|        Parameter         |      Value      |\n",
      "+--------------------------+-----------------+\n",
      "|          Agent           | MonteCarloAgent |\n",
      "|        Obstacles         |        0        |\n",
      "|         Epsilon          |       0.5       |\n",
      "|          Gamma           |      0.99       |\n",
      "|          Alpha           |       0.5       |\n",
      "|         Episodes         |      1000       |\n",
      "| Execution Time (seconds) |      0.17       |\n",
      "|      Success Count       |       995       |\n",
      "+--------------------------+-----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 13 | reward = 100  |\n",
      "| Min reward | Episode 2  | reward = -618 |\n",
      "| Max steps  | Episode 2  |  steps = 100  |\n",
      "| Min steps  | Episode 13 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        69.728         |       54.57993207200779       |     13.246      |    12.746370226615653    |      0.995      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "MonteCarloAgent - 0 Obstacles - 0.5 Epsilon - 0.99 Gamma - 0.9 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------------+\n",
      "|        Parameter         |      Value      |\n",
      "+--------------------------+-----------------+\n",
      "|          Agent           | MonteCarloAgent |\n",
      "|        Obstacles         |        0        |\n",
      "|         Epsilon          |       0.5       |\n",
      "|          Gamma           |      0.99       |\n",
      "|          Alpha           |       0.9       |\n",
      "|         Episodes         |      1000       |\n",
      "| Execution Time (seconds) |      0.17       |\n",
      "|      Success Count       |       993       |\n",
      "+--------------------------+-----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-----------+---------------+\n",
      "|   Metric   |  Episode  |     Value     |\n",
      "+------------+-----------+---------------+\n",
      "| Max reward | Episode 5 | reward = 100  |\n",
      "| Min reward | Episode 0 | reward = -501 |\n",
      "| Max steps  | Episode 0 |  steps = 100  |\n",
      "| Min steps  | Episode 5 |   steps = 1   |\n",
      "+------------+-----------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        62.059         |      57.215252372982704       |     15.546      |    14.866031174556404    |      0.993      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "MonteCarloAgent - 0 Obstacles - 0.6 Epsilon - 0.1 Gamma - 0.1 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------------+\n",
      "|        Parameter         |      Value      |\n",
      "+--------------------------+-----------------+\n",
      "|          Agent           | MonteCarloAgent |\n",
      "|        Obstacles         |        0        |\n",
      "|         Epsilon          |       0.6       |\n",
      "|          Gamma           |       0.1       |\n",
      "|          Alpha           |       0.1       |\n",
      "|         Episodes         |      1000       |\n",
      "| Execution Time (seconds) |      0.38       |\n",
      "|      Success Count       |       862       |\n",
      "+--------------------------+-----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 41 | reward = 100  |\n",
      "| Min reward | Episode 0  | reward = -492 |\n",
      "| Max steps  | Episode 0  |  steps = 100  |\n",
      "| Min steps  | Episode 41 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        -5.075         |      127.46961943490253       |     37.123      |    33.86001139059719     |      0.862      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "MonteCarloAgent - 0 Obstacles - 0.6 Epsilon - 0.1 Gamma - 0.5 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------------+\n",
      "|        Parameter         |      Value      |\n",
      "+--------------------------+-----------------+\n",
      "|          Agent           | MonteCarloAgent |\n",
      "|        Obstacles         |        0        |\n",
      "|         Epsilon          |       0.6       |\n",
      "|          Gamma           |       0.1       |\n",
      "|          Alpha           |       0.5       |\n",
      "|         Episodes         |      1000       |\n",
      "| Execution Time (seconds) |      0.28       |\n",
      "|      Success Count       |       970       |\n",
      "+--------------------------+-----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 23 | reward = 100  |\n",
      "| Min reward | Episode 4  | reward = -735 |\n",
      "| Max steps  | Episode 0  |  steps = 100  |\n",
      "| Min steps  | Episode 23 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        38.011         |       83.49266976750059       |     24.637      |    23.00988234836688     |      0.97       |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "MonteCarloAgent - 0 Obstacles - 0.6 Epsilon - 0.1 Gamma - 0.9 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------------+\n",
      "|        Parameter         |      Value      |\n",
      "+--------------------------+-----------------+\n",
      "|          Agent           | MonteCarloAgent |\n",
      "|        Obstacles         |        0        |\n",
      "|         Epsilon          |       0.6       |\n",
      "|          Gamma           |       0.1       |\n",
      "|          Alpha           |       0.9       |\n",
      "|         Episodes         |      1000       |\n",
      "| Execution Time (seconds) |      0.41       |\n",
      "|      Success Count       |       867       |\n",
      "+--------------------------+-----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 51 | reward = 100  |\n",
      "| Min reward | Episode 6  | reward = -519 |\n",
      "| Max steps  | Episode 1  |  steps = 100  |\n",
      "| Min steps  | Episode 51 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|         1.04          |      122.11363617606406       |     36.632      |    34.15111820826479     |      0.867      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "MonteCarloAgent - 0 Obstacles - 0.6 Epsilon - 0.6 Gamma - 0.1 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------------+\n",
      "|        Parameter         |      Value      |\n",
      "+--------------------------+-----------------+\n",
      "|          Agent           | MonteCarloAgent |\n",
      "|        Obstacles         |        0        |\n",
      "|         Epsilon          |       0.6       |\n",
      "|          Gamma           |       0.6       |\n",
      "|          Alpha           |       0.1       |\n",
      "|         Episodes         |      1000       |\n",
      "| Execution Time (seconds) |      0.19       |\n",
      "|      Success Count       |       994       |\n",
      "+--------------------------+-----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 11 | reward = 100  |\n",
      "| Min reward | Episode 8  | reward = -483 |\n",
      "| Max steps  | Episode 8  |  steps = 100  |\n",
      "| Min steps  | Episode 11 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        62.998         |       49.08371631414238       |     16.657      |    15.420932402295142    |      0.994      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "MonteCarloAgent - 0 Obstacles - 0.6 Epsilon - 0.6 Gamma - 0.5 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------------+\n",
      "|        Parameter         |      Value      |\n",
      "+--------------------------+-----------------+\n",
      "|          Agent           | MonteCarloAgent |\n",
      "|        Obstacles         |        0        |\n",
      "|         Epsilon          |       0.6       |\n",
      "|          Gamma           |       0.6       |\n",
      "|          Alpha           |       0.5       |\n",
      "|         Episodes         |      1000       |\n",
      "| Execution Time (seconds) |      0.21       |\n",
      "|      Success Count       |       988       |\n",
      "+--------------------------+-----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 67 | reward = 100  |\n",
      "| Min reward | Episode 3  | reward = -582 |\n",
      "| Max steps  | Episode 0  |  steps = 100  |\n",
      "| Min steps  | Episode 67 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        58.234         |      59.327691839637744       |     18.445      |    17.975096079828866    |      0.988      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "MonteCarloAgent - 0 Obstacles - 0.6 Epsilon - 0.6 Gamma - 0.9 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------------+\n",
      "|        Parameter         |      Value      |\n",
      "+--------------------------+-----------------+\n",
      "|          Agent           | MonteCarloAgent |\n",
      "|        Obstacles         |        0        |\n",
      "|         Epsilon          |       0.6       |\n",
      "|          Gamma           |       0.6       |\n",
      "|          Alpha           |       0.9       |\n",
      "|         Episodes         |      1000       |\n",
      "| Execution Time (seconds) |      0.32       |\n",
      "|      Success Count       |       983       |\n",
      "+--------------------------+-----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 9  | reward = 100  |\n",
      "| Min reward | Episode 33 | reward = -618 |\n",
      "| Max steps  | Episode 0  |  steps = 100  |\n",
      "| Min steps  | Episode 9  |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        60.715         |       65.82288680563492       |     16.658      |    16.50264746558483     |      0.983      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "MonteCarloAgent - 0 Obstacles - 0.6 Epsilon - 0.99 Gamma - 0.1 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------------+\n",
      "|        Parameter         |      Value      |\n",
      "+--------------------------+-----------------+\n",
      "|          Agent           | MonteCarloAgent |\n",
      "|        Obstacles         |        0        |\n",
      "|         Epsilon          |       0.6       |\n",
      "|          Gamma           |      0.99       |\n",
      "|          Alpha           |       0.1       |\n",
      "|         Episodes         |      1000       |\n",
      "| Execution Time (seconds) |      0.28       |\n",
      "|      Success Count       |       989       |\n",
      "+--------------------------+-----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 17 | reward = 100  |\n",
      "| Min reward | Episode 3  | reward = -555 |\n",
      "| Max steps  | Episode 0  |  steps = 100  |\n",
      "| Min steps  | Episode 17 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        52.336         |       67.74217376277578       |     17.636      |    15.934032678433837    |      0.989      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "MonteCarloAgent - 0 Obstacles - 0.6 Epsilon - 0.99 Gamma - 0.5 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------------+\n",
      "|        Parameter         |      Value      |\n",
      "+--------------------------+-----------------+\n",
      "|          Agent           | MonteCarloAgent |\n",
      "|        Obstacles         |        0        |\n",
      "|         Epsilon          |       0.6       |\n",
      "|          Gamma           |      0.99       |\n",
      "|          Alpha           |       0.5       |\n",
      "|         Episodes         |      1000       |\n",
      "| Execution Time (seconds) |      0.25       |\n",
      "|      Success Count       |       989       |\n",
      "+--------------------------+-----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 11 | reward = 100  |\n",
      "| Min reward | Episode 17 | reward = -528 |\n",
      "| Max steps  | Episode 0  |  steps = 100  |\n",
      "| Min steps  | Episode 11 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        56.354         |       61.78653785623742       |     18.154      |    16.871601433620192    |      0.989      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "MonteCarloAgent - 0 Obstacles - 0.6 Epsilon - 0.99 Gamma - 0.9 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------------+\n",
      "|        Parameter         |      Value      |\n",
      "+--------------------------+-----------------+\n",
      "|          Agent           | MonteCarloAgent |\n",
      "|        Obstacles         |        0        |\n",
      "|         Epsilon          |       0.6       |\n",
      "|          Gamma           |      0.99       |\n",
      "|          Alpha           |       0.9       |\n",
      "|         Episodes         |      1000       |\n",
      "| Execution Time (seconds) |      0.24       |\n",
      "|      Success Count       |       976       |\n",
      "+--------------------------+-----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 16 | reward = 100  |\n",
      "| Min reward | Episode 13 | reward = -573 |\n",
      "| Max steps  | Episode 0  |  steps = 100  |\n",
      "| Min steps  | Episode 16 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        49.917         |       81.26878075697782       |     19.334      |    19.795107736263784    |      0.976      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "MonteCarloAgent - 2 Obstacles - 0.1 Epsilon - 0.1 Gamma - 0.1 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------------+\n",
      "|        Parameter         |      Value      |\n",
      "+--------------------------+-----------------+\n",
      "|          Agent           | MonteCarloAgent |\n",
      "|        Obstacles         |        2        |\n",
      "|         Epsilon          |       0.1       |\n",
      "|          Gamma           |       0.1       |\n",
      "|          Alpha           |       0.1       |\n",
      "|         Episodes         |      1000       |\n",
      "| Execution Time (seconds) |      0.69       |\n",
      "|      Success Count       |       231       |\n",
      "+--------------------------+-----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+----------------+\n",
      "|   Metric   |  Episode   |     Value      |\n",
      "+------------+------------+----------------+\n",
      "| Max reward | Episode 27 |  reward = 100  |\n",
      "| Min reward | Episode 98 | reward = -1050 |\n",
      "| Max steps  | Episode 0  |  steps = 100   |\n",
      "| Min steps  | Episode 27 |   steps = 1    |\n",
      "+------------+------------+----------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        -211.91        |      254.48499073719557       |     81.267      |    36.37148052520815     |      0.231      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "MonteCarloAgent - 2 Obstacles - 0.1 Epsilon - 0.1 Gamma - 0.5 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------------+\n",
      "|        Parameter         |      Value      |\n",
      "+--------------------------+-----------------+\n",
      "|          Agent           | MonteCarloAgent |\n",
      "|        Obstacles         |        2        |\n",
      "|         Epsilon          |       0.1       |\n",
      "|          Gamma           |       0.1       |\n",
      "|          Alpha           |       0.5       |\n",
      "|         Episodes         |      1000       |\n",
      "| Execution Time (seconds) |      0.90       |\n",
      "|      Success Count       |       239       |\n",
      "+--------------------------+-----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-------------+----------------+\n",
      "|   Metric   |   Episode   |     Value      |\n",
      "+------------+-------------+----------------+\n",
      "| Max reward | Episode 68  |  reward = 100  |\n",
      "| Min reward | Episode 646 | reward = -1050 |\n",
      "| Max steps  |  Episode 0  |  steps = 100   |\n",
      "| Min steps  | Episode 68  |   steps = 1    |\n",
      "+------------+-------------+----------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        -198.97        |      249.61767983475465       |     81.667      |    35.78730000771003     |      0.239      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "MonteCarloAgent - 2 Obstacles - 0.1 Epsilon - 0.1 Gamma - 0.9 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------------+\n",
      "|        Parameter         |      Value      |\n",
      "+--------------------------+-----------------+\n",
      "|          Agent           | MonteCarloAgent |\n",
      "|        Obstacles         |        2        |\n",
      "|         Epsilon          |       0.1       |\n",
      "|          Gamma           |       0.1       |\n",
      "|          Alpha           |       0.9       |\n",
      "|         Episodes         |      1000       |\n",
      "| Execution Time (seconds) |      0.83       |\n",
      "|      Success Count       |       181       |\n",
      "+--------------------------+-----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+----------------+\n",
      "|   Metric   |  Episode   |     Value      |\n",
      "+------------+------------+----------------+\n",
      "| Max reward | Episode 17 |  reward = 100  |\n",
      "| Min reward | Episode 53 | reward = -1050 |\n",
      "| Max steps  | Episode 0  |  steps = 100   |\n",
      "| Min steps  | Episode 17 |   steps = 1    |\n",
      "+------------+------------+----------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        -228.59        |       262.477466309625        |     85.906      |    32.389291906897725    |      0.181      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "MonteCarloAgent - 2 Obstacles - 0.1 Epsilon - 0.6 Gamma - 0.1 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------------+\n",
      "|        Parameter         |      Value      |\n",
      "+--------------------------+-----------------+\n",
      "|          Agent           | MonteCarloAgent |\n",
      "|        Obstacles         |        2        |\n",
      "|         Epsilon          |       0.1       |\n",
      "|          Gamma           |       0.6       |\n",
      "|          Alpha           |       0.1       |\n",
      "|         Episodes         |      1000       |\n",
      "| Execution Time (seconds) |      0.64       |\n",
      "|      Success Count       |       294       |\n",
      "+--------------------------+-----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-------------+----------------+\n",
      "|   Metric   |   Episode   |     Value      |\n",
      "+------------+-------------+----------------+\n",
      "| Max reward |  Episode 3  |  reward = 100  |\n",
      "| Min reward | Episode 750 | reward = -1050 |\n",
      "| Max steps  |  Episode 1  |  steps = 100   |\n",
      "| Min steps  |  Episode 3  |   steps = 1    |\n",
      "+------------+-------------+----------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|       -186.836        |      247.94478764781832       |     76.128      |    39.605507346986634    |      0.294      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "MonteCarloAgent - 2 Obstacles - 0.1 Epsilon - 0.6 Gamma - 0.5 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------------+\n",
      "|        Parameter         |      Value      |\n",
      "+--------------------------+-----------------+\n",
      "|          Agent           | MonteCarloAgent |\n",
      "|        Obstacles         |        2        |\n",
      "|         Epsilon          |       0.1       |\n",
      "|          Gamma           |       0.6       |\n",
      "|          Alpha           |       0.5       |\n",
      "|         Episodes         |      1000       |\n",
      "| Execution Time (seconds) |      0.69       |\n",
      "|      Success Count       |       148       |\n",
      "+--------------------------+-----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-------------+----------------+\n",
      "|   Metric   |   Episode   |     Value      |\n",
      "+------------+-------------+----------------+\n",
      "| Max reward | Episode 24  |  reward = 100  |\n",
      "| Min reward | Episode 319 | reward = -1050 |\n",
      "| Max steps  |  Episode 1  |  steps = 100   |\n",
      "| Min steps  | Episode 24  |   steps = 1    |\n",
      "+------------+-------------+----------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|       -219.627        |      226.28863519279403       |     87.296      |    31.837902506855503    |      0.148      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "MonteCarloAgent - 2 Obstacles - 0.1 Epsilon - 0.6 Gamma - 0.9 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------------+\n",
      "|        Parameter         |      Value      |\n",
      "+--------------------------+-----------------+\n",
      "|          Agent           | MonteCarloAgent |\n",
      "|        Obstacles         |        2        |\n",
      "|         Epsilon          |       0.1       |\n",
      "|          Gamma           |       0.6       |\n",
      "|          Alpha           |       0.9       |\n",
      "|         Episodes         |      1000       |\n",
      "| Execution Time (seconds) |      0.94       |\n",
      "|      Success Count       |       224       |\n",
      "+--------------------------+-----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-------------+----------------+\n",
      "|   Metric   |   Episode   |     Value      |\n",
      "+------------+-------------+----------------+\n",
      "| Max reward | Episode 22  |  reward = 100  |\n",
      "| Min reward | Episode 314 | reward = -1050 |\n",
      "| Max steps  |  Episode 0  |  steps = 100   |\n",
      "| Min steps  | Episode 22  |   steps = 1    |\n",
      "+------------+-------------+----------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|       -208.555        |      250.69628957087616       |      80.86      |    37.40271866273043     |      0.224      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "MonteCarloAgent - 2 Obstacles - 0.1 Epsilon - 0.99 Gamma - 0.1 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------------+\n",
      "|        Parameter         |      Value      |\n",
      "+--------------------------+-----------------+\n",
      "|          Agent           | MonteCarloAgent |\n",
      "|        Obstacles         |        2        |\n",
      "|         Epsilon          |       0.1       |\n",
      "|          Gamma           |      0.99       |\n",
      "|          Alpha           |       0.1       |\n",
      "|         Episodes         |      1000       |\n",
      "| Execution Time (seconds) |      0.68       |\n",
      "|      Success Count       |       328       |\n",
      "+--------------------------+-----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-------------+----------------+\n",
      "|   Metric   |   Episode   |     Value      |\n",
      "+------------+-------------+----------------+\n",
      "| Max reward | Episode 39  |  reward = 100  |\n",
      "| Min reward | Episode 229 | reward = -1041 |\n",
      "| Max steps  |  Episode 0  |  steps = 100   |\n",
      "| Min steps  | Episode 39  |   steps = 1    |\n",
      "+------------+-------------+----------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|       -208.627        |       263.5512647287813       |     76.989      |    36.927124062015594    |      0.328      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "MonteCarloAgent - 2 Obstacles - 0.1 Epsilon - 0.99 Gamma - 0.5 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------------+\n",
      "|        Parameter         |      Value      |\n",
      "+--------------------------+-----------------+\n",
      "|          Agent           | MonteCarloAgent |\n",
      "|        Obstacles         |        2        |\n",
      "|         Epsilon          |       0.1       |\n",
      "|          Gamma           |      0.99       |\n",
      "|          Alpha           |       0.5       |\n",
      "|         Episodes         |      1000       |\n",
      "| Execution Time (seconds) |      0.68       |\n",
      "|      Success Count       |       302       |\n",
      "+--------------------------+-----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+----------------+\n",
      "|   Metric   |  Episode   |     Value      |\n",
      "+------------+------------+----------------+\n",
      "| Max reward | Episode 41 |  reward = 100  |\n",
      "| Min reward | Episode 3  | reward = -1050 |\n",
      "| Max steps  | Episode 0  |  steps = 100   |\n",
      "| Min steps  | Episode 41 |   steps = 1    |\n",
      "+------------+------------+----------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|       -228.841        |       269.4650833598857       |     79.993      |     34.7489055574048     |      0.302      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "MonteCarloAgent - 2 Obstacles - 0.1 Epsilon - 0.99 Gamma - 0.9 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------------+\n",
      "|        Parameter         |      Value      |\n",
      "+--------------------------+-----------------+\n",
      "|          Agent           | MonteCarloAgent |\n",
      "|        Obstacles         |        2        |\n",
      "|         Epsilon          |       0.1       |\n",
      "|          Gamma           |      0.99       |\n",
      "|          Alpha           |       0.9       |\n",
      "|         Episodes         |      1000       |\n",
      "| Execution Time (seconds) |      0.76       |\n",
      "|      Success Count       |       281       |\n",
      "+--------------------------+-----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-------------+----------------+\n",
      "|   Metric   |   Episode   |     Value      |\n",
      "+------------+-------------+----------------+\n",
      "| Max reward | Episode 26  |  reward = 100  |\n",
      "| Min reward | Episode 599 | reward = -1050 |\n",
      "| Max steps  |  Episode 0  |  steps = 100   |\n",
      "| Min steps  | Episode 26  |   steps = 1    |\n",
      "+------------+-------------+----------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|       -208.838        |      254.95928300807563       |     78.626      |    37.437116097018766    |      0.281      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "MonteCarloAgent - 2 Obstacles - 0.5 Epsilon - 0.1 Gamma - 0.1 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------------+\n",
      "|        Parameter         |      Value      |\n",
      "+--------------------------+-----------------+\n",
      "|          Agent           | MonteCarloAgent |\n",
      "|        Obstacles         |        2        |\n",
      "|         Epsilon          |       0.5       |\n",
      "|          Gamma           |       0.1       |\n",
      "|          Alpha           |       0.1       |\n",
      "|         Episodes         |      1000       |\n",
      "| Execution Time (seconds) |      0.90       |\n",
      "|      Success Count       |       611       |\n",
      "+--------------------------+-----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-------------+----------------+\n",
      "|   Metric   |   Episode   |     Value      |\n",
      "+------------+-------------+----------------+\n",
      "| Max reward | Episode 79  |  reward = 100  |\n",
      "| Min reward | Episode 762 | reward = -1050 |\n",
      "| Max steps  |  Episode 0  |  steps = 100   |\n",
      "| Min steps  | Episode 79  |   steps = 1    |\n",
      "+------------+-------------+----------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        -122.98        |      206.83249570916388       |     57.178      |    40.46665128060223     |      0.611      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "MonteCarloAgent - 2 Obstacles - 0.5 Epsilon - 0.1 Gamma - 0.5 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------------+\n",
      "|        Parameter         |      Value      |\n",
      "+--------------------------+-----------------+\n",
      "|          Agent           | MonteCarloAgent |\n",
      "|        Obstacles         |        2        |\n",
      "|         Epsilon          |       0.5       |\n",
      "|          Gamma           |       0.1       |\n",
      "|          Alpha           |       0.5       |\n",
      "|         Episodes         |      1000       |\n",
      "| Execution Time (seconds) |      0.76       |\n",
      "|      Success Count       |       554       |\n",
      "+--------------------------+-----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 52 | reward = 100  |\n",
      "| Min reward | Episode 56 | reward = -825 |\n",
      "| Max steps  | Episode 0  |  steps = 100  |\n",
      "| Min steps  | Episode 52 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|       -140.777        |      211.82423557699383       |     60.572      |    41.18136375498899     |      0.554      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "MonteCarloAgent - 2 Obstacles - 0.5 Epsilon - 0.1 Gamma - 0.9 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------------+\n",
      "|        Parameter         |      Value      |\n",
      "+--------------------------+-----------------+\n",
      "|          Agent           | MonteCarloAgent |\n",
      "|        Obstacles         |        2        |\n",
      "|         Epsilon          |       0.5       |\n",
      "|          Gamma           |       0.1       |\n",
      "|          Alpha           |       0.9       |\n",
      "|         Episodes         |      1000       |\n",
      "| Execution Time (seconds) |      0.62       |\n",
      "|      Success Count       |       572       |\n",
      "+--------------------------+-----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-------------+---------------+\n",
      "|   Metric   |   Episode   |     Value     |\n",
      "+------------+-------------+---------------+\n",
      "| Max reward | Episode 39  | reward = 100  |\n",
      "| Min reward | Episode 455 | reward = -717 |\n",
      "| Max steps  |  Episode 2  |  steps = 100  |\n",
      "| Min steps  | Episode 39  |   steps = 1   |\n",
      "+------------+-------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|       -136.809        |      203.14336746925005       |     60.123      |    40.98853413981684     |      0.572      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "MonteCarloAgent - 2 Obstacles - 0.5 Epsilon - 0.6 Gamma - 0.1 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------------+\n",
      "|        Parameter         |      Value      |\n",
      "+--------------------------+-----------------+\n",
      "|          Agent           | MonteCarloAgent |\n",
      "|        Obstacles         |        2        |\n",
      "|         Epsilon          |       0.5       |\n",
      "|          Gamma           |       0.6       |\n",
      "|          Alpha           |       0.1       |\n",
      "|         Episodes         |      1000       |\n",
      "| Execution Time (seconds) |      0.59       |\n",
      "|      Success Count       |       719       |\n",
      "+--------------------------+-----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-------------+---------------+\n",
      "|   Metric   |   Episode   |     Value     |\n",
      "+------------+-------------+---------------+\n",
      "| Max reward | Episode 21  | reward = 100  |\n",
      "| Min reward | Episode 981 | reward = -861 |\n",
      "| Max steps  |  Episode 0  |  steps = 100  |\n",
      "| Min steps  | Episode 21  |   steps = 1   |\n",
      "+------------+-------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        -78.215        |       196.9339781926084       |     46.568      |    39.85699381483701     |      0.719      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "MonteCarloAgent - 2 Obstacles - 0.5 Epsilon - 0.6 Gamma - 0.5 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------------+\n",
      "|        Parameter         |      Value      |\n",
      "+--------------------------+-----------------+\n",
      "|          Agent           | MonteCarloAgent |\n",
      "|        Obstacles         |        2        |\n",
      "|         Epsilon          |       0.5       |\n",
      "|          Gamma           |       0.6       |\n",
      "|          Alpha           |       0.5       |\n",
      "|         Episodes         |      1000       |\n",
      "| Execution Time (seconds) |      0.63       |\n",
      "|      Success Count       |       580       |\n",
      "+--------------------------+-----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-------------+----------------+\n",
      "|   Metric   |   Episode   |     Value      |\n",
      "+------------+-------------+----------------+\n",
      "| Max reward | Episode 98  |  reward = 100  |\n",
      "| Min reward | Episode 589 | reward = -1050 |\n",
      "| Max steps  |  Episode 0  |  steps = 100   |\n",
      "| Min steps  | Episode 98  |   steps = 1    |\n",
      "+------------+-------------+----------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|       -119.375        |       199.0659529618981       |     57.829      |    41.79862299028383     |      0.58       |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "MonteCarloAgent - 2 Obstacles - 0.5 Epsilon - 0.6 Gamma - 0.9 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------------+\n",
      "|        Parameter         |      Value      |\n",
      "+--------------------------+-----------------+\n",
      "|          Agent           | MonteCarloAgent |\n",
      "|        Obstacles         |        2        |\n",
      "|         Epsilon          |       0.5       |\n",
      "|          Gamma           |       0.6       |\n",
      "|          Alpha           |       0.9       |\n",
      "|         Episodes         |      1000       |\n",
      "| Execution Time (seconds) |      0.74       |\n",
      "|      Success Count       |       565       |\n",
      "+--------------------------+-----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-------------+---------------+\n",
      "|   Metric   |   Episode   |     Value     |\n",
      "+------------+-------------+---------------+\n",
      "| Max reward | Episode 16  | reward = 100  |\n",
      "| Min reward | Episode 395 | reward = -915 |\n",
      "| Max steps  |  Episode 0  |  steps = 100  |\n",
      "| Min steps  | Episode 16  |   steps = 1   |\n",
      "+------------+-------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|       -124.094        |       207.2698771311595       |     57.421      |    42.471892124328775    |      0.565      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "MonteCarloAgent - 2 Obstacles - 0.5 Epsilon - 0.99 Gamma - 0.1 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------------+\n",
      "|        Parameter         |      Value      |\n",
      "+--------------------------+-----------------+\n",
      "|          Agent           | MonteCarloAgent |\n",
      "|        Obstacles         |        2        |\n",
      "|         Epsilon          |       0.5       |\n",
      "|          Gamma           |      0.99       |\n",
      "|          Alpha           |       0.1       |\n",
      "|         Episodes         |      1000       |\n",
      "| Execution Time (seconds) |      0.57       |\n",
      "|      Success Count       |       724       |\n",
      "+--------------------------+-----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-------------+---------------+\n",
      "|   Metric   |   Episode   |     Value     |\n",
      "+------------+-------------+---------------+\n",
      "| Max reward | Episode 36  | reward = 100  |\n",
      "| Min reward | Episode 161 | reward = -798 |\n",
      "| Max steps  |  Episode 0  |  steps = 100  |\n",
      "| Min steps  | Episode 36  |   steps = 1   |\n",
      "+------------+-------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        -98.459        |       201.8245164851249       |     50.881      |    38.36708057530402     |      0.724      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "MonteCarloAgent - 2 Obstacles - 0.5 Epsilon - 0.99 Gamma - 0.5 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------------+\n",
      "|        Parameter         |      Value      |\n",
      "+--------------------------+-----------------+\n",
      "|          Agent           | MonteCarloAgent |\n",
      "|        Obstacles         |        2        |\n",
      "|         Epsilon          |       0.5       |\n",
      "|          Gamma           |      0.99       |\n",
      "|          Alpha           |       0.5       |\n",
      "|         Episodes         |      1000       |\n",
      "| Execution Time (seconds) |      0.42       |\n",
      "|      Success Count       |       869       |\n",
      "+--------------------------+-----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-------------+---------------+\n",
      "|   Metric   |   Episode   |     Value     |\n",
      "+------------+-------------+---------------+\n",
      "| Max reward | Episode 23  | reward = 100  |\n",
      "| Min reward | Episode 270 | reward = -960 |\n",
      "| Max steps  |  Episode 0  |  steps = 100  |\n",
      "| Min steps  | Episode 23  |   steps = 1   |\n",
      "+------------+-------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        -54.079        |       189.9237209481185       |     38.251      |    33.261785059514544    |      0.869      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "MonteCarloAgent - 2 Obstacles - 0.5 Epsilon - 0.99 Gamma - 0.9 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------------+\n",
      "|        Parameter         |      Value      |\n",
      "+--------------------------+-----------------+\n",
      "|          Agent           | MonteCarloAgent |\n",
      "|        Obstacles         |        2        |\n",
      "|         Epsilon          |       0.5       |\n",
      "|          Gamma           |      0.99       |\n",
      "|          Alpha           |       0.9       |\n",
      "|         Episodes         |      1000       |\n",
      "| Execution Time (seconds) |      0.44       |\n",
      "|      Success Count       |       845       |\n",
      "+--------------------------+-----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 58 | reward = 100  |\n",
      "| Min reward | Episode 34 | reward = -897 |\n",
      "| Max steps  | Episode 6  |  steps = 100  |\n",
      "| Min steps  | Episode 58 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        -55.378        |      188.20303264691964       |     40.876      |    34.18135440542093     |      0.845      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "MonteCarloAgent - 2 Obstacles - 0.6 Epsilon - 0.1 Gamma - 0.1 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------------+\n",
      "|        Parameter         |      Value      |\n",
      "+--------------------------+-----------------+\n",
      "|          Agent           | MonteCarloAgent |\n",
      "|        Obstacles         |        2        |\n",
      "|         Epsilon          |       0.6       |\n",
      "|          Gamma           |       0.1       |\n",
      "|          Alpha           |       0.1       |\n",
      "|         Episodes         |      1000       |\n",
      "| Execution Time (seconds) |      0.76       |\n",
      "|      Success Count       |       471       |\n",
      "+--------------------------+-----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-------------+---------------+\n",
      "|   Metric   |   Episode   |     Value     |\n",
      "+------------+-------------+---------------+\n",
      "| Max reward | Episode 26  | reward = 100  |\n",
      "| Min reward | Episode 982 | reward = -888 |\n",
      "| Max steps  |  Episode 0  |  steps = 100  |\n",
      "| Min steps  | Episode 26  |   steps = 1   |\n",
      "+------------+-------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|       -171.002        |      202.29073517474185       |     65.916      |    41.064622818762714    |      0.471      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "MonteCarloAgent - 2 Obstacles - 0.6 Epsilon - 0.1 Gamma - 0.5 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------------+\n",
      "|        Parameter         |      Value      |\n",
      "+--------------------------+-----------------+\n",
      "|          Agent           | MonteCarloAgent |\n",
      "|        Obstacles         |        2        |\n",
      "|         Epsilon          |       0.6       |\n",
      "|          Gamma           |       0.1       |\n",
      "|          Alpha           |       0.5       |\n",
      "|         Episodes         |      1000       |\n",
      "| Execution Time (seconds) |      0.87       |\n",
      "|      Success Count       |       508       |\n",
      "+--------------------------+-----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-------------+----------------+\n",
      "|   Metric   |   Episode   |     Value      |\n",
      "+------------+-------------+----------------+\n",
      "| Max reward | Episode 15  |  reward = 100  |\n",
      "| Min reward | Episode 981 | reward = -1050 |\n",
      "| Max steps  |  Episode 0  |  steps = 100   |\n",
      "| Min steps  | Episode 15  |   steps = 1    |\n",
      "+------------+-------------+----------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|       -170.264        |      209.12948487825778       |     65.761      |    39.66528979112864     |      0.508      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "MonteCarloAgent - 2 Obstacles - 0.6 Epsilon - 0.1 Gamma - 0.9 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------------+\n",
      "|        Parameter         |      Value      |\n",
      "+--------------------------+-----------------+\n",
      "|          Agent           | MonteCarloAgent |\n",
      "|        Obstacles         |        2        |\n",
      "|         Epsilon          |       0.6       |\n",
      "|          Gamma           |       0.1       |\n",
      "|          Alpha           |       0.9       |\n",
      "|         Episodes         |      1000       |\n",
      "| Execution Time (seconds) |      0.69       |\n",
      "|      Success Count       |       484       |\n",
      "+--------------------------+-----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-------------+---------------+\n",
      "|   Metric   |   Episode   |     Value     |\n",
      "+------------+-------------+---------------+\n",
      "| Max reward |  Episode 6  | reward = 100  |\n",
      "| Min reward | Episode 207 | reward = -915 |\n",
      "| Max steps  |  Episode 1  |  steps = 100  |\n",
      "| Min steps  |  Episode 6  |   steps = 1   |\n",
      "+------------+-------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|       -171.815        |      203.94507684513832       |     65.722      |     40.9219418078043     |      0.484      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "MonteCarloAgent - 2 Obstacles - 0.6 Epsilon - 0.6 Gamma - 0.1 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------------+\n",
      "|        Parameter         |      Value      |\n",
      "+--------------------------+-----------------+\n",
      "|          Agent           | MonteCarloAgent |\n",
      "|        Obstacles         |        2        |\n",
      "|         Epsilon          |       0.6       |\n",
      "|          Gamma           |       0.6       |\n",
      "|          Alpha           |       0.1       |\n",
      "|         Episodes         |      1000       |\n",
      "| Execution Time (seconds) |      0.50       |\n",
      "|      Success Count       |       787       |\n",
      "+--------------------------+-----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-------------+---------------+\n",
      "|   Metric   |   Episode   |     Value     |\n",
      "+------------+-------------+---------------+\n",
      "| Max reward |  Episode 3  | reward = 100  |\n",
      "| Min reward | Episode 126 | reward = -897 |\n",
      "| Max steps  |  Episode 0  |  steps = 100  |\n",
      "| Min steps  |  Episode 3  |   steps = 1   |\n",
      "+------------+-------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        -64.515        |      187.22793080380146       |     42.992      |    37.79566705918085     |      0.787      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "MonteCarloAgent - 2 Obstacles - 0.6 Epsilon - 0.6 Gamma - 0.5 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------------+\n",
      "|        Parameter         |      Value      |\n",
      "+--------------------------+-----------------+\n",
      "|          Agent           | MonteCarloAgent |\n",
      "|        Obstacles         |        2        |\n",
      "|         Epsilon          |       0.6       |\n",
      "|          Gamma           |       0.6       |\n",
      "|          Alpha           |       0.5       |\n",
      "|         Episodes         |      1000       |\n",
      "| Execution Time (seconds) |      0.77       |\n",
      "|      Success Count       |       711       |\n",
      "+--------------------------+-----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-------------+---------------+\n",
      "|   Metric   |   Episode   |     Value     |\n",
      "+------------+-------------+---------------+\n",
      "| Max reward | Episode 36  | reward = 100  |\n",
      "| Min reward | Episode 481 | reward = -771 |\n",
      "| Max steps  |  Episode 0  |  steps = 100  |\n",
      "| Min steps  | Episode 36  |   steps = 1   |\n",
      "+------------+-------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        -98.327        |      203.58640301968717       |     49.317      |    39.80458410680755     |      0.711      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "MonteCarloAgent - 2 Obstacles - 0.6 Epsilon - 0.6 Gamma - 0.9 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------------+\n",
      "|        Parameter         |      Value      |\n",
      "+--------------------------+-----------------+\n",
      "|          Agent           | MonteCarloAgent |\n",
      "|        Obstacles         |        2        |\n",
      "|         Epsilon          |       0.6       |\n",
      "|          Gamma           |       0.6       |\n",
      "|          Alpha           |       0.9       |\n",
      "|         Episodes         |      1000       |\n",
      "| Execution Time (seconds) |      0.61       |\n",
      "|      Success Count       |       748       |\n",
      "+--------------------------+-----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-------------+---------------+\n",
      "|   Metric   |   Episode   |     Value     |\n",
      "+------------+-------------+---------------+\n",
      "| Max reward |  Episode 3  | reward = 100  |\n",
      "| Min reward | Episode 127 | reward = -834 |\n",
      "| Max steps  |  Episode 0  |  steps = 100  |\n",
      "| Min steps  |  Episode 3  |   steps = 1   |\n",
      "+------------+-------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        -76.663        |      197.67806162305175       |     45.021      |    39.01872652996276     |      0.748      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "MonteCarloAgent - 2 Obstacles - 0.6 Epsilon - 0.99 Gamma - 0.1 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------------+\n",
      "|        Parameter         |      Value      |\n",
      "+--------------------------+-----------------+\n",
      "|          Agent           | MonteCarloAgent |\n",
      "|        Obstacles         |        2        |\n",
      "|         Epsilon          |       0.6       |\n",
      "|          Gamma           |      0.99       |\n",
      "|          Alpha           |       0.1       |\n",
      "|         Episodes         |      1000       |\n",
      "| Execution Time (seconds) |      0.49       |\n",
      "|      Success Count       |       863       |\n",
      "+--------------------------+-----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-------------+----------------+\n",
      "|   Metric   |   Episode   |     Value      |\n",
      "+------------+-------------+----------------+\n",
      "| Max reward | Episode 25  |  reward = 100  |\n",
      "| Min reward | Episode 521 | reward = -1050 |\n",
      "| Max steps  | Episode 15  |  steps = 100   |\n",
      "| Min steps  | Episode 25  |   steps = 1    |\n",
      "+------------+-------------+----------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        -58.601        |      187.13985980043407       |     40.742      |    33.20307033814614     |      0.863      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "MonteCarloAgent - 2 Obstacles - 0.6 Epsilon - 0.99 Gamma - 0.5 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------------+\n",
      "|        Parameter         |      Value      |\n",
      "+--------------------------+-----------------+\n",
      "|          Agent           | MonteCarloAgent |\n",
      "|        Obstacles         |        2        |\n",
      "|         Epsilon          |       0.6       |\n",
      "|          Gamma           |      0.99       |\n",
      "|          Alpha           |       0.5       |\n",
      "|         Episodes         |      1000       |\n",
      "| Execution Time (seconds) |      0.41       |\n",
      "|      Success Count       |       891       |\n",
      "+--------------------------+-----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-------------+---------------+\n",
      "|   Metric   |   Episode   |     Value     |\n",
      "+------------+-------------+---------------+\n",
      "| Max reward | Episode 69  | reward = 100  |\n",
      "| Min reward | Episode 593 | reward = -825 |\n",
      "| Max steps  | Episode 11  |  steps = 100  |\n",
      "| Min steps  | Episode 69  |   steps = 1   |\n",
      "+------------+-------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        -43.139        |      186.59506262598802       |     35.061      |    32.76380237008131     |      0.891      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "MonteCarloAgent - 2 Obstacles - 0.6 Epsilon - 0.99 Gamma - 0.9 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------------+\n",
      "|        Parameter         |      Value      |\n",
      "+--------------------------+-----------------+\n",
      "|          Agent           | MonteCarloAgent |\n",
      "|        Obstacles         |        2        |\n",
      "|         Epsilon          |       0.6       |\n",
      "|          Gamma           |      0.99       |\n",
      "|          Alpha           |       0.9       |\n",
      "|         Episodes         |      1000       |\n",
      "| Execution Time (seconds) |      0.39       |\n",
      "|      Success Count       |       892       |\n",
      "+--------------------------+-----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-------------+---------------+\n",
      "|   Metric   |   Episode   |     Value     |\n",
      "+------------+-------------+---------------+\n",
      "| Max reward | Episode 26  | reward = 100  |\n",
      "| Min reward | Episode 170 | reward = -771 |\n",
      "| Max steps  |  Episode 0  |  steps = 100  |\n",
      "| Min steps  | Episode 26  |   steps = 1   |\n",
      "+------------+-------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        -43.844        |      176.44358873023413       |     36.538      |    31.84233801859547     |      0.892      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the grid of parameters to test\n",
    "param_grid = {\n",
    "    'epsilon': [0.1, 0.5, 0.6],\n",
    "    'gamma': [0.1, 0.6, 0.99],\n",
    "    'alpha': [0.1, 0.5, 0.9]\n",
    "}\n",
    "\n",
    "# Number of obstacles to test\n",
    "num_obstacles_list = [0, 2]\n",
    "\n",
    "# Number of episodes to run for each experiment\n",
    "episodes = 1000\n",
    "\n",
    "# Number of maze to test\n",
    "maze = 3\n",
    "\n",
    "# Run the experiments\n",
    "run_experiments(MonteCarloAgent, maze, episodes, num_obstacles_list, param_grid, save_plots=False, show_plots=False, render=False, max_steps=100)\n",
    "\n",
    "# Open 0 CSV \n",
    "df = pd.read_csv(f'results/MonteCarloAgent.csv')\n",
    "# Converte as strings de volta para arrays do NumPy\n",
    "df['rewards'] = df['rewards'].apply(literal_eval)\n",
    "\n",
    "# Pega o primeiro treinado para avaliar\n",
    "rewards_deterministic = df[df['num_obstacles'] == 0]['rewards'].values[0]\n",
    "rewards_stochastic = df[df['num_obstacles'] == 2]['rewards'].values[0]\n",
    "plot_deterministic_vs_stochastic(rewards_deterministic, rewards_stochastic,method=\"MonteCarloAgent\", show_plots=True, save_plots=False)\n",
    "\n",
    "# Expande a coluna 'rewards', que contém listas, em várias linhas\n",
    "df = df.explode('rewards')\n",
    "# Converte os valores de 'rewards' para numéricos\n",
    "df['rewards'] = pd.to_numeric(df['rewards'])\n",
    "# Agrupa por 'alpha' e calcula a média das recompensas\n",
    "grouped = df.groupby('alpha')['rewards'].mean()\n",
    "# Prepara os dados para o gráfico\n",
    "alpha_values = grouped.index.tolist()\n",
    "average_rewards = grouped.values.tolist()\n",
    "plot_sensitivity_analysis(alpha_values, average_rewards,parameter_name='alpha',metric_name='Recompensa Média',method=\"MonteCarloAgent\",title='Análise de Sensibilidade para Alpha', show_plots=False, save_plots=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1) Experimento do Monte Carlo com Aproximação de função linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Experimento Q-Learning\n",
    "\n",
    "**episódios** = 1000\n",
    "\n",
    "**reward =** -1 quando mover sem obstáculos e -50 quando mover em paredes ou obstáculos -150 se passar de 100 movimentos no episódio\n",
    "\n",
    "**alpha =** variável de acordo com a formula $\\alpha_{t} = 1/N_(s_t, s_a)$\n",
    "\n",
    "**gamma =** 0.1 , 0.6, 0.9, 0.99\n",
    "\n",
    "**epsilon =**  de acordo com a formula $\\epsilon_{t} = N0/(N0+N(st))$\n",
    "\n",
    "**Ambiente =** Matriz 6 x 6 com obstáculo fixo ao centro \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QLearningAgent - 0 Obstacles - 0.01 Epsilon - 0.1 Gamma - 0.1 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+----------------+\n",
      "|        Parameter         |     Value      |\n",
      "+--------------------------+----------------+\n",
      "|          Agent           | QLearningAgent |\n",
      "|        Obstacles         |       0        |\n",
      "|         Epsilon          |      0.01      |\n",
      "|          Gamma           |      0.1       |\n",
      "|          Alpha           |      0.1       |\n",
      "|         Episodes         |      1000      |\n",
      "| Execution Time (seconds) |      0.13      |\n",
      "|      Success Count       |      993       |\n",
      "+--------------------------+----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 24 | reward = 100  |\n",
      "| Min reward | Episode 0  | reward = -190 |\n",
      "| Max steps  | Episode 0  |  steps = 100  |\n",
      "| Min steps  | Episode 24 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        89.166         |      24.194292876788275       |      9.993      |    13.884658250556068    |      0.993      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "QLearningAgent - 0 Obstacles - 0.01 Epsilon - 0.1 Gamma - 0.5 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+----------------+\n",
      "|        Parameter         |     Value      |\n",
      "+--------------------------+----------------+\n",
      "|          Agent           | QLearningAgent |\n",
      "|        Obstacles         |       0        |\n",
      "|         Epsilon          |      0.01      |\n",
      "|          Gamma           |      0.1       |\n",
      "|          Alpha           |      0.5       |\n",
      "|         Episodes         |      1000      |\n",
      "| Execution Time (seconds) |      0.09      |\n",
      "|      Success Count       |      999       |\n",
      "+--------------------------+----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-----------+---------------+\n",
      "|   Metric   |  Episode  |     Value     |\n",
      "+------------+-----------+---------------+\n",
      "| Max reward | Episode 5 | reward = 100  |\n",
      "| Min reward | Episode 7 | reward = -145 |\n",
      "| Max steps  | Episode 7 |  steps = 100  |\n",
      "| Min steps  | Episode 5 |   steps = 1   |\n",
      "+------------+-----------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        94.139         |       12.38382162492815       |      6.148      |    6.762087090671179     |      0.999      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "QLearningAgent - 0 Obstacles - 0.01 Epsilon - 0.1 Gamma - 0.9 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+----------------+\n",
      "|        Parameter         |     Value      |\n",
      "+--------------------------+----------------+\n",
      "|          Agent           | QLearningAgent |\n",
      "|        Obstacles         |       0        |\n",
      "|         Epsilon          |      0.01      |\n",
      "|          Gamma           |      0.1       |\n",
      "|          Alpha           |      0.9       |\n",
      "|         Episodes         |      1000      |\n",
      "| Execution Time (seconds) |      0.11      |\n",
      "|      Success Count       |      999       |\n",
      "+--------------------------+----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 20 | reward = 100  |\n",
      "| Min reward | Episode 2  | reward = -226 |\n",
      "| Max steps  | Episode 2  |  steps = 100  |\n",
      "| Min steps  | Episode 20 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        94.557         |      12.191611312143543       |      5.73       |    4.996405114085933     |      0.999      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "QLearningAgent - 0 Obstacles - 0.01 Epsilon - 0.6 Gamma - 0.1 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+----------------+\n",
      "|        Parameter         |     Value      |\n",
      "+--------------------------+----------------+\n",
      "|          Agent           | QLearningAgent |\n",
      "|        Obstacles         |       0        |\n",
      "|         Epsilon          |      0.01      |\n",
      "|          Gamma           |      0.6       |\n",
      "|          Alpha           |      0.1       |\n",
      "|         Episodes         |      1000      |\n",
      "| Execution Time (seconds) |      0.08      |\n",
      "|      Success Count       |      991       |\n",
      "+--------------------------+----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 28 | reward = 100  |\n",
      "| Min reward | Episode 0  | reward = -199 |\n",
      "| Max steps  | Episode 0  |  steps = 100  |\n",
      "| Min steps  | Episode 28 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|         92.26         |      24.602920633609816       |      6.967      |    10.350606071435916    |      0.991      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "QLearningAgent - 0 Obstacles - 0.01 Epsilon - 0.6 Gamma - 0.5 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+----------------+\n",
      "|        Parameter         |     Value      |\n",
      "+--------------------------+----------------+\n",
      "|          Agent           | QLearningAgent |\n",
      "|        Obstacles         |       0        |\n",
      "|         Epsilon          |      0.01      |\n",
      "|          Gamma           |      0.6       |\n",
      "|          Alpha           |      0.5       |\n",
      "|         Episodes         |      1000      |\n",
      "| Execution Time (seconds) |      0.07      |\n",
      "|      Success Count       |      997       |\n",
      "+--------------------------+----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 13 | reward = 100  |\n",
      "| Min reward | Episode 0  | reward = -244 |\n",
      "| Max steps  | Episode 0  |  steps = 100  |\n",
      "| Min steps  | Episode 13 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        93.969         |       17.25467364984811       |      6.098      |    6.570964432529945     |      0.997      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "QLearningAgent - 0 Obstacles - 0.01 Epsilon - 0.6 Gamma - 0.9 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+----------------+\n",
      "|        Parameter         |     Value      |\n",
      "+--------------------------+----------------+\n",
      "|          Agent           | QLearningAgent |\n",
      "|        Obstacles         |       0        |\n",
      "|         Epsilon          |      0.01      |\n",
      "|          Gamma           |      0.6       |\n",
      "|          Alpha           |      0.9       |\n",
      "|         Episodes         |      1000      |\n",
      "| Execution Time (seconds) |      0.07      |\n",
      "|      Success Count       |      1000      |\n",
      "+--------------------------+----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-----------+---------------+\n",
      "|   Metric   |  Episode  |     Value     |\n",
      "+------------+-----------+---------------+\n",
      "| Max reward | Episode 7 | reward = 100  |\n",
      "| Min reward | Episode 1 | reward = -147 |\n",
      "| Max steps  | Episode 2 |  steps = 87   |\n",
      "| Min steps  | Episode 7 |   steps = 1   |\n",
      "+------------+-----------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|         94.82         |      10.344206233568752       |      5.613      |    5.021797432140155     |       1.0       |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "QLearningAgent - 0 Obstacles - 0.01 Epsilon - 0.99 Gamma - 0.1 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+----------------+\n",
      "|        Parameter         |     Value      |\n",
      "+--------------------------+----------------+\n",
      "|          Agent           | QLearningAgent |\n",
      "|        Obstacles         |       0        |\n",
      "|         Epsilon          |      0.01      |\n",
      "|          Gamma           |      0.99      |\n",
      "|          Alpha           |      0.1       |\n",
      "|         Episodes         |      1000      |\n",
      "| Execution Time (seconds) |      0.08      |\n",
      "|      Success Count       |      990       |\n",
      "+--------------------------+----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 50 | reward = 100  |\n",
      "| Min reward | Episode 1  | reward = -226 |\n",
      "| Max steps  | Episode 0  |  steps = 100  |\n",
      "| Min steps  | Episode 50 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        92.243         |      25.091862277490538       |      6.91       |    10.539971109589958    |      0.99       |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "QLearningAgent - 0 Obstacles - 0.01 Epsilon - 0.99 Gamma - 0.5 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+----------------+\n",
      "|        Parameter         |     Value      |\n",
      "+--------------------------+----------------+\n",
      "|          Agent           | QLearningAgent |\n",
      "|        Obstacles         |       0        |\n",
      "|         Epsilon          |      0.01      |\n",
      "|          Gamma           |      0.99      |\n",
      "|          Alpha           |      0.5       |\n",
      "|         Episodes         |      1000      |\n",
      "| Execution Time (seconds) |      0.08      |\n",
      "|      Success Count       |      999       |\n",
      "+--------------------------+----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 59 | reward = 100  |\n",
      "| Min reward | Episode 1  | reward = -235 |\n",
      "| Max steps  | Episode 1  |  steps = 100  |\n",
      "| Min steps  | Episode 59 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|         94.18         |      12.157606977008564       |      6.251      |    5.108825608601169     |      0.999      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "QLearningAgent - 0 Obstacles - 0.01 Epsilon - 0.99 Gamma - 0.9 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+----------------+\n",
      "|        Parameter         |     Value      |\n",
      "+--------------------------+----------------+\n",
      "|          Agent           | QLearningAgent |\n",
      "|        Obstacles         |       0        |\n",
      "|         Epsilon          |      0.01      |\n",
      "|          Gamma           |      0.99      |\n",
      "|          Alpha           |      0.9       |\n",
      "|         Episodes         |      1000      |\n",
      "| Execution Time (seconds) |      0.07      |\n",
      "|      Success Count       |      1000      |\n",
      "+--------------------------+----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+--------------+\n",
      "|   Metric   |  Episode   |    Value     |\n",
      "+------------+------------+--------------+\n",
      "| Max reward | Episode 42 | reward = 100 |\n",
      "| Min reward | Episode 0  | reward = -53 |\n",
      "| Max steps  | Episode 2  |  steps = 72  |\n",
      "| Min steps  | Episode 42 |  steps = 1   |\n",
      "+------------+------------+--------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        94.664         |       8.542720999897051       |      5.796      |    4.711962073975829     |       1.0       |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "QLearningAgent - 0 Obstacles - 0.01 Epsilon - 0.1 Gamma - 0.1 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+----------------+\n",
      "|        Parameter         |     Value      |\n",
      "+--------------------------+----------------+\n",
      "|          Agent           | QLearningAgent |\n",
      "|        Obstacles         |       0        |\n",
      "|         Epsilon          |      0.01      |\n",
      "|          Gamma           |      0.1       |\n",
      "|          Alpha           |      0.1       |\n",
      "|         Episodes         |      1000      |\n",
      "| Execution Time (seconds) |      0.12      |\n",
      "|      Success Count       |      993       |\n",
      "+--------------------------+----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 18 | reward = 100  |\n",
      "| Min reward | Episode 1  | reward = -280 |\n",
      "| Max steps  | Episode 0  |  steps = 100  |\n",
      "| Min steps  | Episode 18 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        85.776         |      33.699903672080545       |     10.188      |    13.929705405811449    |      0.993      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "QLearningAgent - 0 Obstacles - 0.01 Epsilon - 0.1 Gamma - 0.5 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+----------------+\n",
      "|        Parameter         |     Value      |\n",
      "+--------------------------+----------------+\n",
      "|          Agent           | QLearningAgent |\n",
      "|        Obstacles         |       0        |\n",
      "|         Epsilon          |      0.01      |\n",
      "|          Gamma           |      0.1       |\n",
      "|          Alpha           |      0.5       |\n",
      "|         Episodes         |      1000      |\n",
      "| Execution Time (seconds) |      0.09      |\n",
      "|      Success Count       |      997       |\n",
      "+--------------------------+----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 26 | reward = 100  |\n",
      "| Min reward | Episode 0  | reward = -298 |\n",
      "| Max steps  | Episode 0  |  steps = 100  |\n",
      "| Min steps  | Episode 26 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        91.583         |      22.742346521969214       |      6.72       |    7.931488416968691     |      0.997      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "QLearningAgent - 0 Obstacles - 0.01 Epsilon - 0.1 Gamma - 0.9 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+----------------+\n",
      "|        Parameter         |     Value      |\n",
      "+--------------------------+----------------+\n",
      "|          Agent           | QLearningAgent |\n",
      "|        Obstacles         |       0        |\n",
      "|         Epsilon          |      0.01      |\n",
      "|          Gamma           |      0.1       |\n",
      "|          Alpha           |      0.9       |\n",
      "|         Episodes         |      1000      |\n",
      "| Execution Time (seconds) |      0.08      |\n",
      "|      Success Count       |      999       |\n",
      "+--------------------------+----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-----------+---------------+\n",
      "|   Metric   |  Episode  |     Value     |\n",
      "+------------+-----------+---------------+\n",
      "| Max reward | Episode 8 | reward = 100  |\n",
      "| Min reward | Episode 0 | reward = -289 |\n",
      "| Max steps  | Episode 0 |  steps = 100  |\n",
      "| Min steps  | Episode 8 |   steps = 1   |\n",
      "+------------+-----------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        92.158         |      17.029475594951215       |      6.581      |    6.2680720901827325    |      0.999      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "QLearningAgent - 0 Obstacles - 0.01 Epsilon - 0.6 Gamma - 0.1 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+----------------+\n",
      "|        Parameter         |     Value      |\n",
      "+--------------------------+----------------+\n",
      "|          Agent           | QLearningAgent |\n",
      "|        Obstacles         |       0        |\n",
      "|         Epsilon          |      0.01      |\n",
      "|          Gamma           |      0.6       |\n",
      "|          Alpha           |      0.1       |\n",
      "|         Episodes         |      1000      |\n",
      "| Execution Time (seconds) |      0.14      |\n",
      "|      Success Count       |      994       |\n",
      "+--------------------------+----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 22 | reward = 100  |\n",
      "| Min reward | Episode 1  | reward = -271 |\n",
      "| Max steps  | Episode 1  |  steps = 100  |\n",
      "| Min steps  | Episode 22 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        89.938         |       28.02601672756251       |      7.495      |    9.776889206826604     |      0.994      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "QLearningAgent - 0 Obstacles - 0.01 Epsilon - 0.6 Gamma - 0.5 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+----------------+\n",
      "|        Parameter         |     Value      |\n",
      "+--------------------------+----------------+\n",
      "|          Agent           | QLearningAgent |\n",
      "|        Obstacles         |       0        |\n",
      "|         Epsilon          |      0.01      |\n",
      "|          Gamma           |      0.6       |\n",
      "|          Alpha           |      0.5       |\n",
      "|         Episodes         |      1000      |\n",
      "| Execution Time (seconds) |      0.14      |\n",
      "|      Success Count       |      1000      |\n",
      "+--------------------------+----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-----------+---------------+\n",
      "|   Metric   |  Episode  |     Value     |\n",
      "+------------+-----------+---------------+\n",
      "| Max reward | Episode 4 | reward = 100  |\n",
      "| Min reward | Episode 5 | reward = -112 |\n",
      "| Max steps  | Episode 5 |  steps = 87   |\n",
      "| Min steps  | Episode 4 |   steps = 1   |\n",
      "+------------+-----------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        92.409         |      12.941684828731002       |      6.449      |    5.650958532172425     |       1.0       |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "QLearningAgent - 0 Obstacles - 0.01 Epsilon - 0.6 Gamma - 0.9 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+----------------+\n",
      "|        Parameter         |     Value      |\n",
      "+--------------------------+----------------+\n",
      "|          Agent           | QLearningAgent |\n",
      "|        Obstacles         |       0        |\n",
      "|         Epsilon          |      0.01      |\n",
      "|          Gamma           |      0.6       |\n",
      "|          Alpha           |      0.9       |\n",
      "|         Episodes         |      1000      |\n",
      "| Execution Time (seconds) |      0.39      |\n",
      "|      Success Count       |      999       |\n",
      "+--------------------------+----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 17 | reward = 100  |\n",
      "| Min reward | Episode 3  | reward = -179 |\n",
      "| Max steps  | Episode 3  |  steps = 100  |\n",
      "| Min steps  | Episode 17 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|         92.05         |       16.35882364692251       |      6.574      |    6.191515069743339     |      0.999      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "QLearningAgent - 0 Obstacles - 0.01 Epsilon - 0.99 Gamma - 0.1 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+----------------+\n",
      "|        Parameter         |     Value      |\n",
      "+--------------------------+----------------+\n",
      "|          Agent           | QLearningAgent |\n",
      "|        Obstacles         |       0        |\n",
      "|         Epsilon          |      0.01      |\n",
      "|          Gamma           |      0.99      |\n",
      "|          Alpha           |      0.1       |\n",
      "|         Episodes         |      1000      |\n",
      "| Execution Time (seconds) |      0.13      |\n",
      "|      Success Count       |      998       |\n",
      "+--------------------------+----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 23 | reward = 100  |\n",
      "| Min reward | Episode 0  | reward = -208 |\n",
      "| Max steps  | Episode 0  |  steps = 100  |\n",
      "| Min steps  | Episode 23 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        91.709         |      20.111160527870535       |      7.001      |    8.790123165534443     |      0.998      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "QLearningAgent - 0 Obstacles - 0.01 Epsilon - 0.99 Gamma - 0.5 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+----------------+\n",
      "|        Parameter         |     Value      |\n",
      "+--------------------------+----------------+\n",
      "|          Agent           | QLearningAgent |\n",
      "|        Obstacles         |       0        |\n",
      "|         Epsilon          |      0.01      |\n",
      "|          Gamma           |      0.99      |\n",
      "|          Alpha           |      0.5       |\n",
      "|         Episodes         |      1000      |\n",
      "| Execution Time (seconds) |      0.09      |\n",
      "|      Success Count       |      999       |\n",
      "+--------------------------+----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 15 | reward = 100  |\n",
      "| Min reward | Episode 2  | reward = -235 |\n",
      "| Max steps  | Episode 2  |  steps = 100  |\n",
      "| Min steps  | Episode 15 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        92.307         |      15.909363873482146       |      6.711      |    5.770682782806536     |      0.999      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "QLearningAgent - 0 Obstacles - 0.01 Epsilon - 0.99 Gamma - 0.9 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+----------------+\n",
      "|        Parameter         |     Value      |\n",
      "+--------------------------+----------------+\n",
      "|          Agent           | QLearningAgent |\n",
      "|        Obstacles         |       0        |\n",
      "|         Epsilon          |      0.01      |\n",
      "|          Gamma           |      0.99      |\n",
      "|          Alpha           |      0.9       |\n",
      "|         Episodes         |      1000      |\n",
      "| Execution Time (seconds) |      0.12      |\n",
      "|      Success Count       |      997       |\n",
      "+--------------------------+----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 11 | reward = 100  |\n",
      "| Min reward | Episode 0  | reward = -244 |\n",
      "| Max steps  | Episode 0  |  steps = 100  |\n",
      "| Min steps  | Episode 11 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        92.284         |      19.441638853793197       |      6.577      |    6.370294471345233     |      0.997      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "QLearningAgent - 0 Obstacles - 0.01 Epsilon - 0.1 Gamma - 0.1 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+----------------+\n",
      "|        Parameter         |     Value      |\n",
      "+--------------------------+----------------+\n",
      "|          Agent           | QLearningAgent |\n",
      "|        Obstacles         |       0        |\n",
      "|         Epsilon          |      0.01      |\n",
      "|          Gamma           |      0.1       |\n",
      "|          Alpha           |      0.1       |\n",
      "|         Episodes         |      1000      |\n",
      "| Execution Time (seconds) |      0.15      |\n",
      "|      Success Count       |      993       |\n",
      "+--------------------------+----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 3  | reward = 100  |\n",
      "| Min reward | Episode 68 | reward = -253 |\n",
      "| Max steps  | Episode 10 |  steps = 100  |\n",
      "| Min steps  | Episode 3  |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        85.524         |       32.34011916037582       |     10.386      |    13.721562785823096    |      0.993      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "QLearningAgent - 0 Obstacles - 0.01 Epsilon - 0.1 Gamma - 0.5 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+----------------+\n",
      "|        Parameter         |     Value      |\n",
      "+--------------------------+----------------+\n",
      "|          Agent           | QLearningAgent |\n",
      "|        Obstacles         |       0        |\n",
      "|         Epsilon          |      0.01      |\n",
      "|          Gamma           |      0.1       |\n",
      "|          Alpha           |      0.5       |\n",
      "|         Episodes         |      1000      |\n",
      "| Execution Time (seconds) |      0.11      |\n",
      "|      Success Count       |      999       |\n",
      "+--------------------------+----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 31 | reward = 100  |\n",
      "| Min reward | Episode 0  | reward = -253 |\n",
      "| Max steps  | Episode 0  |  steps = 100  |\n",
      "| Min steps  | Episode 31 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        90.708         |      21.249382868033422       |      7.014      |    8.096750049332224     |      0.999      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "QLearningAgent - 0 Obstacles - 0.01 Epsilon - 0.1 Gamma - 0.9 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+----------------+\n",
      "|        Parameter         |     Value      |\n",
      "+--------------------------+----------------+\n",
      "|          Agent           | QLearningAgent |\n",
      "|        Obstacles         |       0        |\n",
      "|         Epsilon          |      0.01      |\n",
      "|          Gamma           |      0.1       |\n",
      "|          Alpha           |      0.9       |\n",
      "|         Episodes         |      1000      |\n",
      "| Execution Time (seconds) |      0.10      |\n",
      "|      Success Count       |      997       |\n",
      "+--------------------------+----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-----------+---------------+\n",
      "|   Metric   |  Episode  |     Value     |\n",
      "+------------+-----------+---------------+\n",
      "| Max reward | Episode 3 | reward = 100  |\n",
      "| Min reward | Episode 0 | reward = -280 |\n",
      "| Max steps  | Episode 0 |  steps = 100  |\n",
      "| Min steps  | Episode 3 |   steps = 1   |\n",
      "+------------+-----------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        91.585         |       22.13229797408334       |      6.628      |    6.942752899089655     |      0.997      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "QLearningAgent - 0 Obstacles - 0.01 Epsilon - 0.6 Gamma - 0.1 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+----------------+\n",
      "|        Parameter         |     Value      |\n",
      "+--------------------------+----------------+\n",
      "|          Agent           | QLearningAgent |\n",
      "|        Obstacles         |       0        |\n",
      "|         Epsilon          |      0.01      |\n",
      "|          Gamma           |      0.6       |\n",
      "|          Alpha           |      0.1       |\n",
      "|         Episodes         |      1000      |\n",
      "| Execution Time (seconds) |      0.11      |\n",
      "|      Success Count       |      997       |\n",
      "+--------------------------+----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 16 | reward = 100  |\n",
      "| Min reward | Episode 0  | reward = -289 |\n",
      "| Max steps  | Episode 0  |  steps = 100  |\n",
      "| Min steps  | Episode 16 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        90.217         |       25.36369902494958       |      7.33       |     9.44490901524765     |      0.997      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "QLearningAgent - 0 Obstacles - 0.01 Epsilon - 0.6 Gamma - 0.5 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+----------------+\n",
      "|        Parameter         |     Value      |\n",
      "+--------------------------+----------------+\n",
      "|          Agent           | QLearningAgent |\n",
      "|        Obstacles         |       0        |\n",
      "|         Epsilon          |      0.01      |\n",
      "|          Gamma           |      0.6       |\n",
      "|          Alpha           |      0.5       |\n",
      "|         Episodes         |      1000      |\n",
      "| Execution Time (seconds) |      0.09      |\n",
      "|      Success Count       |      999       |\n",
      "+--------------------------+----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 55 | reward = 100  |\n",
      "| Min reward | Episode 5  | reward = -262 |\n",
      "| Max steps  | Episode 5  |  steps = 100  |\n",
      "| Min steps  | Episode 55 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        91.473         |       19.48822880772037       |      6.717      |    7.105166739380181     |      0.999      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "QLearningAgent - 0 Obstacles - 0.01 Epsilon - 0.6 Gamma - 0.9 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+----------------+\n",
      "|        Parameter         |     Value      |\n",
      "+--------------------------+----------------+\n",
      "|          Agent           | QLearningAgent |\n",
      "|        Obstacles         |       0        |\n",
      "|         Epsilon          |      0.01      |\n",
      "|          Gamma           |      0.6       |\n",
      "|          Alpha           |      0.9       |\n",
      "|         Episodes         |      1000      |\n",
      "| Execution Time (seconds) |      0.09      |\n",
      "|      Success Count       |      998       |\n",
      "+--------------------------+----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 11 | reward = 100  |\n",
      "| Min reward | Episode 0  | reward = -343 |\n",
      "| Max steps  | Episode 0  |  steps = 100  |\n",
      "| Min steps  | Episode 11 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        91.173         |      22.583469265905187       |      6.736      |    6.612263305860258     |      0.998      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "QLearningAgent - 0 Obstacles - 0.01 Epsilon - 0.99 Gamma - 0.1 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+----------------+\n",
      "|        Parameter         |     Value      |\n",
      "+--------------------------+----------------+\n",
      "|          Agent           | QLearningAgent |\n",
      "|        Obstacles         |       0        |\n",
      "|         Epsilon          |      0.01      |\n",
      "|          Gamma           |      0.99      |\n",
      "|          Alpha           |      0.1       |\n",
      "|         Episodes         |      1000      |\n",
      "| Execution Time (seconds) |      0.09      |\n",
      "|      Success Count       |      998       |\n",
      "+--------------------------+----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 18 | reward = 100  |\n",
      "| Min reward | Episode 1  | reward = -208 |\n",
      "| Max steps  | Episode 1  |  steps = 100  |\n",
      "| Min steps  | Episode 18 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        91.198         |       19.37669353249548       |      6.864      |    7.440084600248768     |      0.998      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "QLearningAgent - 0 Obstacles - 0.01 Epsilon - 0.99 Gamma - 0.5 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+----------------+\n",
      "|        Parameter         |     Value      |\n",
      "+--------------------------+----------------+\n",
      "|          Agent           | QLearningAgent |\n",
      "|        Obstacles         |       0        |\n",
      "|         Epsilon          |      0.01      |\n",
      "|          Gamma           |      0.99      |\n",
      "|          Alpha           |      0.5       |\n",
      "|         Episodes         |      1000      |\n",
      "| Execution Time (seconds) |      0.09      |\n",
      "|      Success Count       |      998       |\n",
      "+--------------------------+----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 12 | reward = 100  |\n",
      "| Min reward | Episode 0  | reward = -307 |\n",
      "| Max steps  | Episode 0  |  steps = 100  |\n",
      "| Min steps  | Episode 12 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        91.086         |       23.40077349576719       |      6.796      |    7.123421026945349     |      0.998      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "QLearningAgent - 0 Obstacles - 0.01 Epsilon - 0.99 Gamma - 0.9 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+----------------+\n",
      "|        Parameter         |     Value      |\n",
      "+--------------------------+----------------+\n",
      "|          Agent           | QLearningAgent |\n",
      "|        Obstacles         |       0        |\n",
      "|         Epsilon          |      0.01      |\n",
      "|          Gamma           |      0.99      |\n",
      "|          Alpha           |      0.9       |\n",
      "|         Episodes         |      1000      |\n",
      "| Execution Time (seconds) |      0.09      |\n",
      "|      Success Count       |      999       |\n",
      "+--------------------------+----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-----------+---------------+\n",
      "|   Metric   |  Episode  |     Value     |\n",
      "+------------+-----------+---------------+\n",
      "| Max reward | Episode 2 | reward = 100  |\n",
      "| Min reward | Episode 5 | reward = -280 |\n",
      "| Max steps  | Episode 5 |  steps = 100  |\n",
      "| Min steps  | Episode 2 |   steps = 1   |\n",
      "+------------+-----------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        92.309         |      17.651376436133532       |      6.403      |    6.575091088557086     |      0.999      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "QLearningAgent - 2 Obstacles - 0.01 Epsilon - 0.1 Gamma - 0.1 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+----------------+\n",
      "|        Parameter         |     Value      |\n",
      "+--------------------------+----------------+\n",
      "|          Agent           | QLearningAgent |\n",
      "|        Obstacles         |       2        |\n",
      "|         Epsilon          |      0.01      |\n",
      "|          Gamma           |      0.1       |\n",
      "|          Alpha           |      0.1       |\n",
      "|         Episodes         |      1000      |\n",
      "| Execution Time (seconds) |      0.76      |\n",
      "|      Success Count       |      200       |\n",
      "+--------------------------+----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 25 | reward = 100  |\n",
      "| Min reward | Episode 6  | reward = -208 |\n",
      "| Max steps  | Episode 0  |  steps = 100  |\n",
      "| Min steps  | Episode 25 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        -70.546        |       79.76754192492547       |     82.756      |    36.20758834075328     |       0.2       |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "QLearningAgent - 2 Obstacles - 0.01 Epsilon - 0.1 Gamma - 0.5 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+----------------+\n",
      "|        Parameter         |     Value      |\n",
      "+--------------------------+----------------+\n",
      "|          Agent           | QLearningAgent |\n",
      "|        Obstacles         |       2        |\n",
      "|         Epsilon          |      0.01      |\n",
      "|          Gamma           |      0.1       |\n",
      "|          Alpha           |      0.5       |\n",
      "|         Episodes         |      1000      |\n",
      "| Execution Time (seconds) |      0.60      |\n",
      "|      Success Count       |      266       |\n",
      "+--------------------------+----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-------------+----------------+\n",
      "|   Metric   |   Episode   |     Value      |\n",
      "+------------+-------------+----------------+\n",
      "| Max reward | Episode 80  |  reward = 100  |\n",
      "| Min reward | Episode 670 | reward = -1000 |\n",
      "| Max steps  | Episode 28  |  steps = 100   |\n",
      "| Min steps  | Episode 80  |   steps = 1    |\n",
      "+------------+-------------+----------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        -57.398        |       94.10763966779659       |     76.335      |    40.74151010512062     |      0.266      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "QLearningAgent - 2 Obstacles - 0.01 Epsilon - 0.1 Gamma - 0.9 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+----------------+\n",
      "|        Parameter         |     Value      |\n",
      "+--------------------------+----------------+\n",
      "|          Agent           | QLearningAgent |\n",
      "|        Obstacles         |       2        |\n",
      "|         Epsilon          |      0.01      |\n",
      "|          Gamma           |      0.1       |\n",
      "|          Alpha           |      0.9       |\n",
      "|         Episodes         |      1000      |\n",
      "| Execution Time (seconds) |      0.78      |\n",
      "|      Success Count       |      255       |\n",
      "+--------------------------+----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 13 | reward = 100  |\n",
      "| Min reward | Episode 44 | reward = -199 |\n",
      "| Max steps  | Episode 27 |  steps = 100  |\n",
      "| Min steps  | Episode 13 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        -58.139        |       87.38287845136739       |     76.955      |    40.50463677902322     |      0.255      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "QLearningAgent - 2 Obstacles - 0.01 Epsilon - 0.6 Gamma - 0.1 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+----------------+\n",
      "|        Parameter         |     Value      |\n",
      "+--------------------------+----------------+\n",
      "|          Agent           | QLearningAgent |\n",
      "|        Obstacles         |       2        |\n",
      "|         Epsilon          |      0.01      |\n",
      "|          Gamma           |      0.6       |\n",
      "|          Alpha           |      0.1       |\n",
      "|         Episodes         |      1000      |\n",
      "| Execution Time (seconds) |      0.75      |\n",
      "|      Success Count       |      388       |\n",
      "+--------------------------+----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-------------+---------------+\n",
      "|   Metric   |   Episode   |     Value     |\n",
      "+------------+-------------+---------------+\n",
      "| Max reward | Episode 42  | reward = 100  |\n",
      "| Min reward | Episode 318 | reward = -388 |\n",
      "| Max steps  |  Episode 0  |  steps = 100  |\n",
      "| Min steps  | Episode 42  |   steps = 1   |\n",
      "+------------+-------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        -36.502        |      101.33149593323918       |     65.619      |    44.97491057891702     |      0.388      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "QLearningAgent - 2 Obstacles - 0.01 Epsilon - 0.6 Gamma - 0.5 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+----------------+\n",
      "|        Parameter         |     Value      |\n",
      "+--------------------------+----------------+\n",
      "|          Agent           | QLearningAgent |\n",
      "|        Obstacles         |       2        |\n",
      "|         Epsilon          |      0.01      |\n",
      "|          Gamma           |      0.6       |\n",
      "|          Alpha           |      0.5       |\n",
      "|         Episodes         |      1000      |\n",
      "| Execution Time (seconds) |      0.63      |\n",
      "|      Success Count       |      345       |\n",
      "+--------------------------+----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-----------+---------------+\n",
      "|   Metric   |  Episode  |     Value     |\n",
      "+------------+-----------+---------------+\n",
      "| Max reward | Episode 3 | reward = 100  |\n",
      "| Min reward | Episode 0 | reward = -244 |\n",
      "| Max steps  | Episode 0 |  steps = 100  |\n",
      "| Min steps  | Episode 3 |   steps = 1   |\n",
      "+------------+-----------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        -41.578        |       95.05940356538225       |     69.142      |    44.094400462554795    |      0.345      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "QLearningAgent - 2 Obstacles - 0.01 Epsilon - 0.6 Gamma - 0.9 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+----------------+\n",
      "|        Parameter         |     Value      |\n",
      "+--------------------------+----------------+\n",
      "|          Agent           | QLearningAgent |\n",
      "|        Obstacles         |       2        |\n",
      "|         Epsilon          |      0.01      |\n",
      "|          Gamma           |      0.6       |\n",
      "|          Alpha           |      0.9       |\n",
      "|         Episodes         |      1000      |\n",
      "| Execution Time (seconds) |      0.73      |\n",
      "|      Success Count       |      264       |\n",
      "+--------------------------+----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-------------+----------------+\n",
      "|   Metric   |   Episode   |     Value      |\n",
      "+------------+-------------+----------------+\n",
      "| Max reward |  Episode 5  |  reward = 100  |\n",
      "| Min reward | Episode 488 | reward = -1000 |\n",
      "| Max steps  |  Episode 8  |  steps = 100   |\n",
      "| Min steps  |  Episode 5  |   steps = 1    |\n",
      "+------------+-------------+----------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        -58.646        |       93.22564939814421       |     76.607      |    40.472392634554794    |      0.264      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "QLearningAgent - 2 Obstacles - 0.01 Epsilon - 0.99 Gamma - 0.1 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+----------------+\n",
      "|        Parameter         |     Value      |\n",
      "+--------------------------+----------------+\n",
      "|          Agent           | QLearningAgent |\n",
      "|        Obstacles         |       2        |\n",
      "|         Epsilon          |      0.01      |\n",
      "|          Gamma           |      0.99      |\n",
      "|          Alpha           |      0.1       |\n",
      "|         Episodes         |      1000      |\n",
      "| Execution Time (seconds) |      0.28      |\n",
      "|      Success Count       |      811       |\n",
      "+--------------------------+----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-------------+----------------+\n",
      "|   Metric   |   Episode   |     Value      |\n",
      "+------------+-------------+----------------+\n",
      "| Max reward |  Episode 8  |  reward = 100  |\n",
      "| Min reward | Episode 322 | reward = -1000 |\n",
      "| Max steps  |  Episode 0  |  steps = 100   |\n",
      "| Min steps  |  Episode 8  |   steps = 1    |\n",
      "+------------+-------------+----------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        -11.641        |      193.36168405312327       |     33.155      |    38.26966870176096     |      0.811      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "QLearningAgent - 2 Obstacles - 0.01 Epsilon - 0.99 Gamma - 0.5 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+----------------+\n",
      "|        Parameter         |     Value      |\n",
      "+--------------------------+----------------+\n",
      "|          Agent           | QLearningAgent |\n",
      "|        Obstacles         |       2        |\n",
      "|         Epsilon          |      0.01      |\n",
      "|          Gamma           |      0.99      |\n",
      "|          Alpha           |      0.5       |\n",
      "|         Episodes         |      1000      |\n",
      "| Execution Time (seconds) |      0.28      |\n",
      "|      Success Count       |      822       |\n",
      "+--------------------------+----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-------------+----------------+\n",
      "|   Metric   |   Episode   |     Value      |\n",
      "+------------+-------------+----------------+\n",
      "| Max reward | Episode 23  |  reward = 100  |\n",
      "| Min reward | Episode 908 | reward = -1000 |\n",
      "| Max steps  |  Episode 2  |  steps = 100   |\n",
      "| Min steps  | Episode 23  |   steps = 1    |\n",
      "+------------+-------------+----------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        25.205         |      118.04474379038032       |     29.651      |    37.540997929270944    |      0.822      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "QLearningAgent - 2 Obstacles - 0.01 Epsilon - 0.99 Gamma - 0.9 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+----------------+\n",
      "|        Parameter         |     Value      |\n",
      "+--------------------------+----------------+\n",
      "|          Agent           | QLearningAgent |\n",
      "|        Obstacles         |       2        |\n",
      "|         Epsilon          |      0.01      |\n",
      "|          Gamma           |      0.99      |\n",
      "|          Alpha           |      0.9       |\n",
      "|         Episodes         |      1000      |\n",
      "| Execution Time (seconds) |      0.33      |\n",
      "|      Success Count       |      733       |\n",
      "+--------------------------+----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-------------+---------------+\n",
      "|   Metric   |   Episode   |     Value     |\n",
      "+------------+-------------+---------------+\n",
      "| Max reward |  Episode 4  | reward = 100  |\n",
      "| Min reward | Episode 708 | reward = -361 |\n",
      "| Max steps  |  Episode 0  |  steps = 100  |\n",
      "| Min steps  |  Episode 4  |   steps = 1   |\n",
      "+------------+-------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|         8.88          |      116.81328239092907       |     38.677      |    41.84879918221063     |      0.733      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "QLearningAgent - 2 Obstacles - 0.01 Epsilon - 0.1 Gamma - 0.1 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+----------------+\n",
      "|        Parameter         |     Value      |\n",
      "+--------------------------+----------------+\n",
      "|          Agent           | QLearningAgent |\n",
      "|        Obstacles         |       2        |\n",
      "|         Epsilon          |      0.01      |\n",
      "|          Gamma           |      0.1       |\n",
      "|          Alpha           |      0.1       |\n",
      "|         Episodes         |      1000      |\n",
      "| Execution Time (seconds) |      0.65      |\n",
      "|      Success Count       |      327       |\n",
      "+--------------------------+----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 11 | reward = 100  |\n",
      "| Min reward | Episode 40 | reward = -298 |\n",
      "| Max steps  | Episode 4  |  steps = 100  |\n",
      "| Min steps  | Episode 11 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        -60.691        |       93.82779531766786       |     74.937      |    39.420413493518815    |      0.327      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "QLearningAgent - 2 Obstacles - 0.01 Epsilon - 0.1 Gamma - 0.5 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+----------------+\n",
      "|        Parameter         |     Value      |\n",
      "+--------------------------+----------------+\n",
      "|          Agent           | QLearningAgent |\n",
      "|        Obstacles         |       2        |\n",
      "|         Epsilon          |      0.01      |\n",
      "|          Gamma           |      0.1       |\n",
      "|          Alpha           |      0.5       |\n",
      "|         Episodes         |      1000      |\n",
      "| Execution Time (seconds) |      0.62      |\n",
      "|      Success Count       |      295       |\n",
      "+--------------------------+----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-------------+----------------+\n",
      "|   Metric   |   Episode   |     Value      |\n",
      "+------------+-------------+----------------+\n",
      "| Max reward | Episode 10  |  reward = 100  |\n",
      "| Min reward | Episode 334 | reward = -1000 |\n",
      "| Max steps  |  Episode 1  |  steps = 100   |\n",
      "| Min steps  | Episode 10  |   steps = 1    |\n",
      "+------------+-------------+----------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        -65.043        |      105.97825504102973       |     73.951      |    41.869148840460376    |      0.295      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "QLearningAgent - 2 Obstacles - 0.01 Epsilon - 0.1 Gamma - 0.9 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+----------------+\n",
      "|        Parameter         |     Value      |\n",
      "+--------------------------+----------------+\n",
      "|          Agent           | QLearningAgent |\n",
      "|        Obstacles         |       2        |\n",
      "|         Epsilon          |      0.01      |\n",
      "|          Gamma           |      0.1       |\n",
      "|          Alpha           |      0.9       |\n",
      "|         Episodes         |      1000      |\n",
      "| Execution Time (seconds) |      0.65      |\n",
      "|      Success Count       |      370       |\n",
      "+--------------------------+----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 69 | reward = 100  |\n",
      "| Min reward | Episode 19 | reward = -352 |\n",
      "| Max steps  | Episode 0  |  steps = 100  |\n",
      "| Min steps  | Episode 69 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        -47.413        |       102.0236406635945       |     68.097      |    43.53880129514612     |      0.37       |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "QLearningAgent - 2 Obstacles - 0.01 Epsilon - 0.6 Gamma - 0.1 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+----------------+\n",
      "|        Parameter         |     Value      |\n",
      "+--------------------------+----------------+\n",
      "|          Agent           | QLearningAgent |\n",
      "|        Obstacles         |       2        |\n",
      "|         Epsilon          |      0.01      |\n",
      "|          Gamma           |      0.6       |\n",
      "|          Alpha           |      0.1       |\n",
      "|         Episodes         |      1000      |\n",
      "| Execution Time (seconds) |      0.44      |\n",
      "|      Success Count       |      566       |\n",
      "+--------------------------+----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-------------+---------------+\n",
      "|   Metric   |   Episode   |     Value     |\n",
      "+------------+-------------+---------------+\n",
      "| Max reward | Episode 37  | reward = 100  |\n",
      "| Min reward | Episode 481 | reward = -667 |\n",
      "| Max steps  |  Episode 3  |  steps = 100  |\n",
      "| Min steps  | Episode 37  |   steps = 1   |\n",
      "+------------+-------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        -14.642        |       111.8440025969963       |     52.757      |    44.860477186644076    |      0.566      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "QLearningAgent - 2 Obstacles - 0.01 Epsilon - 0.6 Gamma - 0.5 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+----------------+\n",
      "|        Parameter         |     Value      |\n",
      "+--------------------------+----------------+\n",
      "|          Agent           | QLearningAgent |\n",
      "|        Obstacles         |       2        |\n",
      "|         Epsilon          |      0.01      |\n",
      "|          Gamma           |      0.6       |\n",
      "|          Alpha           |      0.5       |\n",
      "|         Episodes         |      1000      |\n",
      "| Execution Time (seconds) |      0.50      |\n",
      "|      Success Count       |      520       |\n",
      "+--------------------------+----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-------------+---------------+\n",
      "|   Metric   |   Episode   |     Value     |\n",
      "+------------+-------------+---------------+\n",
      "| Max reward | Episode 26  | reward = 100  |\n",
      "| Min reward | Episode 146 | reward = -316 |\n",
      "| Max steps  |  Episode 0  |  steps = 100  |\n",
      "| Min steps  | Episode 26  |   steps = 1   |\n",
      "+------------+-------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        -16.107        |      102.70292811036325       |     54.796      |     45.5947723677536     |      0.52       |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "QLearningAgent - 2 Obstacles - 0.01 Epsilon - 0.6 Gamma - 0.9 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+----------------+\n",
      "|        Parameter         |     Value      |\n",
      "+--------------------------+----------------+\n",
      "|          Agent           | QLearningAgent |\n",
      "|        Obstacles         |       2        |\n",
      "|         Epsilon          |      0.01      |\n",
      "|          Gamma           |      0.6       |\n",
      "|          Alpha           |      0.9       |\n",
      "|         Episodes         |      1000      |\n",
      "| Execution Time (seconds) |      0.50      |\n",
      "|      Success Count       |      475       |\n",
      "+--------------------------+----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-------------+---------------+\n",
      "|   Metric   |   Episode   |     Value     |\n",
      "+------------+-------------+---------------+\n",
      "| Max reward | Episode 16  | reward = 100  |\n",
      "| Min reward | Episode 156 | reward = -271 |\n",
      "| Max steps  |  Episode 1  |  steps = 100  |\n",
      "| Min steps  | Episode 16  |   steps = 1   |\n",
      "+------------+-------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        -25.661        |       99.39445324733335       |     60.228      |    44.367583870417135    |      0.475      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "QLearningAgent - 2 Obstacles - 0.01 Epsilon - 0.99 Gamma - 0.1 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+----------------+\n",
      "|        Parameter         |     Value      |\n",
      "+--------------------------+----------------+\n",
      "|          Agent           | QLearningAgent |\n",
      "|        Obstacles         |       2        |\n",
      "|         Epsilon          |      0.01      |\n",
      "|          Gamma           |      0.99      |\n",
      "|          Alpha           |      0.1       |\n",
      "|         Episodes         |      1000      |\n",
      "| Execution Time (seconds) |      0.24      |\n",
      "|      Success Count       |      887       |\n",
      "+--------------------------+----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-------------+----------------+\n",
      "|   Metric   |   Episode   |     Value      |\n",
      "+------------+-------------+----------------+\n",
      "| Max reward |  Episode 7  |  reward = 100  |\n",
      "| Min reward | Episode 165 | reward = -1000 |\n",
      "| Max steps  |  Episode 0  |  steps = 100   |\n",
      "| Min steps  |  Episode 7  |   steps = 1    |\n",
      "+------------+-------------+----------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        16.457         |      162.91143079994507       |     23.164      |    32.146640734461045    |      0.887      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "QLearningAgent - 2 Obstacles - 0.01 Epsilon - 0.99 Gamma - 0.5 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+----------------+\n",
      "|        Parameter         |     Value      |\n",
      "+--------------------------+----------------+\n",
      "|          Agent           | QLearningAgent |\n",
      "|        Obstacles         |       2        |\n",
      "|         Epsilon          |      0.01      |\n",
      "|          Gamma           |      0.99      |\n",
      "|          Alpha           |      0.5       |\n",
      "|         Episodes         |      1000      |\n",
      "| Execution Time (seconds) |      0.42      |\n",
      "|      Success Count       |      914       |\n",
      "+--------------------------+----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-------------+----------------+\n",
      "|   Metric   |   Episode   |     Value      |\n",
      "+------------+-------------+----------------+\n",
      "| Max reward | Episode 21  |  reward = 100  |\n",
      "| Min reward | Episode 762 | reward = -1000 |\n",
      "| Max steps  | Episode 25  |  steps = 100   |\n",
      "| Min steps  | Episode 21  |   steps = 1    |\n",
      "+------------+-------------+----------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        48.647         |       93.49682372704994       |     22.274      |    30.605614315475318    |      0.914      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "QLearningAgent - 2 Obstacles - 0.01 Epsilon - 0.99 Gamma - 0.9 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+----------------+\n",
      "|        Parameter         |     Value      |\n",
      "+--------------------------+----------------+\n",
      "|          Agent           | QLearningAgent |\n",
      "|        Obstacles         |       2        |\n",
      "|         Epsilon          |      0.01      |\n",
      "|          Gamma           |      0.99      |\n",
      "|          Alpha           |      0.9       |\n",
      "|         Episodes         |      1000      |\n",
      "| Execution Time (seconds) |      0.31      |\n",
      "|      Success Count       |      827       |\n",
      "+--------------------------+----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 25 | reward = 100  |\n",
      "| Min reward | Episode 14 | reward = -361 |\n",
      "| Max steps  | Episode 3  |  steps = 100  |\n",
      "| Min steps  | Episode 25 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        25.932         |      101.94913929505732       |     31.985      |    36.87528388448774     |      0.827      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "QLearningAgent - 2 Obstacles - 0.01 Epsilon - 0.1 Gamma - 0.1 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+----------------+\n",
      "|        Parameter         |     Value      |\n",
      "+--------------------------+----------------+\n",
      "|          Agent           | QLearningAgent |\n",
      "|        Obstacles         |       2        |\n",
      "|         Epsilon          |      0.01      |\n",
      "|          Gamma           |      0.1       |\n",
      "|          Alpha           |      0.1       |\n",
      "|         Episodes         |      1000      |\n",
      "| Execution Time (seconds) |      0.65      |\n",
      "|      Success Count       |      341       |\n",
      "+--------------------------+----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 3  | reward = 100  |\n",
      "| Min reward | Episode 20 | reward = -343 |\n",
      "| Max steps  | Episode 10 |  steps = 100  |\n",
      "| Min steps  | Episode 3  |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        -60.876        |      105.85044610648053       |     71.667      |    41.930087565413494    |      0.341      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "QLearningAgent - 2 Obstacles - 0.01 Epsilon - 0.1 Gamma - 0.5 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+----------------+\n",
      "|        Parameter         |     Value      |\n",
      "+--------------------------+----------------+\n",
      "|          Agent           | QLearningAgent |\n",
      "|        Obstacles         |       2        |\n",
      "|         Epsilon          |      0.01      |\n",
      "|          Gamma           |      0.1       |\n",
      "|          Alpha           |      0.5       |\n",
      "|         Episodes         |      1000      |\n",
      "| Execution Time (seconds) |      0.58      |\n",
      "|      Success Count       |      396       |\n",
      "+--------------------------+----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 9  | reward = 100  |\n",
      "| Min reward | Episode 52 | reward = -343 |\n",
      "| Max steps  | Episode 1  |  steps = 100  |\n",
      "| Min steps  | Episode 9  |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        -47.055        |      106.56383920836763       |     66.785      |    43.71537268294541     |      0.396      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "QLearningAgent - 2 Obstacles - 0.01 Epsilon - 0.1 Gamma - 0.9 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+----------------+\n",
      "|        Parameter         |     Value      |\n",
      "+--------------------------+----------------+\n",
      "|          Agent           | QLearningAgent |\n",
      "|        Obstacles         |       2        |\n",
      "|         Epsilon          |      0.01      |\n",
      "|          Gamma           |      0.1       |\n",
      "|          Alpha           |      0.9       |\n",
      "|         Episodes         |      1000      |\n",
      "| Execution Time (seconds) |      0.59      |\n",
      "|      Success Count       |      425       |\n",
      "+--------------------------+----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 81 | reward = 100  |\n",
      "| Min reward | Episode 36 | reward = -289 |\n",
      "| Max steps  | Episode 4  |  steps = 100  |\n",
      "| Min steps  | Episode 81 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        -37.188        |      100.91225862074408       |     64.327      |    43.992014825060785    |      0.425      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "QLearningAgent - 2 Obstacles - 0.01 Epsilon - 0.6 Gamma - 0.1 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+----------------+\n",
      "|        Parameter         |     Value      |\n",
      "+--------------------------+----------------+\n",
      "|          Agent           | QLearningAgent |\n",
      "|        Obstacles         |       2        |\n",
      "|         Epsilon          |      0.01      |\n",
      "|          Gamma           |      0.6       |\n",
      "|          Alpha           |      0.1       |\n",
      "|         Episodes         |      1000      |\n",
      "| Execution Time (seconds) |      0.44      |\n",
      "|      Success Count       |      606       |\n",
      "+--------------------------+----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-------------+---------------+\n",
      "|   Metric   |   Episode   |     Value     |\n",
      "+------------+-------------+---------------+\n",
      "| Max reward | Episode 53  | reward = 100  |\n",
      "| Min reward | Episode 147 | reward = -433 |\n",
      "| Max steps  |  Episode 4  |  steps = 100  |\n",
      "| Min steps  | Episode 53  |   steps = 1   |\n",
      "+------------+-------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        -7.888         |      110.58401457842032       |     47.692      |    45.08347430772894     |      0.606      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "QLearningAgent - 2 Obstacles - 0.01 Epsilon - 0.6 Gamma - 0.5 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+----------------+\n",
      "|        Parameter         |     Value      |\n",
      "+--------------------------+----------------+\n",
      "|          Agent           | QLearningAgent |\n",
      "|        Obstacles         |       2        |\n",
      "|         Epsilon          |      0.01      |\n",
      "|          Gamma           |      0.6       |\n",
      "|          Alpha           |      0.5       |\n",
      "|         Episodes         |      1000      |\n",
      "| Execution Time (seconds) |      0.50      |\n",
      "|      Success Count       |      498       |\n",
      "+--------------------------+----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 50 | reward = 100  |\n",
      "| Min reward | Episode 17 | reward = -316 |\n",
      "| Max steps  | Episode 3  |  steps = 100  |\n",
      "| Min steps  | Episode 50 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        -22.837        |      105.58047958154324       |     57.072      |    45.29366768470326     |      0.498      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "QLearningAgent - 2 Obstacles - 0.01 Epsilon - 0.6 Gamma - 0.9 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+----------------+\n",
      "|        Parameter         |     Value      |\n",
      "+--------------------------+----------------+\n",
      "|          Agent           | QLearningAgent |\n",
      "|        Obstacles         |       2        |\n",
      "|         Epsilon          |      0.01      |\n",
      "|          Gamma           |      0.6       |\n",
      "|          Alpha           |      0.9       |\n",
      "|         Episodes         |      1000      |\n",
      "| Execution Time (seconds) |      0.53      |\n",
      "|      Success Count       |      489       |\n",
      "+--------------------------+----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 23 | reward = 100  |\n",
      "| Min reward | Episode 5  | reward = -334 |\n",
      "| Max steps  | Episode 2  |  steps = 100  |\n",
      "| Min steps  | Episode 23 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        -27.012        |      101.18386249807835       |     59.762      |    44.09008155233205     |      0.489      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "QLearningAgent - 2 Obstacles - 0.01 Epsilon - 0.99 Gamma - 0.1 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+----------------+\n",
      "|        Parameter         |     Value      |\n",
      "+--------------------------+----------------+\n",
      "|          Agent           | QLearningAgent |\n",
      "|        Obstacles         |       2        |\n",
      "|         Epsilon          |      0.01      |\n",
      "|          Gamma           |      0.99      |\n",
      "|          Alpha           |      0.1       |\n",
      "|         Episodes         |      1000      |\n",
      "| Execution Time (seconds) |      0.32      |\n",
      "|      Success Count       |      887       |\n",
      "+--------------------------+----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-------------+---------------+\n",
      "|   Metric   |   Episode   |     Value     |\n",
      "+------------+-------------+---------------+\n",
      "| Max reward | Episode 57  | reward = 100  |\n",
      "| Min reward | Episode 400 | reward = -937 |\n",
      "| Max steps  |  Episode 0  |  steps = 100  |\n",
      "| Min steps  | Episode 57  |   steps = 1   |\n",
      "+------------+-------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        14.578         |       158.6406277243152       |     23.817      |    32.200596103013204    |      0.887      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "QLearningAgent - 2 Obstacles - 0.01 Epsilon - 0.99 Gamma - 0.5 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+----------------+\n",
      "|        Parameter         |     Value      |\n",
      "+--------------------------+----------------+\n",
      "|          Agent           | QLearningAgent |\n",
      "|        Obstacles         |       2        |\n",
      "|         Epsilon          |      0.01      |\n",
      "|          Gamma           |      0.99      |\n",
      "|          Alpha           |      0.5       |\n",
      "|         Episodes         |      1000      |\n",
      "| Execution Time (seconds) |      0.26      |\n",
      "|      Success Count       |      925       |\n",
      "+--------------------------+----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 52 | reward = 100  |\n",
      "| Min reward | Episode 24 | reward = -487 |\n",
      "| Max steps  | Episode 1  |  steps = 100  |\n",
      "| Min steps  | Episode 52 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|         49.91         |       85.09082411369835       |     22.338      |    29.884625593218445    |      0.925      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "QLearningAgent - 2 Obstacles - 0.01 Epsilon - 0.99 Gamma - 0.9 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+----------------+\n",
      "|        Parameter         |     Value      |\n",
      "+--------------------------+----------------+\n",
      "|          Agent           | QLearningAgent |\n",
      "|        Obstacles         |       2        |\n",
      "|         Epsilon          |      0.01      |\n",
      "|          Gamma           |      0.99      |\n",
      "|          Alpha           |      0.9       |\n",
      "|         Episodes         |      1000      |\n",
      "| Execution Time (seconds) |      0.24      |\n",
      "|      Success Count       |      886       |\n",
      "+--------------------------+----------------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-------------+----------------+\n",
      "|   Metric   |   Episode   |     Value      |\n",
      "+------------+-------------+----------------+\n",
      "| Max reward | Episode 26  |  reward = 100  |\n",
      "| Min reward | Episode 635 | reward = -1000 |\n",
      "| Max steps  |  Episode 0  |  steps = 100   |\n",
      "| Min steps  | Episode 26  |   steps = 1    |\n",
      "+------------+-------------+----------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        41.579         |       96.22887173169369       |     25.573      |    32.80888742542322     |      0.886      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Define the grid of parameters to test\n",
    "param_grid = {\n",
    "    'epsilon': [0.1, 0.5, 0.6],\n",
    "    'gamma': [0.1, 0.6, 0.99],\n",
    "    'alpha': [0.1, 0.5, 0.9]\n",
    "}\n",
    "\n",
    "# Number of obstacles to test\n",
    "num_obstacles_list = [0, 2]\n",
    "\n",
    "# Number of episodes to run for each experiment\n",
    "episodes = 1000\n",
    "\n",
    "# Number of maze to test\n",
    "maze = 3\n",
    "\n",
    "# Run the experiments\n",
    "run_experiments(QLearningAgent, maze, episodes, num_obstacles_list, param_grid, save_plots=False, show_plots=False, render=False, max_steps=100)\n",
    "\n",
    "# Open the CSV file with the results\n",
    "df = pd.read_csv(f'results/QLearningAgent.csv')\n",
    "# Converte as strings de volta para arrays do NumPy\n",
    "df['rewards'] = df['rewards'].apply(literal_eval)\n",
    "\n",
    "# Pega o primeiro treinado para avaliar\n",
    "rewards_deterministic = df[df['num_obstacles'] == 0]['rewards'].values[0]\n",
    "rewards_stochastic = df[df['num_obstacles'] == 2]['rewards'].values[0]\n",
    "plot_deterministic_vs_stochastic(rewards_deterministic, rewards_stochastic,method=\"QLearningAgent\" ,show_plots=False, save_plots=False)\n",
    "\n",
    "# Expande a coluna 'rewards', que contém listas, em várias linhas\n",
    "df = df.explode('rewards')\n",
    "# Converte os valores de 'rewards' para numéricos\n",
    "df['rewards'] = pd.to_numeric(df['rewards'])\n",
    "# Agrupa por 'alpha' e calcula a média das recompensas\n",
    "grouped = df.groupby('alpha')['rewards'].mean()\n",
    "# Prepara os dados para o gráfico\n",
    "alpha_values = grouped.index.tolist()\n",
    "average_rewards = grouped.values.tolist()\n",
    "plot_sensitivity_analysis(alpha_values, average_rewards,parameter_name='alpha',metric_name='Recompensa Média',method=\"QLearningAgent\",title='Análise de Sensibilidade para Alpha', show_plots=False, save_plots=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1) Experimento do Q-Learning com Aproximação de função linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\juliano soares\\AppData\\Local\\Temp\\ipykernel_17692\\3167324544.py:54: RuntimeWarning: invalid value encountered in scalar subtract\n",
      "  self.theta += self.alpha * (target - predict) * current_features\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LinearQLearningAgent' object has no attribute 'execute_optimal_policy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\Projetos\\2023-segundo-semestre\\MO436A - Reinforcement Learning\\maze_game\\beta.ipynb Cell 56\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projetos/2023-segundo-semestre/MO436A%20-%20Reinforcement%20Learning/maze_game/beta.ipynb#Y233sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m agent \u001b[39m=\u001b[39m LinearQLearningAgent(env, num_features\u001b[39m=\u001b[39m\u001b[39m7\u001b[39m, alpha\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m, gamma\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m, epsilon\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projetos/2023-segundo-semestre/MO436A%20-%20Reinforcement%20Learning/maze_game/beta.ipynb#Y233sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m agent_data \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mtrain(episodes, render\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, num_obstacles\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, max_steps\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Projetos/2023-segundo-semestre/MO436A%20-%20Reinforcement%20Learning/maze_game/beta.ipynb#Y233sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m agent\u001b[39m.\u001b[39;49mexecute_optimal_policy(max_steps\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, start_pos\u001b[39m=\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projetos/2023-segundo-semestre/MO436A%20-%20Reinforcement%20Learning/maze_game/beta.ipynb#Y233sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m agent\u001b[39m.\u001b[39mshowQ()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'LinearQLearningAgent' object has no attribute 'execute_optimal_policy'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Experimento Sarsa\n",
    "\n",
    "**episódios** = 1000\n",
    "\n",
    "**reward =** -1 quando mover sem obstáculos e -50 quando mover em paredes ou obstáculos -150 se passar de 100 movimentos no episódio\n",
    "\n",
    "**alpha =** variável de acordo com a formula $\\alpha_{t} = 1/N_(s_t, s_a)$\n",
    "\n",
    "**gamma =** 0.1 , 0.6, 0.9, 0.99\n",
    "\n",
    "**epsilon =**  de acordo com a formula $\\epsilon_{t} = N0/(N0+N(st))$\n",
    "\n",
    "**Ambiente =** Matriz 6 x 6 com obstáculo fixo ao centro \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SARSAgent - 0 Obstacles - 0.01 Epsilon - 0.1 Gamma - 0.1 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------+\n",
      "|        Parameter         |   Value   |\n",
      "+--------------------------+-----------+\n",
      "|          Agent           | SARSAgent |\n",
      "|        Obstacles         |     0     |\n",
      "|         Epsilon          |   0.01    |\n",
      "|          Gamma           |    0.1    |\n",
      "|          Alpha           |    0.1    |\n",
      "|         Episodes         |   1000    |\n",
      "| Execution Time (seconds) |   1.59    |\n",
      "|      Success Count       |    973    |\n",
      "+--------------------------+-----------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 54 | reward = 100  |\n",
      "| Min reward | Episode 0  | reward = -235 |\n",
      "| Max steps  | Episode 0  |  steps = 100  |\n",
      "| Min steps  | Episode 54 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        85.163         |       36.97536488810636       |     11.814      |    18.369998934699503    |      0.973      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "SARSAgent - 0 Obstacles - 0.01 Epsilon - 0.1 Gamma - 0.5 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------+\n",
      "|        Parameter         |   Value   |\n",
      "+--------------------------+-----------+\n",
      "|          Agent           | SARSAgent |\n",
      "|        Obstacles         |     0     |\n",
      "|         Epsilon          |   0.01    |\n",
      "|          Gamma           |    0.1    |\n",
      "|          Alpha           |    0.5    |\n",
      "|         Episodes         |   1000    |\n",
      "| Execution Time (seconds) |   1.92    |\n",
      "|      Success Count       |    923    |\n",
      "+--------------------------+-----------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-----------+---------------+\n",
      "|   Metric   |  Episode  |     Value     |\n",
      "+------------+-----------+---------------+\n",
      "| Max reward | Episode 4 | reward = 100  |\n",
      "| Min reward | Episode 0 | reward = -352 |\n",
      "| Max steps  | Episode 0 |  steps = 100  |\n",
      "| Min steps  | Episode 4 |   steps = 1   |\n",
      "+------------+-----------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        77.666         |       55.40949969686296       |     14.603      |    26.495497776082686    |      0.923      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "SARSAgent - 0 Obstacles - 0.01 Epsilon - 0.1 Gamma - 0.9 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------+\n",
      "|        Parameter         |   Value   |\n",
      "+--------------------------+-----------+\n",
      "|          Agent           | SARSAgent |\n",
      "|        Obstacles         |     0     |\n",
      "|         Epsilon          |   0.01    |\n",
      "|          Gamma           |    0.1    |\n",
      "|          Alpha           |    0.9    |\n",
      "|         Episodes         |   1000    |\n",
      "| Execution Time (seconds) |   1.84    |\n",
      "|      Success Count       |    910    |\n",
      "+--------------------------+-----------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 13 | reward = 100  |\n",
      "| Min reward | Episode 2  | reward = -210 |\n",
      "| Max steps  | Episode 56 |  steps = 100  |\n",
      "| Min steps  | Episode 13 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        75.342         |       58.17400838219119       |     15.425      |    28.019019262644445    |      0.91       |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "SARSAgent - 0 Obstacles - 0.01 Epsilon - 0.6 Gamma - 0.1 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------+\n",
      "|        Parameter         |   Value   |\n",
      "+--------------------------+-----------+\n",
      "|          Agent           | SARSAgent |\n",
      "|        Obstacles         |     0     |\n",
      "|         Epsilon          |   0.01    |\n",
      "|          Gamma           |    0.6    |\n",
      "|          Alpha           |    0.1    |\n",
      "|         Episodes         |   1000    |\n",
      "| Execution Time (seconds) |   0.77    |\n",
      "|      Success Count       |    995    |\n",
      "+--------------------------+-----------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 15 | reward = 100  |\n",
      "| Min reward | Episode 3  | reward = -226 |\n",
      "| Max steps  | Episode 0  |  steps = 100  |\n",
      "| Min steps  | Episode 15 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        92.929         |      20.644712716895896       |      6.756      |    8.252549216367544     |      0.995      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "SARSAgent - 0 Obstacles - 0.01 Epsilon - 0.6 Gamma - 0.5 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------+\n",
      "|        Parameter         |   Value   |\n",
      "+--------------------------+-----------+\n",
      "|          Agent           | SARSAgent |\n",
      "|        Obstacles         |     0     |\n",
      "|         Epsilon          |   0.01    |\n",
      "|          Gamma           |    0.6    |\n",
      "|          Alpha           |    0.5    |\n",
      "|         Episodes         |   1000    |\n",
      "| Execution Time (seconds) |   0.66    |\n",
      "|      Success Count       |   1000    |\n",
      "+--------------------------+-----------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 12 | reward = 100  |\n",
      "| Min reward | Episode 0  | reward = -208 |\n",
      "| Max steps  | Episode 0  |  steps = 84   |\n",
      "| Min steps  | Episode 12 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|         94.32         |      12.775241947094656       |      5.861      |    4.774355817644248     |       1.0       |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "SARSAgent - 0 Obstacles - 0.01 Epsilon - 0.6 Gamma - 0.9 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------+\n",
      "|        Parameter         |   Value   |\n",
      "+--------------------------+-----------+\n",
      "|          Agent           | SARSAgent |\n",
      "|        Obstacles         |     0     |\n",
      "|         Epsilon          |   0.01    |\n",
      "|          Gamma           |    0.6    |\n",
      "|          Alpha           |    0.9    |\n",
      "|         Episodes         |   1000    |\n",
      "| Execution Time (seconds) |   0.75    |\n",
      "|      Success Count       |    997    |\n",
      "+--------------------------+-----------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-------------+---------------+\n",
      "|   Metric   |   Episode   |     Value     |\n",
      "+------------+-------------+---------------+\n",
      "| Max reward | Episode 61  | reward = 100  |\n",
      "| Min reward |  Episode 2  | reward = -277 |\n",
      "| Max steps  | Episode 216 |  steps = 100  |\n",
      "| Min steps  | Episode 61  |   steps = 1   |\n",
      "+------------+-------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        92.587         |      19.116115108903713       |      7.003      |    7.9200832520068065    |      0.997      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "SARSAgent - 0 Obstacles - 0.01 Epsilon - 0.99 Gamma - 0.1 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------+\n",
      "|        Parameter         |   Value   |\n",
      "+--------------------------+-----------+\n",
      "|          Agent           | SARSAgent |\n",
      "|        Obstacles         |     0     |\n",
      "|         Epsilon          |   0.01    |\n",
      "|          Gamma           |   0.99    |\n",
      "|          Alpha           |    0.1    |\n",
      "|         Episodes         |   1000    |\n",
      "| Execution Time (seconds) |   0.66    |\n",
      "|      Success Count       |   1000    |\n",
      "+--------------------------+-----------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 18 | reward = 100  |\n",
      "| Min reward | Episode 0  | reward = -139 |\n",
      "| Max steps  | Episode 0  |  steps = 87   |\n",
      "| Min steps  | Episode 18 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        94.325         |      11.425231328551336       |      5.964      |    5.351573912723559     |       1.0       |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "SARSAgent - 0 Obstacles - 0.01 Epsilon - 0.99 Gamma - 0.5 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------+\n",
      "|        Parameter         |   Value   |\n",
      "+--------------------------+-----------+\n",
      "|          Agent           | SARSAgent |\n",
      "|        Obstacles         |     0     |\n",
      "|         Epsilon          |   0.01    |\n",
      "|          Gamma           |   0.99    |\n",
      "|          Alpha           |    0.5    |\n",
      "|         Episodes         |   1000    |\n",
      "| Execution Time (seconds) |   0.67    |\n",
      "|      Success Count       |   1000    |\n",
      "+--------------------------+-----------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 69 | reward = 100  |\n",
      "| Min reward | Episode 1  | reward = -106 |\n",
      "| Max steps  | Episode 1  |  steps = 63   |\n",
      "| Min steps  | Episode 69 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        94.025         |       9.750766762283488       |      6.192      |    3.8180772273113748    |       1.0       |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "SARSAgent - 0 Obstacles - 0.01 Epsilon - 0.99 Gamma - 0.9 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------+\n",
      "|        Parameter         |   Value   |\n",
      "+--------------------------+-----------+\n",
      "|          Agent           | SARSAgent |\n",
      "|        Obstacles         |     0     |\n",
      "|         Epsilon          |   0.01    |\n",
      "|          Gamma           |   0.99    |\n",
      "|          Alpha           |    0.9    |\n",
      "|         Episodes         |   1000    |\n",
      "| Execution Time (seconds) |   0.94    |\n",
      "|      Success Count       |    998    |\n",
      "+--------------------------+-----------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 2  | reward = 100  |\n",
      "| Min reward | Episode 81 | reward = -289 |\n",
      "| Max steps  | Episode 81 |  steps = 100  |\n",
      "| Min steps  | Episode 2  |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        91.327         |      20.502205037785846       |      7.896      |    8.925068114072904     |      0.998      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "SARSAgent - 0 Obstacles - 0.01 Epsilon - 0.1 Gamma - 0.1 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------+\n",
      "|        Parameter         |   Value   |\n",
      "+--------------------------+-----------+\n",
      "|          Agent           | SARSAgent |\n",
      "|        Obstacles         |     0     |\n",
      "|         Epsilon          |   0.01    |\n",
      "|          Gamma           |    0.1    |\n",
      "|          Alpha           |    0.1    |\n",
      "|         Episodes         |   1000    |\n",
      "| Execution Time (seconds) |   3.39    |\n",
      "|      Success Count       |    813    |\n",
      "+--------------------------+-----------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 29 | reward = 100  |\n",
      "| Min reward | Episode 4  | reward = -244 |\n",
      "| Max steps  | Episode 4  |  steps = 100  |\n",
      "| Min steps  | Episode 29 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        46.976         |       80.50479766104904       |     29.638      |    38.34998927871659     |      0.813      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "SARSAgent - 0 Obstacles - 0.01 Epsilon - 0.1 Gamma - 0.5 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------+\n",
      "|        Parameter         |   Value   |\n",
      "+--------------------------+-----------+\n",
      "|          Agent           | SARSAgent |\n",
      "|        Obstacles         |     0     |\n",
      "|         Epsilon          |   0.01    |\n",
      "|          Gamma           |    0.1    |\n",
      "|          Alpha           |    0.5    |\n",
      "|         Episodes         |   1000    |\n",
      "| Execution Time (seconds) |   2.09    |\n",
      "|      Success Count       |    917    |\n",
      "+--------------------------+-----------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-----------+---------------+\n",
      "|   Metric   |  Episode  |     Value     |\n",
      "+------------+-----------+---------------+\n",
      "| Max reward | Episode 1 | reward = 100  |\n",
      "| Min reward | Episode 3 | reward = -316 |\n",
      "| Max steps  | Episode 3 |  steps = 100  |\n",
      "| Min steps  | Episode 1 |   steps = 1   |\n",
      "+------------+-----------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        69.852         |       64.49556398523679       |     17.853      |    28.94289346423543     |      0.917      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "SARSAgent - 0 Obstacles - 0.01 Epsilon - 0.1 Gamma - 0.9 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------+\n",
      "|        Parameter         |   Value   |\n",
      "+--------------------------+-----------+\n",
      "|          Agent           | SARSAgent |\n",
      "|        Obstacles         |     0     |\n",
      "|         Epsilon          |   0.01    |\n",
      "|          Gamma           |    0.1    |\n",
      "|          Alpha           |    0.9    |\n",
      "|         Episodes         |   1000    |\n",
      "| Execution Time (seconds) |   2.28    |\n",
      "|      Success Count       |    911    |\n",
      "+--------------------------+-----------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-------------+---------------+\n",
      "|   Metric   |   Episode   |     Value     |\n",
      "+------------+-------------+---------------+\n",
      "| Max reward | Episode 114 | reward = 100  |\n",
      "| Min reward |  Episode 0  | reward = -352 |\n",
      "| Max steps  |  Episode 0  |  steps = 100  |\n",
      "| Min steps  | Episode 114 |   steps = 1   |\n",
      "+------------+-------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        66.899         |       65.26757736928539       |     19.784      |    29.559687398873475    |      0.911      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "SARSAgent - 0 Obstacles - 0.01 Epsilon - 0.6 Gamma - 0.1 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------+\n",
      "|        Parameter         |   Value   |\n",
      "+--------------------------+-----------+\n",
      "|          Agent           | SARSAgent |\n",
      "|        Obstacles         |     0     |\n",
      "|         Epsilon          |   0.01    |\n",
      "|          Gamma           |    0.6    |\n",
      "|          Alpha           |    0.1    |\n",
      "|         Episodes         |   1000    |\n",
      "| Execution Time (seconds) |   0.80    |\n",
      "|      Success Count       |    995    |\n",
      "+--------------------------+-----------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 18 | reward = 100  |\n",
      "| Min reward | Episode 0  | reward = -334 |\n",
      "| Max steps  | Episode 0  |  steps = 100  |\n",
      "| Min steps  | Episode 18 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        90.071         |      27.582508112094406       |      7.445      |    9.496586763683053     |      0.995      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "SARSAgent - 0 Obstacles - 0.01 Epsilon - 0.6 Gamma - 0.5 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------+\n",
      "|        Parameter         |   Value   |\n",
      "+--------------------------+-----------+\n",
      "|          Agent           | SARSAgent |\n",
      "|        Obstacles         |     0     |\n",
      "|         Epsilon          |   0.01    |\n",
      "|          Gamma           |    0.6    |\n",
      "|          Alpha           |    0.5    |\n",
      "|         Episodes         |   1000    |\n",
      "| Execution Time (seconds) |   0.93    |\n",
      "|      Success Count       |    991    |\n",
      "+--------------------------+-----------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-----------+---------------+\n",
      "|   Metric   |  Episode  |     Value     |\n",
      "+------------+-----------+---------------+\n",
      "| Max reward | Episode 6 | reward = 100  |\n",
      "| Min reward | Episode 2 | reward = -298 |\n",
      "| Max steps  | Episode 0 |  steps = 100  |\n",
      "| Min steps  | Episode 6 |   steps = 1   |\n",
      "+------------+-----------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        88.873         |      28.800596267184215       |      8.509      |    12.23321587198438     |      0.991      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "SARSAgent - 0 Obstacles - 0.01 Epsilon - 0.6 Gamma - 0.9 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------+\n",
      "|        Parameter         |   Value   |\n",
      "+--------------------------+-----------+\n",
      "|          Agent           | SARSAgent |\n",
      "|        Obstacles         |     0     |\n",
      "|         Epsilon          |   0.01    |\n",
      "|          Gamma           |    0.6    |\n",
      "|          Alpha           |    0.9    |\n",
      "|         Episodes         |   1000    |\n",
      "| Execution Time (seconds) |   1.04    |\n",
      "|      Success Count       |    986    |\n",
      "+--------------------------+-----------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 36 | reward = 100  |\n",
      "| Min reward | Episode 0  | reward = -289 |\n",
      "| Max steps  | Episode 0  |  steps = 100  |\n",
      "| Min steps  | Episode 36 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|         85.86         |       34.99947661556331       |      9.802      |    15.157986692188398    |      0.986      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "SARSAgent - 0 Obstacles - 0.01 Epsilon - 0.99 Gamma - 0.1 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------+\n",
      "|        Parameter         |   Value   |\n",
      "+--------------------------+-----------+\n",
      "|          Agent           | SARSAgent |\n",
      "|        Obstacles         |     0     |\n",
      "|         Epsilon          |   0.01    |\n",
      "|          Gamma           |   0.99    |\n",
      "|          Alpha           |    0.1    |\n",
      "|         Episodes         |   1000    |\n",
      "| Execution Time (seconds) |   0.79    |\n",
      "|      Success Count       |    993    |\n",
      "+--------------------------+-----------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 18 | reward = 100  |\n",
      "| Min reward | Episode 0  | reward = -298 |\n",
      "| Max steps  | Episode 0  |  steps = 100  |\n",
      "| Min steps  | Episode 18 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        90.542         |      29.196826722962392       |      6.961      |    9.448531830594305     |      0.993      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "SARSAgent - 0 Obstacles - 0.01 Epsilon - 0.99 Gamma - 0.5 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------+\n",
      "|        Parameter         |   Value   |\n",
      "+--------------------------+-----------+\n",
      "|          Agent           | SARSAgent |\n",
      "|        Obstacles         |     0     |\n",
      "|         Epsilon          |   0.01    |\n",
      "|          Gamma           |   0.99    |\n",
      "|          Alpha           |    0.5    |\n",
      "|         Episodes         |   1000    |\n",
      "| Execution Time (seconds) |   0.84    |\n",
      "|      Success Count       |    999    |\n",
      "+--------------------------+-----------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 17 | reward = 100  |\n",
      "| Min reward | Episode 0  | reward = -280 |\n",
      "| Max steps  | Episode 0  |  steps = 100  |\n",
      "| Min steps  | Episode 17 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        91.369         |       17.60109762593338       |      7.226      |    6.1850771360735015    |      0.999      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "SARSAgent - 0 Obstacles - 0.01 Epsilon - 0.99 Gamma - 0.9 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------+\n",
      "|        Parameter         |   Value   |\n",
      "+--------------------------+-----------+\n",
      "|          Agent           | SARSAgent |\n",
      "|        Obstacles         |     0     |\n",
      "|         Epsilon          |   0.01    |\n",
      "|          Gamma           |   0.99    |\n",
      "|          Alpha           |    0.9    |\n",
      "|         Episodes         |   1000    |\n",
      "| Execution Time (seconds) |   0.98    |\n",
      "|      Success Count       |    999    |\n",
      "+--------------------------+-----------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-------------+---------------+\n",
      "|   Metric   |   Episode   |     Value     |\n",
      "+------------+-------------+---------------+\n",
      "| Max reward | Episode 58  | reward = 100  |\n",
      "| Min reward | Episode 110 | reward = -325 |\n",
      "| Max steps  | Episode 110 |  steps = 100  |\n",
      "| Min steps  | Episode 58  |   steps = 1   |\n",
      "+------------+-------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        87.044         |      25.025033792510975       |      9.301      |     8.88827319564382     |      0.999      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "SARSAgent - 0 Obstacles - 0.01 Epsilon - 0.1 Gamma - 0.1 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------+\n",
      "|        Parameter         |   Value   |\n",
      "+--------------------------+-----------+\n",
      "|          Agent           | SARSAgent |\n",
      "|        Obstacles         |     0     |\n",
      "|         Epsilon          |   0.01    |\n",
      "|          Gamma           |    0.1    |\n",
      "|          Alpha           |    0.1    |\n",
      "|         Episodes         |   1000    |\n",
      "| Execution Time (seconds) |   3.54    |\n",
      "|      Success Count       |    758    |\n",
      "+--------------------------+-----------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 9  | reward = 100  |\n",
      "| Min reward | Episode 16 | reward = -262 |\n",
      "| Max steps  | Episode 2  |  steps = 100  |\n",
      "| Min steps  | Episode 9  |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        36.729         |       90.0474547286213        |     32.809      |    41.49996103330418     |      0.758      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "SARSAgent - 0 Obstacles - 0.01 Epsilon - 0.1 Gamma - 0.5 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------+\n",
      "|        Parameter         |   Value   |\n",
      "+--------------------------+-----------+\n",
      "|          Agent           | SARSAgent |\n",
      "|        Obstacles         |     0     |\n",
      "|         Epsilon          |   0.01    |\n",
      "|          Gamma           |    0.1    |\n",
      "|          Alpha           |    0.5    |\n",
      "|         Episodes         |   1000    |\n",
      "| Execution Time (seconds) |   2.62    |\n",
      "|      Success Count       |    854    |\n",
      "+--------------------------+-----------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 17 | reward = 100  |\n",
      "| Min reward | Episode 5  | reward = -280 |\n",
      "| Max steps  | Episode 5  |  steps = 100  |\n",
      "| Min steps  | Episode 17 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        54.912         |       77.2530795133391        |     25.078      |    35.276200565343295    |      0.854      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "SARSAgent - 0 Obstacles - 0.01 Epsilon - 0.1 Gamma - 0.9 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------+\n",
      "|        Parameter         |   Value   |\n",
      "+--------------------------+-----------+\n",
      "|          Agent           | SARSAgent |\n",
      "|        Obstacles         |     0     |\n",
      "|         Epsilon          |   0.01    |\n",
      "|          Gamma           |    0.1    |\n",
      "|          Alpha           |    0.9    |\n",
      "|         Episodes         |   1000    |\n",
      "| Execution Time (seconds) |   1.74    |\n",
      "|      Success Count       |    939    |\n",
      "+--------------------------+-----------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-----------+---------------+\n",
      "|   Metric   |  Episode  |     Value     |\n",
      "+------------+-----------+---------------+\n",
      "| Max reward | Episode 4 | reward = 100  |\n",
      "| Min reward | Episode 3 | reward = -307 |\n",
      "| Max steps  | Episode 0 |  steps = 100  |\n",
      "| Min steps  | Episode 4 |   steps = 1   |\n",
      "+------------+-----------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        72.266         |       58.61406702194323       |      16.77      |    26.129827168733204    |      0.939      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "SARSAgent - 0 Obstacles - 0.01 Epsilon - 0.6 Gamma - 0.1 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------+\n",
      "|        Parameter         |   Value   |\n",
      "+--------------------------+-----------+\n",
      "|          Agent           | SARSAgent |\n",
      "|        Obstacles         |     0     |\n",
      "|         Epsilon          |   0.01    |\n",
      "|          Gamma           |    0.6    |\n",
      "|          Alpha           |    0.1    |\n",
      "|         Episodes         |   1000    |\n",
      "| Execution Time (seconds) |   0.82    |\n",
      "|      Success Count       |    994    |\n",
      "+--------------------------+-----------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-----------+---------------+\n",
      "|   Metric   |  Episode  |     Value     |\n",
      "+------------+-----------+---------------+\n",
      "| Max reward | Episode 9 | reward = 100  |\n",
      "| Min reward | Episode 0 | reward = -280 |\n",
      "| Max steps  | Episode 0 |  steps = 100  |\n",
      "| Min steps  | Episode 9 |   steps = 1   |\n",
      "+------------+-----------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        89.474         |       28.64974925077237       |      7.466      |    9.879800324624707     |      0.994      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "SARSAgent - 0 Obstacles - 0.01 Epsilon - 0.6 Gamma - 0.5 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------+\n",
      "|        Parameter         |   Value   |\n",
      "+--------------------------+-----------+\n",
      "|          Agent           | SARSAgent |\n",
      "|        Obstacles         |     0     |\n",
      "|         Epsilon          |   0.01    |\n",
      "|          Gamma           |    0.6    |\n",
      "|          Alpha           |    0.5    |\n",
      "|         Episodes         |   1000    |\n",
      "| Execution Time (seconds) |   0.90    |\n",
      "|      Success Count       |    996    |\n",
      "+--------------------------+-----------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-----------+---------------+\n",
      "|   Metric   |  Episode  |     Value     |\n",
      "+------------+-----------+---------------+\n",
      "| Max reward | Episode 8 | reward = 100  |\n",
      "| Min reward | Episode 2 | reward = -280 |\n",
      "| Max steps  | Episode 2 |  steps = 100  |\n",
      "| Min steps  | Episode 8 |   steps = 1   |\n",
      "+------------+-----------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|         89.89         |      23.238716357362293       |      7.727      |    10.356820686616851    |      0.996      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "SARSAgent - 0 Obstacles - 0.01 Epsilon - 0.6 Gamma - 0.9 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------+\n",
      "|        Parameter         |   Value   |\n",
      "+--------------------------+-----------+\n",
      "|          Agent           | SARSAgent |\n",
      "|        Obstacles         |     0     |\n",
      "|         Epsilon          |   0.01    |\n",
      "|          Gamma           |    0.6    |\n",
      "|          Alpha           |    0.9    |\n",
      "|         Episodes         |   1000    |\n",
      "| Execution Time (seconds) |   1.12    |\n",
      "|      Success Count       |    989    |\n",
      "+--------------------------+-----------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 61 | reward = 100  |\n",
      "| Min reward | Episode 0  | reward = -397 |\n",
      "| Max steps  | Episode 0  |  steps = 100  |\n",
      "| Min steps  | Episode 61 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        85.564         |       36.8608820923839        |      9.933      |    13.871803701679822    |      0.989      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "SARSAgent - 0 Obstacles - 0.01 Epsilon - 0.99 Gamma - 0.1 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------+\n",
      "|        Parameter         |   Value   |\n",
      "+--------------------------+-----------+\n",
      "|          Agent           | SARSAgent |\n",
      "|        Obstacles         |     0     |\n",
      "|         Epsilon          |   0.01    |\n",
      "|          Gamma           |   0.99    |\n",
      "|          Alpha           |    0.1    |\n",
      "|         Episodes         |   1000    |\n",
      "| Execution Time (seconds) |   0.89    |\n",
      "|      Success Count       |    997    |\n",
      "+--------------------------+-----------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-----------+---------------+\n",
      "|   Metric   |  Episode  |     Value     |\n",
      "+------------+-----------+---------------+\n",
      "| Max reward | Episode 6 | reward = 100  |\n",
      "| Min reward | Episode 2 | reward = -280 |\n",
      "| Max steps  | Episode 2 |  steps = 100  |\n",
      "| Min steps  | Episode 6 |   steps = 1   |\n",
      "+------------+-----------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        90.569         |      23.452873518532353       |      7.14       |    7.590653332751932     |      0.997      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "SARSAgent - 0 Obstacles - 0.01 Epsilon - 0.99 Gamma - 0.5 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------+\n",
      "|        Parameter         |   Value   |\n",
      "+--------------------------+-----------+\n",
      "|          Agent           | SARSAgent |\n",
      "|        Obstacles         |     0     |\n",
      "|         Epsilon          |   0.01    |\n",
      "|          Gamma           |   0.99    |\n",
      "|          Alpha           |    0.5    |\n",
      "|         Episodes         |   1000    |\n",
      "| Execution Time (seconds) |   0.77    |\n",
      "|      Success Count       |   1000    |\n",
      "+--------------------------+-----------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 4  | reward = 100  |\n",
      "| Min reward | Episode 1  | reward = -210 |\n",
      "| Max steps  | Episode 21 |  steps = 86   |\n",
      "| Min steps  | Episode 4  |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        91.697         |      16.510353805499832       |      6.729      |    6.0767166698378725    |       1.0       |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "SARSAgent - 0 Obstacles - 0.01 Epsilon - 0.99 Gamma - 0.9 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------+\n",
      "|        Parameter         |   Value   |\n",
      "+--------------------------+-----------+\n",
      "|          Agent           | SARSAgent |\n",
      "|        Obstacles         |     0     |\n",
      "|         Epsilon          |   0.01    |\n",
      "|          Gamma           |   0.99    |\n",
      "|          Alpha           |    0.9    |\n",
      "|         Episodes         |   1000    |\n",
      "| Execution Time (seconds) |   1.03    |\n",
      "|      Success Count       |    996    |\n",
      "+--------------------------+-----------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 21 | reward = 100  |\n",
      "| Min reward | Episode 0  | reward = -343 |\n",
      "| Max steps  | Episode 0  |  steps = 100  |\n",
      "| Min steps  | Episode 21 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        86.047         |       30.97124487341458       |      9.005      |    10.095587942061417    |      0.996      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "SARSAgent - 2 Obstacles - 0.01 Epsilon - 0.1 Gamma - 0.1 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------+\n",
      "|        Parameter         |   Value   |\n",
      "+--------------------------+-----------+\n",
      "|          Agent           | SARSAgent |\n",
      "|        Obstacles         |     2     |\n",
      "|         Epsilon          |   0.01    |\n",
      "|          Gamma           |    0.1    |\n",
      "|          Alpha           |    0.1    |\n",
      "|         Episodes         |   1000    |\n",
      "| Execution Time (seconds) |   9.31    |\n",
      "|      Success Count       |    219    |\n",
      "+--------------------------+-----------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-------------+----------------+\n",
      "|   Metric   |   Episode   |     Value      |\n",
      "+------------+-------------+----------------+\n",
      "| Max reward | Episode 20  |  reward = 100  |\n",
      "| Min reward | Episode 332 | reward = -1000 |\n",
      "| Max steps  |  Episode 1  |  steps = 100   |\n",
      "| Min steps  | Episode 20  |   steps = 1    |\n",
      "+------------+-------------+----------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        -70.013        |       88.67652652961412       |     80.866      |    37.67728256684155     |      0.219      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "SARSAgent - 2 Obstacles - 0.01 Epsilon - 0.1 Gamma - 0.5 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------+\n",
      "|        Parameter         |   Value   |\n",
      "+--------------------------+-----------+\n",
      "|          Agent           | SARSAgent |\n",
      "|        Obstacles         |     2     |\n",
      "|         Epsilon          |   0.01    |\n",
      "|          Gamma           |    0.1    |\n",
      "|          Alpha           |    0.5    |\n",
      "|         Episodes         |   1000    |\n",
      "| Execution Time (seconds) |   8.89    |\n",
      "|      Success Count       |    215    |\n",
      "+--------------------------+-----------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-----------+---------------+\n",
      "|   Metric   |  Episode  |     Value     |\n",
      "+------------+-----------+---------------+\n",
      "| Max reward | Episode 8 | reward = 100  |\n",
      "| Min reward | Episode 0 | reward = -316 |\n",
      "| Max steps  | Episode 0 |  steps = 100  |\n",
      "| Min steps  | Episode 8 |   steps = 1   |\n",
      "+------------+-----------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        -68.866        |       84.1749099165509        |     80.879      |    37.92258537443724     |      0.215      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "SARSAgent - 2 Obstacles - 0.01 Epsilon - 0.1 Gamma - 0.9 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------+\n",
      "|        Parameter         |   Value   |\n",
      "+--------------------------+-----------+\n",
      "|          Agent           | SARSAgent |\n",
      "|        Obstacles         |     2     |\n",
      "|         Epsilon          |   0.01    |\n",
      "|          Gamma           |    0.1    |\n",
      "|          Alpha           |    0.9    |\n",
      "|         Episodes         |   1000    |\n",
      "| Execution Time (seconds) |   8.72    |\n",
      "|      Success Count       |    317    |\n",
      "+--------------------------+-----------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 12 | reward = 100  |\n",
      "| Min reward | Episode 97 | reward = -226 |\n",
      "| Max steps  | Episode 11 |  steps = 100  |\n",
      "| Min steps  | Episode 12 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        -49.293        |       93.49044664892031       |     72.031      |    42.65094538392019     |      0.317      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "SARSAgent - 2 Obstacles - 0.01 Epsilon - 0.6 Gamma - 0.1 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------+\n",
      "|        Parameter         |   Value   |\n",
      "+--------------------------+-----------+\n",
      "|          Agent           | SARSAgent |\n",
      "|        Obstacles         |     2     |\n",
      "|         Epsilon          |   0.01    |\n",
      "|          Gamma           |    0.6    |\n",
      "|          Alpha           |    0.1    |\n",
      "|         Episodes         |   1000    |\n",
      "| Execution Time (seconds) |   8.48    |\n",
      "|      Success Count       |    295    |\n",
      "+--------------------------+-----------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-------------+----------------+\n",
      "|   Metric   |   Episode   |     Value      |\n",
      "+------------+-------------+----------------+\n",
      "| Max reward | Episode 22  |  reward = 100  |\n",
      "| Min reward | Episode 930 | reward = -1000 |\n",
      "| Max steps  |  Episode 0  |  steps = 100   |\n",
      "| Min steps  | Episode 22  |   steps = 1    |\n",
      "+------------+-------------+----------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        -57.875        |      101.66130234949479       |     73.713      |     42.1653239705987     |      0.295      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "SARSAgent - 2 Obstacles - 0.01 Epsilon - 0.6 Gamma - 0.5 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------+\n",
      "|        Parameter         |   Value   |\n",
      "+--------------------------+-----------+\n",
      "|          Agent           | SARSAgent |\n",
      "|        Obstacles         |     2     |\n",
      "|         Epsilon          |   0.01    |\n",
      "|          Gamma           |    0.6    |\n",
      "|          Alpha           |    0.5    |\n",
      "|         Episodes         |   1000    |\n",
      "| Execution Time (seconds) |   7.48    |\n",
      "|      Success Count       |    340    |\n",
      "+--------------------------+-----------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-----------+---------------+\n",
      "|   Metric   |  Episode  |     Value     |\n",
      "+------------+-----------+---------------+\n",
      "| Max reward | Episode 5 | reward = 100  |\n",
      "| Min reward | Episode 1 | reward = -397 |\n",
      "| Max steps  | Episode 1 |  steps = 100  |\n",
      "| Min steps  | Episode 5 |   steps = 1   |\n",
      "+------------+-----------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        -45.862        |       96.55286329991156       |     69.861      |    43.68113689821639     |      0.34       |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "SARSAgent - 2 Obstacles - 0.01 Epsilon - 0.6 Gamma - 0.9 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------+\n",
      "|        Parameter         |   Value   |\n",
      "+--------------------------+-----------+\n",
      "|          Agent           | SARSAgent |\n",
      "|        Obstacles         |     2     |\n",
      "|         Epsilon          |   0.01    |\n",
      "|          Gamma           |    0.6    |\n",
      "|          Alpha           |    0.9    |\n",
      "|         Episodes         |   1000    |\n",
      "| Execution Time (seconds) |   7.88    |\n",
      "|      Success Count       |    296    |\n",
      "+--------------------------+-----------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 11 | reward = 100  |\n",
      "| Min reward | Episode 0  | reward = -397 |\n",
      "| Max steps  | Episode 0  |  steps = 100  |\n",
      "| Min steps  | Episode 11 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        -56.089        |       92.86687965220074       |     74.348      |    41.48146101100596     |      0.296      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "SARSAgent - 2 Obstacles - 0.01 Epsilon - 0.99 Gamma - 0.1 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------+\n",
      "|        Parameter         |   Value   |\n",
      "+--------------------------+-----------+\n",
      "|          Agent           | SARSAgent |\n",
      "|        Obstacles         |     2     |\n",
      "|         Epsilon          |   0.01    |\n",
      "|          Gamma           |   0.99    |\n",
      "|          Alpha           |    0.1    |\n",
      "|         Episodes         |   1000    |\n",
      "| Execution Time (seconds) |   3.32    |\n",
      "|      Success Count       |    829    |\n",
      "+--------------------------+-----------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-------------+---------------+\n",
      "|   Metric   |   Episode   |     Value     |\n",
      "+------------+-------------+---------------+\n",
      "| Max reward | Episode 10  | reward = 100  |\n",
      "| Min reward | Episode 668 | reward = -955 |\n",
      "| Max steps  |  Episode 0  |  steps = 100  |\n",
      "| Min steps  | Episode 10  |   steps = 1   |\n",
      "+------------+-------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|         4.846         |       168.7679913932702       |     29.394      |    37.332753695628966    |      0.829      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "SARSAgent - 2 Obstacles - 0.01 Epsilon - 0.99 Gamma - 0.5 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------+\n",
      "|        Parameter         |   Value   |\n",
      "+--------------------------+-----------+\n",
      "|          Agent           | SARSAgent |\n",
      "|        Obstacles         |     2     |\n",
      "|         Epsilon          |   0.01    |\n",
      "|          Gamma           |   0.99    |\n",
      "|          Alpha           |    0.5    |\n",
      "|         Episodes         |   1000    |\n",
      "| Execution Time (seconds) |   3.65    |\n",
      "|      Success Count       |    797    |\n",
      "+--------------------------+-----------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-------------+---------------+\n",
      "|   Metric   |   Episode   |     Value     |\n",
      "+------------+-------------+---------------+\n",
      "| Max reward |  Episode 3  | reward = 100  |\n",
      "| Min reward | Episode 762 | reward = -442 |\n",
      "| Max steps  |  Episode 2  |  steps = 100  |\n",
      "| Min steps  |  Episode 3  |   steps = 1   |\n",
      "+------------+-------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        17.451         |      120.94541492918361       |      31.96      |    38.871704482945034    |      0.797      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "SARSAgent - 2 Obstacles - 0.01 Epsilon - 0.99 Gamma - 0.9 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------+\n",
      "|        Parameter         |   Value   |\n",
      "+--------------------------+-----------+\n",
      "|          Agent           | SARSAgent |\n",
      "|        Obstacles         |     2     |\n",
      "|         Epsilon          |   0.01    |\n",
      "|          Gamma           |   0.99    |\n",
      "|          Alpha           |    0.9    |\n",
      "|         Episodes         |   1000    |\n",
      "| Execution Time (seconds) |   3.68    |\n",
      "|      Success Count       |    780    |\n",
      "+--------------------------+-----------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 10 | reward = 100  |\n",
      "| Min reward | Episode 31 | reward = -424 |\n",
      "| Max steps  | Episode 31 |  steps = 100  |\n",
      "| Min steps  | Episode 10 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        13.037         |       120.8365801821208       |     33.779      |    40.03977200662289     |      0.78       |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "SARSAgent - 2 Obstacles - 0.01 Epsilon - 0.1 Gamma - 0.1 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------+\n",
      "|        Parameter         |   Value   |\n",
      "+--------------------------+-----------+\n",
      "|          Agent           | SARSAgent |\n",
      "|        Obstacles         |     2     |\n",
      "|         Epsilon          |   0.01    |\n",
      "|          Gamma           |    0.1    |\n",
      "|          Alpha           |    0.1    |\n",
      "|         Episodes         |   1000    |\n",
      "| Execution Time (seconds) |   10.29   |\n",
      "|      Success Count       |    326    |\n",
      "+--------------------------+-----------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 86 | reward = 100  |\n",
      "| Min reward | Episode 0  | reward = -325 |\n",
      "| Max steps  | Episode 0  |  steps = 100  |\n",
      "| Min steps  | Episode 86 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        -61.404        |      101.13955701897252       |     72.622      |    41.65001608450878     |      0.326      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "SARSAgent - 2 Obstacles - 0.01 Epsilon - 0.1 Gamma - 0.5 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------+\n",
      "|        Parameter         |   Value   |\n",
      "+--------------------------+-----------+\n",
      "|          Agent           | SARSAgent |\n",
      "|        Obstacles         |     2     |\n",
      "|         Epsilon          |   0.01    |\n",
      "|          Gamma           |    0.1    |\n",
      "|          Alpha           |    0.5    |\n",
      "|         Episodes         |   1000    |\n",
      "| Execution Time (seconds) |   12.03   |\n",
      "|      Success Count       |    401    |\n",
      "+--------------------------+-----------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-------------+----------------+\n",
      "|   Metric   |   Episode   |     Value      |\n",
      "+------------+-------------+----------------+\n",
      "| Max reward | Episode 54  |  reward = 100  |\n",
      "| Min reward | Episode 738 | reward = -1000 |\n",
      "| Max steps  |  Episode 2  |  steps = 100   |\n",
      "| Min steps  | Episode 54  |   steps = 1    |\n",
      "+------------+-------------+----------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        -46.75         |      107.16036974947802       |      67.84      |    42.99815392829372     |      0.401      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "SARSAgent - 2 Obstacles - 0.01 Epsilon - 0.1 Gamma - 0.9 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------+\n",
      "|        Parameter         |   Value   |\n",
      "+--------------------------+-----------+\n",
      "|          Agent           | SARSAgent |\n",
      "|        Obstacles         |     2     |\n",
      "|         Epsilon          |   0.01    |\n",
      "|          Gamma           |    0.1    |\n",
      "|          Alpha           |    0.9    |\n",
      "|         Episodes         |   1000    |\n",
      "| Execution Time (seconds) |   7.55    |\n",
      "|      Success Count       |    388    |\n",
      "+--------------------------+-----------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-----------+---------------+\n",
      "|   Metric   |  Episode  |     Value     |\n",
      "+------------+-----------+---------------+\n",
      "| Max reward | Episode 8 | reward = 100  |\n",
      "| Min reward | Episode 0 | reward = -334 |\n",
      "| Max steps  | Episode 0 |  steps = 100  |\n",
      "| Min steps  | Episode 8 |   steps = 1   |\n",
      "+------------+-----------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        -48.587        |      102.81522942834103       |     68.519      |    42.88025600850997     |      0.388      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "SARSAgent - 2 Obstacles - 0.01 Epsilon - 0.6 Gamma - 0.1 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------+\n",
      "|        Parameter         |   Value   |\n",
      "+--------------------------+-----------+\n",
      "|          Agent           | SARSAgent |\n",
      "|        Obstacles         |     2     |\n",
      "|         Epsilon          |   0.01    |\n",
      "|          Gamma           |    0.6    |\n",
      "|          Alpha           |    0.1    |\n",
      "|         Episodes         |   1000    |\n",
      "| Execution Time (seconds) |   6.80    |\n",
      "|      Success Count       |    455    |\n",
      "+--------------------------+-----------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-------------+----------------+\n",
      "|   Metric   |   Episode   |     Value      |\n",
      "+------------+-------------+----------------+\n",
      "| Max reward | Episode 27  |  reward = 100  |\n",
      "| Min reward | Episode 834 | reward = -1000 |\n",
      "| Max steps  |  Episode 0  |  steps = 100   |\n",
      "| Min steps  | Episode 27  |   steps = 1    |\n",
      "+------------+-------------+----------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        -41.31         |       115.7918462226579       |     62.722      |    44.36750152072113     |      0.455      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "SARSAgent - 2 Obstacles - 0.01 Epsilon - 0.6 Gamma - 0.5 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------+\n",
      "|        Parameter         |   Value   |\n",
      "+--------------------------+-----------+\n",
      "|          Agent           | SARSAgent |\n",
      "|        Obstacles         |     2     |\n",
      "|         Epsilon          |   0.01    |\n",
      "|          Gamma           |    0.6    |\n",
      "|          Alpha           |    0.5    |\n",
      "|         Episodes         |   1000    |\n",
      "| Execution Time (seconds) |   6.20    |\n",
      "|      Success Count       |    520    |\n",
      "+--------------------------+-----------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-------------+----------------+\n",
      "|   Metric   |   Episode   |     Value      |\n",
      "+------------+-------------+----------------+\n",
      "| Max reward | Episode 55  |  reward = 100  |\n",
      "| Min reward | Episode 273 | reward = -1000 |\n",
      "| Max steps  |  Episode 1  |  steps = 100   |\n",
      "| Min steps  | Episode 55  |   steps = 1    |\n",
      "+------------+-------------+----------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        -19.881        |      110.26395139835131       |     55.638      |    45.35853119703451     |      0.52       |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "SARSAgent - 2 Obstacles - 0.01 Epsilon - 0.6 Gamma - 0.9 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------+\n",
      "|        Parameter         |   Value   |\n",
      "+--------------------------+-----------+\n",
      "|          Agent           | SARSAgent |\n",
      "|        Obstacles         |     2     |\n",
      "|         Epsilon          |   0.01    |\n",
      "|          Gamma           |    0.6    |\n",
      "|          Alpha           |    0.9    |\n",
      "|         Episodes         |   1000    |\n",
      "| Execution Time (seconds) |   7.18    |\n",
      "|      Success Count       |    418    |\n",
      "+--------------------------+-----------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 41 | reward = 100  |\n",
      "| Min reward | Episode 0  | reward = -325 |\n",
      "| Max steps  | Episode 0  |  steps = 100  |\n",
      "| Min steps  | Episode 41 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        -41.694        |      104.53948951888405       |     65.144      |    43.92214041253476     |      0.418      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "SARSAgent - 2 Obstacles - 0.01 Epsilon - 0.99 Gamma - 0.1 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------+\n",
      "|        Parameter         |   Value   |\n",
      "+--------------------------+-----------+\n",
      "|          Agent           | SARSAgent |\n",
      "|        Obstacles         |     2     |\n",
      "|         Epsilon          |   0.01    |\n",
      "|          Gamma           |   0.99    |\n",
      "|          Alpha           |    0.1    |\n",
      "|         Episodes         |   1000    |\n",
      "| Execution Time (seconds) |   3.48    |\n",
      "|      Success Count       |    867    |\n",
      "+--------------------------+-----------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-------------+---------------+\n",
      "|   Metric   |   Episode   |     Value     |\n",
      "+------------+-------------+---------------+\n",
      "| Max reward | Episode 44  | reward = 100  |\n",
      "| Min reward | Episode 204 | reward = -712 |\n",
      "| Max steps  |  Episode 3  |  steps = 100  |\n",
      "| Min steps  | Episode 44  |   steps = 1   |\n",
      "+------------+-------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        10.769         |      146.86215350161228       |     27.428      |    34.387924411708035    |      0.867      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "SARSAgent - 2 Obstacles - 0.01 Epsilon - 0.99 Gamma - 0.5 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------+\n",
      "|        Parameter         |   Value   |\n",
      "+--------------------------+-----------+\n",
      "|          Agent           | SARSAgent |\n",
      "|        Obstacles         |     2     |\n",
      "|         Epsilon          |   0.01    |\n",
      "|          Gamma           |   0.99    |\n",
      "|          Alpha           |    0.5    |\n",
      "|         Episodes         |   1000    |\n",
      "| Execution Time (seconds) |   3.26    |\n",
      "|      Success Count       |    851    |\n",
      "+--------------------------+-----------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 21 | reward = 100  |\n",
      "| Min reward | Episode 1  | reward = -424 |\n",
      "| Max steps  | Episode 0  |  steps = 100  |\n",
      "| Min steps  | Episode 21 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        23.905         |      116.68437435328784       |      27.92      |    35.40546775777926     |      0.851      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "SARSAgent - 2 Obstacles - 0.01 Epsilon - 0.99 Gamma - 0.9 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------+\n",
      "|        Parameter         |   Value   |\n",
      "+--------------------------+-----------+\n",
      "|          Agent           | SARSAgent |\n",
      "|        Obstacles         |     2     |\n",
      "|         Epsilon          |   0.01    |\n",
      "|          Gamma           |   0.99    |\n",
      "|          Alpha           |    0.9    |\n",
      "|         Episodes         |   1000    |\n",
      "| Execution Time (seconds) |   3.30    |\n",
      "|      Success Count       |    846    |\n",
      "+--------------------------+-----------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-------------+---------------+\n",
      "|   Metric   |   Episode   |     Value     |\n",
      "+------------+-------------+---------------+\n",
      "| Max reward | Episode 14  | reward = 100  |\n",
      "| Min reward | Episode 234 | reward = -451 |\n",
      "| Max steps  |  Episode 0  |  steps = 100  |\n",
      "| Min steps  | Episode 14  |   steps = 1   |\n",
      "+------------+-------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        20.075         |      118.05527698253654       |     29.299      |    35.77845296678849     |      0.846      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "SARSAgent - 2 Obstacles - 0.01 Epsilon - 0.1 Gamma - 0.1 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------+\n",
      "|        Parameter         |   Value   |\n",
      "+--------------------------+-----------+\n",
      "|          Agent           | SARSAgent |\n",
      "|        Obstacles         |     2     |\n",
      "|         Epsilon          |   0.01    |\n",
      "|          Gamma           |    0.1    |\n",
      "|          Alpha           |    0.1    |\n",
      "|         Episodes         |   1000    |\n",
      "| Execution Time (seconds) |   9.10    |\n",
      "|      Success Count       |    324    |\n",
      "+--------------------------+-----------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-----------+---------------+\n",
      "|   Metric   |  Episode  |     Value     |\n",
      "+------------+-----------+---------------+\n",
      "| Max reward | Episode 8 | reward = 100  |\n",
      "| Min reward | Episode 0 | reward = -334 |\n",
      "| Max steps  | Episode 0 |  steps = 100  |\n",
      "| Min steps  | Episode 8 |   steps = 1   |\n",
      "+------------+-----------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        -65.936        |      104.44836698276026       |     73.889      |    40.827436345655585    |      0.324      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "SARSAgent - 2 Obstacles - 0.01 Epsilon - 0.1 Gamma - 0.5 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------+\n",
      "|        Parameter         |   Value   |\n",
      "+--------------------------+-----------+\n",
      "|          Agent           | SARSAgent |\n",
      "|        Obstacles         |     2     |\n",
      "|         Epsilon          |   0.01    |\n",
      "|          Gamma           |    0.1    |\n",
      "|          Alpha           |    0.5    |\n",
      "|         Episodes         |   1000    |\n",
      "| Execution Time (seconds) |   7.83    |\n",
      "|      Success Count       |    389    |\n",
      "+--------------------------+-----------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 25 | reward = 100  |\n",
      "| Min reward | Episode 8  | reward = -334 |\n",
      "| Max steps  | Episode 0  |  steps = 100  |\n",
      "| Min steps  | Episode 25 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        -50.871        |      105.40361607308016       |     68.166      |    42.92193770948298     |      0.389      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "SARSAgent - 2 Obstacles - 0.01 Epsilon - 0.1 Gamma - 0.9 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------+\n",
      "|        Parameter         |   Value   |\n",
      "+--------------------------+-----------+\n",
      "|          Agent           | SARSAgent |\n",
      "|        Obstacles         |     2     |\n",
      "|         Epsilon          |   0.01    |\n",
      "|          Gamma           |    0.1    |\n",
      "|          Alpha           |    0.9    |\n",
      "|         Episodes         |   1000    |\n",
      "| Execution Time (seconds) |   7.26    |\n",
      "|      Success Count       |    465    |\n",
      "+--------------------------+-----------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 67 | reward = 100  |\n",
      "| Min reward | Episode 4  | reward = -334 |\n",
      "| Max steps  | Episode 4  |  steps = 100  |\n",
      "| Min steps  | Episode 67 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        -35.682        |      105.54979694739178       |     62.356      |    43.83741533191691     |      0.465      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "SARSAgent - 2 Obstacles - 0.01 Epsilon - 0.6 Gamma - 0.1 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------+\n",
      "|        Parameter         |   Value   |\n",
      "+--------------------------+-----------+\n",
      "|          Agent           | SARSAgent |\n",
      "|        Obstacles         |     2     |\n",
      "|         Epsilon          |   0.01    |\n",
      "|          Gamma           |    0.6    |\n",
      "|          Alpha           |    0.1    |\n",
      "|         Episodes         |   1000    |\n",
      "| Execution Time (seconds) |   6.75    |\n",
      "|      Success Count       |    531    |\n",
      "+--------------------------+-----------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-------------+----------------+\n",
      "|   Metric   |   Episode   |     Value      |\n",
      "+------------+-------------+----------------+\n",
      "| Max reward |  Episode 5  |  reward = 100  |\n",
      "| Min reward | Episode 188 | reward = -1000 |\n",
      "| Max steps  |  Episode 0  |  steps = 100   |\n",
      "| Min steps  |  Episode 5  |   steps = 1    |\n",
      "+------------+-------------+----------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        -28.348        |      118.41403962698334       |     56.243      |    44.83518869309179     |      0.531      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "SARSAgent - 2 Obstacles - 0.01 Epsilon - 0.6 Gamma - 0.5 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------+\n",
      "|        Parameter         |   Value   |\n",
      "+--------------------------+-----------+\n",
      "|          Agent           | SARSAgent |\n",
      "|        Obstacles         |     2     |\n",
      "|         Epsilon          |   0.01    |\n",
      "|          Gamma           |    0.6    |\n",
      "|          Alpha           |    0.5    |\n",
      "|         Episodes         |   1000    |\n",
      "| Execution Time (seconds) |   7.14    |\n",
      "|      Success Count       |    445    |\n",
      "+--------------------------+-----------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 35 | reward = 100  |\n",
      "| Min reward | Episode 3  | reward = -370 |\n",
      "| Max steps  | Episode 1  |  steps = 100  |\n",
      "| Min steps  | Episode 35 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        -37.36         |      104.99835739217568       |     62.678      |    44.144842089230345    |      0.445      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "SARSAgent - 2 Obstacles - 0.01 Epsilon - 0.6 Gamma - 0.9 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------+\n",
      "|        Parameter         |   Value   |\n",
      "+--------------------------+-----------+\n",
      "|          Agent           | SARSAgent |\n",
      "|        Obstacles         |     2     |\n",
      "|         Epsilon          |   0.01    |\n",
      "|          Gamma           |    0.6    |\n",
      "|          Alpha           |    0.9    |\n",
      "|         Episodes         |   1000    |\n",
      "| Execution Time (seconds) |   7.59    |\n",
      "|      Success Count       |    395    |\n",
      "+--------------------------+-----------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 83 | reward = 100  |\n",
      "| Min reward | Episode 2  | reward = -415 |\n",
      "| Max steps  | Episode 2  |  steps = 100  |\n",
      "| Min steps  | Episode 83 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        -50.978        |      100.84405306729317       |     68.681      |    41.95678265323273     |      0.395      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "SARSAgent - 2 Obstacles - 0.01 Epsilon - 0.99 Gamma - 0.1 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------+\n",
      "|        Parameter         |   Value   |\n",
      "+--------------------------+-----------+\n",
      "|          Agent           | SARSAgent |\n",
      "|        Obstacles         |     2     |\n",
      "|         Epsilon          |   0.01    |\n",
      "|          Gamma           |   0.99    |\n",
      "|          Alpha           |    0.1    |\n",
      "|         Episodes         |   1000    |\n",
      "| Execution Time (seconds) |   3.06    |\n",
      "|      Success Count       |    870    |\n",
      "+--------------------------+-----------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-------------+----------------+\n",
      "|   Metric   |   Episode   |     Value      |\n",
      "+------------+-------------+----------------+\n",
      "| Max reward | Episode 65  |  reward = 100  |\n",
      "| Min reward | Episode 417 | reward = -1000 |\n",
      "| Max steps  |  Episode 0  |  steps = 100   |\n",
      "| Min steps  | Episode 65  |   steps = 1    |\n",
      "+------------+-------------+----------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|         9.937         |      149.38565897068602       |     27.128      |    34.14562255311125     |      0.87       |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "SARSAgent - 2 Obstacles - 0.01 Epsilon - 0.99 Gamma - 0.5 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------+\n",
      "|        Parameter         |   Value   |\n",
      "+--------------------------+-----------+\n",
      "|          Agent           | SARSAgent |\n",
      "|        Obstacles         |     2     |\n",
      "|         Epsilon          |   0.01    |\n",
      "|          Gamma           |   0.99    |\n",
      "|          Alpha           |    0.5    |\n",
      "|         Episodes         |   1000    |\n",
      "| Execution Time (seconds) |   2.63    |\n",
      "|      Success Count       |    899    |\n",
      "+--------------------------+-----------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+-------------+---------------+\n",
      "|   Metric   |   Episode   |     Value     |\n",
      "+------------+-------------+---------------+\n",
      "| Max reward | Episode 30  | reward = 100  |\n",
      "| Min reward | Episode 506 | reward = -388 |\n",
      "| Max steps  |  Episode 0  |  steps = 100  |\n",
      "| Min steps  | Episode 30  |   steps = 1   |\n",
      "+------------+-------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|         36.26         |      104.59128901461028       |     23.788      |    32.26071620112935     |      0.899      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n",
      "SARSAgent - 2 Obstacles - 0.01 Epsilon - 0.99 Gamma - 0.9 Alpha\n",
      "\n",
      "Training Details:\n",
      "+--------------------------+-----------+\n",
      "|        Parameter         |   Value   |\n",
      "+--------------------------+-----------+\n",
      "|          Agent           | SARSAgent |\n",
      "|        Obstacles         |     2     |\n",
      "|         Epsilon          |   0.01    |\n",
      "|          Gamma           |   0.99    |\n",
      "|          Alpha           |    0.9    |\n",
      "|         Episodes         |   1000    |\n",
      "| Execution Time (seconds) |   3.89    |\n",
      "|      Success Count       |    833    |\n",
      "+--------------------------+-----------+\n",
      "\n",
      "Performance Metrics:\n",
      "+------------+------------+---------------+\n",
      "|   Metric   |  Episode   |     Value     |\n",
      "+------------+------------+---------------+\n",
      "| Max reward | Episode 48 | reward = 100  |\n",
      "| Min reward | Episode 9  | reward = -379 |\n",
      "| Max steps  | Episode 1  |  steps = 100  |\n",
      "| Min steps  | Episode 48 |   steps = 1   |\n",
      "+------------+------------+---------------+\n",
      "\n",
      "Performance Table:\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "| Média das Recompensas | Desvio Padrão das Recompensas | Média de Passos | Desvio Padrão dos Passos | Taxa de Sucesso |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "|        16.221         |      115.00845666343021       |      31.57      |    36.84074293958172     |      0.833      |\n",
      "+-----------------------+-------------------------------+-----------------+--------------------------+-----------------+\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Define the grid of parameters to test\n",
    "param_grid = {\n",
    "    'epsilon': [0.1, 0.5, 0.6],\n",
    "    'gamma': [0.1, 0.6, 0.99],\n",
    "    'alpha': [0.1, 0.5, 0.9]\n",
    "}\n",
    "\n",
    "# Number of obstacles to test\n",
    "num_obstacles_list = [0, 2]\n",
    "\n",
    "# Number of episodes to run for each experiment\n",
    "episodes = 1000\n",
    "\n",
    "# Number of maze to test\n",
    "maze = 3\n",
    "\n",
    "# Run the experiments\n",
    "run_experiments(SARSAgent, maze, episodes, num_obstacles_list, param_grid, save_plots=False, show_plots=False, render=False, max_steps=100)\n",
    "\n",
    "# Open the CSV file with the results\n",
    "df = pd.read_csv(f'results/SARSAgent.csv')\n",
    "# Converte as strings de volta para arrays do NumPy\n",
    "df['rewards'] = df['rewards'].apply(literal_eval)\n",
    "\n",
    "# Pega o primeiro treinado para avaliar\n",
    "rewards_deterministic = df[df['num_obstacles'] == 0]['rewards'].values[0]\n",
    "rewards_stochastic = df[df['num_obstacles'] == 2]['rewards'].values[0]\n",
    "plot_deterministic_vs_stochastic(rewards_deterministic, rewards_stochastic,method=\"SARSAgent\" ,show_plots=False, save_plots=False)\n",
    "\n",
    "# Expande a coluna 'rewards', que contém listas, em várias linhas\n",
    "df = df.explode('rewards')\n",
    "# Converte os valores de 'rewards' para numéricos\n",
    "df['rewards'] = pd.to_numeric(df['rewards'])\n",
    "# Agrupa por 'alpha' e calcula a média das recompensas\n",
    "grouped = df.groupby('alpha')['rewards'].mean()\n",
    "# Prepara os dados para o gráfico\n",
    "alpha_values = grouped.index.tolist()\n",
    "average_rewards = grouped.values.tolist()\n",
    "plot_sensitivity_analysis(alpha_values, average_rewards,parameter_name='alpha',metric_name='Recompensa Média',method=\"SARSAgent\",title='Análise de Sensibilidade para Alpha', show_plots=False, save_plots=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1) Experimento do SARSA com Aproximação de função linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8) Comparação entre os algoritmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the CSV file with the results\n",
    "df_MonteCarloAgent = pd.read_csv(f'results/MonteCarloAgent.csv')\n",
    "df_QLearningAgent = pd.read_csv(f'results/QLearningAgent.csv')\n",
    "df_SARSAAgent = pd.read_csv(f'results/SARSAgent.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Comparação do Tempo de Execução\n",
    "Compara o tempo de execução de cada método."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "times = [df_MonteCarloAgent['execution_time'].mean(), df_QLearningAgent['execution_time'].mean(), df_SARSAAgent['execution_time'].mean()]\n",
    "methods = ['Monte Carlo', 'Q-Learning', 'SARSA']\n",
    "\n",
    "plt.bar(methods, times)\n",
    "plt.xlabel('Método')\n",
    "plt.ylabel('Tempo Médio de Execução (s)')\n",
    "plt.title('Comparação do Tempo de Execução')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Comparação da Taxa de Sucesso\n",
    "Compara a taxa de sucesso de cada método."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "success_rates = [df_MonteCarloAgent['success_count'].mean(), df_QLearningAgent['success_count'].mean(), df_SARSAAgent['success_count'].mean()]\n",
    "\n",
    "plt.bar(methods, success_rates)\n",
    "plt.xlabel('Método')\n",
    "plt.ylabel('Taxa Média de Sucesso')\n",
    "plt.title('Comparação da Taxa de Sucesso')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Melhores Conjuntos de Parâmetros\n",
    "Identifica e exibe os melhores conjuntos de parâmetros para cada método baseado em critérios como recompensa total ou taxa de sucesso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores Parâmetros Monte Carlo:\n",
      " execution_time                                             0.159519\n",
      "alpha                                                           0.5\n",
      "gamma                                                          0.99\n",
      "epsilon                                                         0.5\n",
      "num_obstacles                                                     0\n",
      "success_count                                                   998\n",
      "failure_count                                                     2\n",
      "rewards           [-456, -165, -339, -86, -87, 81, 88, -83, -317...\n",
      "steps             [100, 77, 100, 34, 80, 11, 4, 49, 94, 13, 26, ...\n",
      "visits            {(0, 0): 209, (0, 1): 206, (0, 2): 192, (0, 3)...\n",
      "Name: 16, dtype: object\n",
      "Melhores Parâmetros Q-Learning:\n",
      " execution_time                                             0.067357\n",
      "alpha                                                           0.9\n",
      "gamma                                                           0.6\n",
      "epsilon                                                         0.1\n",
      "num_obstacles                                                     0\n",
      "success_count                                                  1000\n",
      "failure_count                                                     0\n",
      "rewards           [66, -147, -67, 86, 79, 10, 83, 100, 58, 92, 9...\n",
      "steps             [17, 86, 87, 6, 13, 64, 18, 1, 34, 9, 3, 13, 6...\n",
      "visits            {(0, 0): 14, (0, 1): 20, (0, 2): 22, (0, 3): 5...\n",
      "Name: 5, dtype: object\n",
      "Melhores Parâmetros SARSA:\n",
      " execution_time                                             0.657777\n",
      "alpha                                                           0.5\n",
      "gamma                                                           0.6\n",
      "epsilon                                                         0.1\n",
      "num_obstacles                                                     0\n",
      "success_count                                                  1000\n",
      "failure_count                                                     0\n",
      "rewards           [-208, 9, 92, 30, 94, -82, -33, 87, 97, 93, 88...\n",
      "steps             [84, 29, 9, 35, 7, 66, 62, 14, 4, 8, 13, 7, 1,...\n",
      "visits            {(0, 0): 52, (0, 1): 23, (0, 2): 19, (0, 3): 5...\n",
      "Name: 4, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Exemplo: identifica o conjunto de parâmetros com a maior taxa de sucesso para cada método\n",
    "best_params_monte_carlo = df_MonteCarloAgent.loc[df_MonteCarloAgent['success_count'].idxmax()]\n",
    "best_params_q_learning = df_QLearningAgent.loc[df_QLearningAgent['success_count'].idxmax()]\n",
    "best_params_sarsa = df_SARSAAgent.loc[df_SARSAAgent['success_count'].idxmax()]\n",
    "\n",
    "# Imprime os melhores conjuntos de parâmetros\n",
    "print(\"Melhores Parâmetros Monte Carlo:\\n\", best_params_monte_carlo)\n",
    "print(\"Melhores Parâmetros Q-Learning:\\n\", best_params_q_learning)\n",
    "print(\"Melhores Parâmetros SARSA:\\n\", best_params_sarsa)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
