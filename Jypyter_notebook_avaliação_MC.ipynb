{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e95def91",
   "metadata": {},
   "source": [
    "# MC934B/MO436A - Reinforcement Learning - Project_1 - 2s 2023\n",
    "\n",
    "# Equipe : Israel Silva | Jorge Frasson | Juliano Soares | Mario\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1d68aa",
   "metadata": {},
   "source": [
    "## Descrevendo o Ambiente: \n",
    "\n",
    "### Consiste em um labirinto construido com a biblioteca PyGame com as devidas adaptações para acomodar nossa necessidade de avaliação.  Deixamos o código parametrizável de modo a poder fazer experimentos em cenários diferentes de labirintos com obstáculos e matrizes de tamanhos diferentes. \n",
    "### O objetivo no labirinto é: dado um destino especificado, encontrar o melhor caminho (ou melhor política) a partir de um ponto de partida escolhido randomicamente. \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b40ec44",
   "metadata": {},
   "source": [
    "## 1) Código comum para todos os algorítmos de RL utilizados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b91f02",
   "metadata": {},
   "source": [
    "### 1.1) Importações e Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934042a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gymnasium\n",
    "!pip install numpy\n",
    "!pip install pygame\n",
    "!pip install random\n",
    "!pip install tqdm\n",
    "!pip install matplotlib\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import pygame\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda373c8",
   "metadata": {},
   "source": [
    "### 1.2) Ambiente gráfico do labirinto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12d95224",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MazeGameEnv(gym.Env):\n",
    "    def __init__(self, maze):\n",
    "        super(MazeGameEnv, self).__init__()\n",
    "        self.maze = np.array(maze)\n",
    "        self.start_pos = tuple(np.argwhere(self.maze == 'S')[0])\n",
    "        self.goal_pos = tuple(np.argwhere(self.maze == 'G')[0])\n",
    "        self.current_pos = self.start_pos\n",
    "        self.num_rows, self.num_cols = self.maze.shape\n",
    "\n",
    "        # Define ação como Discrete com 4 ações (cima, baixo, esquerda, direita)\n",
    "        self.action_space = gym.spaces.Discrete(4)\n",
    "\n",
    "        # Defina o espaço de observação como uma tupla com o número de linhas e colunas\n",
    "        self.observation_space = gym.spaces.Tuple((gym.spaces.Discrete(self.num_rows), gym.spaces.Discrete(self.num_cols)))\n",
    "\n",
    "        # Inicialize o ambiente Pygame\n",
    "        pygame.init()\n",
    "\n",
    "        # Defina cores\n",
    "        self.WHITE = (255, 255, 255)\n",
    "        self.GREEN = (0, 255, 0)\n",
    "        self.RED = (255, 0, 0)\n",
    "        self.BLACK = (0, 0, 0)\n",
    "        self.PURPLE = (255, 0, 255)\n",
    "        self.BLUE = (0, 0, 255)\n",
    "\n",
    "    def reset(self, num_obstacles = 0, fixed_start_pos = None):\n",
    "        self.reset_obstacles(num_obstacles)\n",
    "        if fixed_start_pos is not None:\n",
    "            row, col = fixed_start_pos\n",
    "        else:\n",
    "            row, col = np.random.randint(0, self.num_rows), np.random.randint(0, self.num_cols)\n",
    "            while self.maze[row, col] in ['#', 'G']:  # Garante que a posição inicial não é um obstáculo ou a posição final\n",
    "                row, col = np.random.randint(0, self.num_rows), np.random.randint(0, self.num_cols)\n",
    "        self.maze[self.start_pos] = '.'  # Limpa a posição inicial antiga\n",
    "        self.start_pos = (row, col)\n",
    "        self.maze[row, col] = 'S'  # Define a nova posição inicial\n",
    "        self.current_pos = self.start_pos\n",
    "        return self.current_pos\n",
    "    \n",
    "    def reset_obstacles(self, num_obstacles = 0):\n",
    "        for row in range(self.num_rows):\n",
    "            for col in range(self.num_cols):\n",
    "                if self.maze[row, col] == 'R':\n",
    "                    self.maze[row, col] = '.'\n",
    "        \n",
    "        for _ in range(num_obstacles):\n",
    "            row, col = np.random.randint(0, self.num_rows), np.random.randint(0, self.num_cols)\n",
    "            while self.maze[row, col] in ['S', 'G', '#','R']:\n",
    "                row, col = np.random.randint(0, self.num_rows), np.random.randint(0, self.num_cols)\n",
    "            self.maze[row, col] = 'R'\n",
    "\n",
    "    def step(self, action, reward = 1):\n",
    "        old_pos = self.current_pos\n",
    "            \n",
    "        if action == 0:  # Cima\n",
    "            self.move('up')\n",
    "        elif action == 1:  # Baixo\n",
    "            self.move('down')\n",
    "        elif action == 2:  # Esquerda\n",
    "            self.move('left')\n",
    "        elif action == 3:  # Direita\n",
    "            self.move('right')\n",
    "\n",
    "        obs = self.current_pos\n",
    "        #\n",
    "        # reward = 1 if self.current_pos == self.goal_pos else 0\n",
    "        # distance_to_goal = abs(self.current_pos[0] - self.goal_pos[0]) + abs(self.current_pos[1] - self.goal_pos[1])\n",
    "        # reward = 1 / (distance_to_goal + 1)\n",
    "        # done = self.current_pos == self.goal_pos\n",
    "        #\n",
    "        # max_distance = self.num_rows + self.num_cols - 2\n",
    "        # if self.current_pos == old_pos:\n",
    "        #     reward = -max_distance  # Penalidade por ação inválida\n",
    "        #     done = False\n",
    "        # else:\n",
    "        #     if self.current_pos == self.goal_pos:\n",
    "        #         reward = max_distance\n",
    "        #         done = True\n",
    "        #     else:\n",
    "        #         distance_to_goal = abs(self.current_pos[0] - self.goal_pos[0]) + abs(self.current_pos[1] - self.goal_pos[1])\n",
    "        #         reward = -1 + (1 - distance_to_goal / max_distance)\n",
    "        #         # reward = -distance_to_goal\n",
    "        #         done = False\n",
    "        \n",
    "        new_pos = self.current_pos\n",
    "\n",
    "    # Recompensas e penalidades\n",
    "        if new_pos == self.goal_pos:\n",
    "            reward = 100  # Recompensa por atingir o objetivo\n",
    "            done = True\n",
    "        elif new_pos == old_pos:\n",
    "            reward = -10  # Penalidade por bater em um obstáculo ou parede\n",
    "            done = False\n",
    "        else:\n",
    "            reward = -1  # Penalidade por cada movimento\n",
    "            done = False\n",
    "        \n",
    "        info = {}\n",
    "\n",
    "        return new_pos, reward, done, info\n",
    "    \n",
    "    def move(self, action):\n",
    "        new_pos = list(self.current_pos)\n",
    "\n",
    "        if action == 'up':\n",
    "            new_pos[0] -= 1\n",
    "        elif action == 'down':\n",
    "            new_pos[0] += 1\n",
    "        elif action == 'left':\n",
    "            new_pos[1] -= 1\n",
    "        elif action == 'right':\n",
    "            new_pos[1] += 1\n",
    "\n",
    "        new_pos = tuple(new_pos)\n",
    "\n",
    "        if self.is_valid_position(new_pos[0], new_pos[1]):\n",
    "            self.current_pos = new_pos\n",
    "\n",
    "    def is_valid_position(self, row, col):\n",
    "        return 0 <= row < self.num_rows and 0 <= col < self.num_cols and self.maze[row, col] != '#' and self.maze[row, col] != 'R'\n",
    "\n",
    "\n",
    "    def render(self, path=None):\n",
    "        self.draw_maze(path)\n",
    "        pygame.display.update()\n",
    "        \n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "            elif event.type == pygame.KEYDOWN:\n",
    "                if event.key == pygame.K_ESCAPE:\n",
    "                    pygame.quit()\n",
    "                    sys.exit()\n",
    "\n",
    "    def play_path(self, path, delay=0.5):\n",
    "        for position in path:\n",
    "            self.current_pos = position\n",
    "            self.render(path = path)\n",
    "            time.sleep(delay)\n",
    "\n",
    "    def draw_maze(self, path = None):\n",
    "        self.cell_size = 50\n",
    "        self.screen_width = self.num_cols * self.cell_size\n",
    "        self.screen_height = self.num_rows * self.cell_size\n",
    "        self.screen = pygame.display.set_mode((self.screen_width, self.screen_height))\n",
    "        self.screen.fill(self.WHITE)\n",
    "        \n",
    "        for row in range(self.num_rows):\n",
    "            for col in range(self.num_cols):\n",
    "                cell_left = col * self.cell_size\n",
    "                cell_top = row * self.cell_size\n",
    "\n",
    "                if self.maze[row, col] == '#' or self.maze[row, col] == 'R':\n",
    "                    pygame.draw.rect(self.screen, self.BLACK, (cell_left, cell_top, self.cell_size, self.cell_size))\n",
    "                elif self.maze[row, col] == 'S':\n",
    "                    pygame.draw.rect(self.screen, self.GREEN, (cell_left, cell_top, self.cell_size, self.cell_size))\n",
    "                elif self.maze[row, col] == 'G':\n",
    "                    pygame.draw.rect(self.screen, self.RED, (cell_left, cell_top, self.cell_size, self.cell_size))\n",
    "                    \n",
    "        if path:\n",
    "            for i in range(len(path)-1):\n",
    "                start = path[i]\n",
    "                end = path[i+1]\n",
    "                self.draw_line(start, end)\n",
    "\n",
    "        current_row, current_col = self.current_pos\n",
    "        pygame.draw.rect(self.screen, self.PURPLE, (current_col * self.cell_size, current_row * self.cell_size, self.cell_size, self.cell_size))\n",
    "\n",
    "    def draw_line(self, start, end):\n",
    "        start_x = start[1] * self.cell_size + self.cell_size // 2\n",
    "        start_y = start[0] * self.cell_size + self.cell_size // 2\n",
    "        end_x = end[1] * self.cell_size + self.cell_size // 2\n",
    "        end_y = end[0] * self.cell_size + self.cell_size // 2\n",
    "\n",
    "        pygame.draw.line(self.screen, self.BLUE, (start_x, start_y), (end_x, end_y), 2)\n",
    "        \n",
    "    def draw_arrows(self, Q):\n",
    "        arrow_image = pygame.image.load('arrow.png')  # Carrega a imagem da seta\n",
    "        arrow_image = pygame.transform.scale(arrow_image, (self.cell_size, self.cell_size))  # Redimensiona para caber na célula\n",
    "\n",
    "        for row in range(self.num_rows):\n",
    "            for col in range(self.num_cols):\n",
    "                state = (row, col)\n",
    "                if self.maze[row][col] in ['.', 'R']:  # Não desenhar em '#', 'S' ou 'G'\n",
    "                    best_action = max(Q[state], key=Q[state].get)\n",
    "                    center = (col * self.cell_size, row * self.cell_size)\n",
    "\n",
    "                    if best_action == 0:  # Cima\n",
    "                        rotated_image = pygame.transform.rotate(arrow_image, 90)\n",
    "                    elif best_action == 1:  # Baixo\n",
    "                        rotated_image = pygame.transform.rotate(arrow_image, -90)\n",
    "                    elif best_action == 2:  # Esquerda\n",
    "                        rotated_image = pygame.transform.rotate(arrow_image, 180)\n",
    "                    elif best_action == 3:  # Direita\n",
    "                        rotated_image = arrow_image  # Não precisa rotacionar\n",
    "\n",
    "                    self.screen.blit(rotated_image, center)  # Desenha a imagem rotacionada\n",
    "\n",
    "\n",
    "        \n",
    "    def close(self):\n",
    "        pygame.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc2b505",
   "metadata": {},
   "source": [
    "### 1.3) Funções comuns para extração de dados e plotagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79856f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(episodes_data):\n",
    "    rewards = [info['reward'] for episode, info in episodes_data.items()]\n",
    "    steps = [info['steps'] for episode, info in episodes_data.items()]\n",
    "    return rewards, steps\n",
    "\n",
    "\n",
    "def plot_parameter_variation(agent_class, env, parameter_values, parameter_name, episodes, num_obstacles):\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 5))\n",
    "    for value in parameter_values:\n",
    "        agent = agent_class(env, **{parameter_name: value})\n",
    "        episodes_data = agent.train(episodes, num_obstacles=num_obstacles)\n",
    "        rewards, steps = extract_data(episodes_data)\n",
    "        axes[0].plot(steps, label=f'{parameter_name}={value}')\n",
    "        axes[1].plot(rewards, label=f'{parameter_name}={value}')\n",
    "    axes[0].set_title(f'Number of Steps - Variation of {parameter_name.capitalize()}')\n",
    "    axes[1].set_title(f'Total Reward - Variation of {parameter_name.capitalize()}')\n",
    "    for ax in axes:\n",
    "        ax.set_xlabel('Episode')\n",
    "        ax.set_ylabel('Value')\n",
    "        ax.legend()\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328ec43e",
   "metadata": {},
   "source": [
    "### 1.4) Função para criação do espaço labiríntico a ser analisado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b230b97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mazes = [\n",
    "    [\n",
    "    ['S', '.', '.', '.', '.', '.', '.', '.', '.', '.'],\n",
    "    ['.', '.', '.', '.', '#', '.', '#', '.', '.', '.'],\n",
    "    ['.', '.', '.', '.', '#', '.', '.', '.', '.', '.'],\n",
    "    ['.', '.', '.', '.', '#', '.', '.', '.', '#', '.'],\n",
    "    ['.', '.', '.', '.', '#', '.', '.', '.', '.', '.'],\n",
    "    ['.', '#', '#', '#', '#', '.', '.', '.', '.', '.'],\n",
    "    ['.', '.', '.', '.', '.', '.', '.', '.', '.', '.'],\n",
    "    ['.', '.', '.', '.', '.', '#', '.', '.', '.', '.'],\n",
    "    ['.', '.', '.', '.', '.', '.', '.', '.', '.', '.'],\n",
    "    ['.', '#', '.', '.', '.', '.', '.', '.', '.', 'G'],\n",
    "    ],\n",
    "    [\n",
    "    ['S','.','.','.','.','.'],\n",
    "    ['.','.','.','#','.','.'],\n",
    "    ['.','.','.','#','.','.'],\n",
    "    ['.','.','.','#','.','.'],\n",
    "    ['.','#','#','#','.','.'],\n",
    "    ['.','.','.','.','.','G']\n",
    "    ],\n",
    "    [\n",
    "    ['S','.','#','.','#','.'],\n",
    "    ['.','.','.','.','#','.'],\n",
    "    ['.','#','#','.','#','.'],\n",
    "    ['.','#','#','.','#','.'],\n",
    "    ['.','.','.','.','#','.'],\n",
    "    ['#','.','.','.','.','G']  \n",
    "    ],\n",
    "    [\n",
    "    ['S','.','.','.','.','.'],\n",
    "    ['.','.','.','.','.','.'],\n",
    "    ['.','.','#','#','.','.'],\n",
    "    ['.','.','#','#','.','.'],\n",
    "    ['.','.','.','.','.','.'],\n",
    "    ['.','.','.','.','.','G']\n",
    "    ]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dea4fe",
   "metadata": {},
   "source": [
    "## 2) Função Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7dd9f9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo ----------------------------------------------        \n",
    "\n",
    "class MonteCarloAgent:\n",
    "    def __init__(self, env, alpha = 0.05, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01):\n",
    "        self.env = env\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.Q = {}\n",
    "        self.returns = {}  # Dicionário para armazenar retornos para cada estado-ação\n",
    "        self.initialize_Q_and_returns()\n",
    "    \n",
    "    def initialize_Q_and_returns(self):\n",
    "        for row in range(self.env.num_rows):\n",
    "            for col in range(self.env.num_cols):\n",
    "                self.Q[(row, col)] = {}\n",
    "                self.returns[(row, col)] = {}\n",
    "                for a in range(self.env.action_space.n): #traz o numero da Discretização(movimentos possíveis) definida em gym.spaces.Discrete(4) como 4 \n",
    "                    # Inicialização ligeiramente aleatória para promover a exploração inicial\n",
    "                    # self.Q[(row, col)][a] = np.random.uniform(low=-0.1, high=0.1)\n",
    "                    self.Q[(row, col)][a] = 0\n",
    "                    self.returns[(row, col)][a] = []\n",
    "        #print(\"inicialização de Q \", self.Q)\n",
    "        #print(\"inicialização de returns \", self.returns)\n",
    "    def choose_action(self, state):\n",
    "        if np.random.uniform(0, 1) < self.epsilon:\n",
    "            return self.env.action_space.sample() #.sample retorna uma amostra aleatório \n",
    "        else:\n",
    "            return max(self.Q[state], key=self.Q[state].get)\n",
    "    \n",
    "    def learn(self, episode):\n",
    "        G = 0\n",
    "        for state, action, reward in reversed(episode):\n",
    "            G = self.gamma * G + reward\n",
    "            # Verifique se a combinação estado-ação não aparece no episódio\n",
    "            #if not (state, action) in [(x[0], x[1]) for x in episode[:-1]]:\n",
    "            if state is not None and action is not None:    \n",
    "                self.returns[state][action].append(G)\n",
    "                self.Q[state][action] = np.mean(self.returns[state][action])\n",
    "\n",
    "    def update_epsilon(self):\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon_decay * self.epsilon)\n",
    "\n",
    "    def train(self, episodes, num_obstacles=0, render=False, max_steps=100):\n",
    "        episodes_data = {}\n",
    "        max_penalty = -50\n",
    "        \n",
    "        for episode_num in range(episodes):\n",
    "            episode = []\n",
    "            state = self.env.reset(num_obstacles)\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            step_count = 0\n",
    "            \n",
    "            while not done and step_count < max_steps:\n",
    "                action = self.choose_action(state)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                reward = -1\n",
    "                episode.append((state, action, reward))\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "                step_count += 1\n",
    "                if render:\n",
    "                    self.env.render()\n",
    "                    \n",
    "            if step_count >= max_steps and not done:\n",
    "                episode[-1] = (episode[-1][0], episode[-1][1], max_penalty)\n",
    "                total_reward += max_penalty\n",
    "\n",
    "            self.learn(episode)  # Aprendizado é feito após o término do episódio\n",
    "            self.update_epsilon()  # Atualiza epsilon após cada episódio\n",
    "            \n",
    "            episodes_data[episode_num] = {\n",
    "                'reward': total_reward,\n",
    "                'steps': step_count,\n",
    "                'path': [x[0] for x in episode]\n",
    "            }\n",
    "            \n",
    "        if render:\n",
    "            self.env.close()\n",
    "            \n",
    "        return episodes_data\n",
    "    \n",
    "    def execute_optimal_policy(self, max_steps=100, start_pos = None):\n",
    "        state = self.env.reset(num_obstacles=0, fixed_start_pos = start_pos)\n",
    "        path = [state]  # Iniciar o caminho com o estado inicial\n",
    "        self.env.render(path=path)  # Passa o caminho atual para a função render\n",
    "        step_count = 0\n",
    "        done = False\n",
    "        while not done and step_count < max_steps:\n",
    "            action = max(self.Q[state], key=self.Q[state].get)\n",
    "            next_state, reward, done, _ = self.env.step(action)\n",
    "            path.append(next_state)  # Adicionar o novo estado ao caminho\n",
    "            self.env.render(path=path)  # Renderizar novamente com o caminho atualizado\n",
    "            state = next_state\n",
    "            step_count += 1\n",
    "            time.sleep(0.2)\n",
    "            if done:\n",
    "                break\n",
    "        self.env.close()\n",
    "    \n",
    "    def showQ(self):\n",
    "        self.env.render()\n",
    "        running = True\n",
    "        while running:\n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.KEYDOWN:\n",
    "                    if event.key == pygame.K_ESCAPE:  # Tecla ESC\n",
    "                        running = False\n",
    "\n",
    "            self.env.draw_maze()\n",
    "            self.env.draw_arrows(self.Q)\n",
    "            \n",
    "            pygame.display.update()\n",
    "        self.env.close()\n",
    "        return self.Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412a965a",
   "metadata": {},
   "source": [
    "### 2.1) Primeiro experimento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5b6b1a",
   "metadata": {},
   "source": [
    "#### 2.1.1) Parâmetros\n",
    "\n",
    "**episódios** 1000\n",
    "\n",
    "**reward =** -1 quando mover corretamente e -50 quando mover em paredes ou obstáculos\n",
    "\n",
    "**alpha =** de acordo com a formula $\\alpha_{t} = 1/N_(s_t, s_a)$\n",
    "\n",
    "**gamma =** 0.3 \n",
    "\n",
    "**epsilon =**  de acordo com a formula $\\epsilon_{t} = N0/(N0+N(st))$\n",
    "\n",
    "**Ambiente =** Matriz 6 x 6 com obstáculo fixo ao centro \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18983a78",
   "metadata": {},
   "source": [
    "####  Função para execução e extração de dados para análise do algoritmo a ser testado "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "029b24ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_and_analyze_agent(agent_class, maze, episodes=1000, num_obstacles=0, \n",
    "                              epsilon_values=[0.1, 0.5, 0.6], \n",
    "                              gamma_values=[0.8, 0.9, 0.99], \n",
    "                              alpha_values=[0.1, 0.5, 0.9]):\n",
    "    # Configurações iniciais\n",
    "    env = MazeGameEnv(maze)\n",
    "    agent = agent_class(env)\n",
    "    agent_data = agent.train(episodes, render=False, num_obstacles=num_obstacles)\n",
    "\n",
    "  \n",
    "\n",
    "    # Extração e análise de dados\n",
    "    rewards, steps = extract_data(agent_data)\n",
    "    metrics = {'reward': rewards, 'steps': steps}\n",
    "\n",
    "    for metric, values in metrics.items():\n",
    "        print(f\"Max {metric} in episode {values.index(max(values))} with {metric} = {max(values)}\")\n",
    "        print(f\"Min {metric} in episode {values.index(min(values))} with {metric} = {min(values)}\")\n",
    "\n",
    "    # Execução da política ótima\n",
    "    agent.showQ()\n",
    "    \n",
    "    #agent.execute_optimal_policy(max_steps=100, start_pos=(0, 0))\n",
    "\n",
    "    # Análise da variação de parâmetros\n",
    "    #plot_parameter_variation(agent_class, env, epsilon_values, 'epsilon', episodes, num_obstacles)\n",
    "    #plot_parameter_variation(agent_class, env, gamma_values, 'gamma', episodes, num_obstacles)\n",
    "    #plot_parameter_variation(agent_class, env, alpha_values, 'alpha', episodes, num_obstacles)\n",
    "\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5ca402",
   "metadata": {},
   "source": [
    "#### Função para enviar parametros para execução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478fd61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_and_analyze_agent(MonteCarloAgent, mazes[3], episodes= 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d29ee8",
   "metadata": {},
   "source": [
    "#### Análises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e84498",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
