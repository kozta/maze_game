{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gymnasium\n",
    "!pip install numpy\n",
    "!pip install pygame\n",
    "!pip install random\n",
    "!pip install tqdm\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import pygame\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MazeGameEnv(gym.Env):\n",
    "    def __init__(self, maze, alpha=0.1, gamma=0.99, epsilon=0.1):\n",
    "        super(MazeGameEnv, self).__init__()\n",
    "        self.maze = np.array(maze)\n",
    "        self.start_pos = tuple(np.argwhere(self.maze == 'S')[0])\n",
    "        self.goal_pos = tuple(np.argwhere(self.maze == 'G')[0])\n",
    "        self.current_pos = self.start_pos\n",
    "        self.num_rows, self.num_cols = self.maze.shape\n",
    "\n",
    "        self.alpha = alpha  # Taxa de aprendizado\n",
    "        self.gamma = gamma  # Fator de desconto\n",
    "        self.epsilon = epsilon  # Probabilidade de exploração\n",
    "\n",
    "        # Define ação como Discrete com 4 ações (cima, baixo, esquerda, direita)\n",
    "        self.action_space = gym.spaces.Discrete(4)\n",
    "\n",
    "        # Defina o espaço de observação como uma tupla com o número de linhas e colunas\n",
    "        self.observation_space = gym.spaces.Tuple((gym.spaces.Discrete(self.num_rows), gym.spaces.Discrete(self.num_cols)))\n",
    "\n",
    "        # Inicialize o ambiente Pygame\n",
    "        pygame.init()\n",
    "\n",
    "        # Defina cores\n",
    "        self.WHITE = (255, 255, 255)\n",
    "        self.GREEN = (0, 255, 0)\n",
    "        self.RED = (255, 0, 0)\n",
    "        self.BLACK = (0, 0, 0)\n",
    "        self.PURPLE = (255, 0, 255)\n",
    "        self.BLUE = (0, 0, 255)\n",
    "\n",
    "    def reset(self, num_obstacles=None):\n",
    "        self.reset_obstacles(num_obstacles)\n",
    "        row, col = np.random.randint(0, self.num_rows), np.random.randint(0, self.num_cols)\n",
    "        while self.maze[row, col] in ['#', 'G']:  # Garante que a posição inicial não é um obstáculo ou a posição final\n",
    "            row, col = np.random.randint(0, self.num_rows), np.random.randint(0, self.num_cols)\n",
    "        self.maze[self.start_pos] = '.'  # Limpa a posição inicial antiga\n",
    "        self.start_pos = (row, col)\n",
    "        self.maze[row, col] = 'S'  # Define a nova posição inicial\n",
    "        self.current_pos = self.start_pos\n",
    "        return self.current_pos\n",
    "    \n",
    "    def reset_obstacles(self, num_obstacles = 0):\n",
    "        for row in range(self.num_rows):\n",
    "            for col in range(self.num_cols):\n",
    "                if self.maze[row, col] == 'R':\n",
    "                    self.maze[row, col] = '.'\n",
    "        \n",
    "        for _ in range(num_obstacles):\n",
    "            row, col = np.random.randint(0, self.num_rows), np.random.randint(0, self.num_cols)\n",
    "            while self.maze[row, col] in ['S', 'G', '#','R']:\n",
    "                row, col = np.random.randint(0, self.num_rows), np.random.randint(0, self.num_cols)\n",
    "            self.maze[row, col] = 'R'\n",
    "\n",
    "    def step(self, action, reward = 1):\n",
    "        if action == 0:  # Cima\n",
    "            self.move('up')\n",
    "        elif action == 1:  # Baixo\n",
    "            self.move('down')\n",
    "        elif action == 2:  # Esquerda\n",
    "            self.move('left')\n",
    "        elif action == 3:  # Direita\n",
    "            self.move('right')\n",
    "\n",
    "        obs = self.current_pos\n",
    "        #\n",
    "        # reward = 1 if self.current_pos == self.goal_pos else 0\n",
    "        # distance_to_goal = abs(self.current_pos[0] - self.goal_pos[0]) + abs(self.current_pos[1] - self.goal_pos[1])\n",
    "        # reward = 1 / (distance_to_goal + 1)\n",
    "        # done = self.current_pos == self.goal_pos\n",
    "        #\n",
    "        if self.current_pos == self.goal_pos:\n",
    "            reward = 10\n",
    "            done = True\n",
    "        else:\n",
    "            distance_to_goal = abs(self.current_pos[0] - self.goal_pos[0]) + abs(self.current_pos[1] - self.goal_pos[1])\n",
    "            reward = -distance_to_goal\n",
    "            done = False\n",
    "            \n",
    "        info = {}\n",
    "\n",
    "        return obs, reward, done, info\n",
    "    \n",
    "    def move(self, action):\n",
    "        new_pos = list(self.current_pos)\n",
    "\n",
    "        if action == 'up':\n",
    "            new_pos[0] -= 1\n",
    "        elif action == 'down':\n",
    "            new_pos[0] += 1\n",
    "        elif action == 'left':\n",
    "            new_pos[1] -= 1\n",
    "        elif action == 'right':\n",
    "            new_pos[1] += 1\n",
    "\n",
    "        new_pos = tuple(new_pos)\n",
    "\n",
    "        if self.is_valid_position(new_pos[0], new_pos[1]):\n",
    "            self.current_pos = new_pos\n",
    "\n",
    "    def is_valid_position(self, row, col):\n",
    "        return 0 <= row < self.num_rows and 0 <= col < self.num_cols and self.maze[row, col] != '#' and self.maze[row, col] != 'R'\n",
    "\n",
    "\n",
    "    def render(self, mode='human', path=None):\n",
    "        if mode == 'human':\n",
    "            self.draw_maze(path)\n",
    "            pygame.display.update()\n",
    "            \n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.QUIT:\n",
    "                    pygame.quit()\n",
    "        \n",
    "        if mode == 'blank':\n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.QUIT:\n",
    "                    pygame.quit()\n",
    "\n",
    "    def play_path(self, path, delay=1):\n",
    "        for position in path:\n",
    "            self.current_pos = position\n",
    "            self.render(mode='human', path=path)\n",
    "            time.sleep(delay)\n",
    "\n",
    "    def draw_maze(self, path = None):\n",
    "        self.cell_size = 50\n",
    "        self.screen_width = self.num_cols * self.cell_size\n",
    "        self.screen_height = self.num_rows * self.cell_size\n",
    "        self.screen = pygame.display.set_mode((self.screen_width, self.screen_height))\n",
    "        self.screen.fill(self.WHITE)\n",
    "        \n",
    "        for row in range(self.num_rows):\n",
    "            for col in range(self.num_cols):\n",
    "                cell_left = col * self.cell_size\n",
    "                cell_top = row * self.cell_size\n",
    "\n",
    "                if self.maze[row, col] == '#' or self.maze[row, col] == 'R':\n",
    "                    pygame.draw.rect(self.screen, self.BLACK, (cell_left, cell_top, self.cell_size, self.cell_size))\n",
    "                elif self.maze[row, col] == 'S':\n",
    "                    pygame.draw.rect(self.screen, self.GREEN, (cell_left, cell_top, self.cell_size, self.cell_size))\n",
    "                elif self.maze[row, col] == 'G':\n",
    "                    pygame.draw.rect(self.screen, self.RED, (cell_left, cell_top, self.cell_size, self.cell_size))\n",
    "                    \n",
    "        if path:\n",
    "            for i in range(len(path)-1):\n",
    "                start = path[i]\n",
    "                end = path[i+1]\n",
    "                self.draw_arrow(start, end)\n",
    "\n",
    "        current_row, current_col = self.current_pos\n",
    "        pygame.draw.rect(self.screen, self.PURPLE, (current_col * self.cell_size, current_row * self.cell_size, self.cell_size, self.cell_size))\n",
    "\n",
    "    def draw_arrow(self, start, end):\n",
    "        start_x = start[1] * self.cell_size + self.cell_size // 2\n",
    "        start_y = start[0] * self.cell_size + self.cell_size // 2\n",
    "        end_x = end[1] * self.cell_size + self.cell_size // 2\n",
    "        end_y = end[0] * self.cell_size + self.cell_size // 2\n",
    "\n",
    "        pygame.draw.line(self.screen, self.BLUE, (start_x, start_y), (end_x, end_y), 2)\n",
    "\n",
    "    def close(self):\n",
    "        pygame.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze = [\n",
    "    ['S', '.', '.', '.', '.', '.', '.', '.', '.', '.'],\n",
    "    ['.', '.', '.', '.', '#', '.', '#', '.', '.', '.'],\n",
    "    ['.', '.', '.', '.', '#', '.', '.', '.', '.', '.'],\n",
    "    ['.', '.', '.', '.', '#', '.', '.', '.', '#', '.'],\n",
    "    ['.', '.', '.', '.', '#', '.', '.', '.', '.', '.'],\n",
    "    ['.', '#', '#', '#', '#', '.', '.', '.', '.', '.'],\n",
    "    ['.', '.', '.', '.', '.', '.', '.', '.', '.', '.'],\n",
    "    ['.', '.', '.', '.', '.', '#', '.', '.', '.', '.'],\n",
    "    ['.', '.', '.', '.', '.', '.', '.', '.', '.', '.'],\n",
    "    ['.', '#', '.', '.', '.', '.', '.', '.', '.', 'G'],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze = [\n",
    "    ['S','.','.','.','.','.'],\n",
    "    ['.','.','.','#','.','.'],\n",
    "    ['.','.','.','#','.','.'],\n",
    "    ['.','.','.','#','.','.'],\n",
    "    ['.','#','#','#','.','.'],\n",
    "    ['.','.','.','.','.','G']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze = [\n",
    "    ['S','.','#','.','#','.'],\n",
    "    ['.','.','.','.','#','.'],\n",
    "    ['.','#','#','.','#','.'],\n",
    "    ['.','#','#','.','#','.'],\n",
    "    ['.','.','.','.','#','.'],\n",
    "    ['#','.','.','.','.','G']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze = [\n",
    "    ['S','.','.','.','.','.'],\n",
    "    ['.','.','.','.','.','.'],\n",
    "    ['.','.','#','#','.','.'],\n",
    "    ['.','.','#','#','.','.'],\n",
    "    ['.','.','.','.','.','.'],\n",
    "    ['.','.','.','.','.','G']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MazeGameEnv(maze)\n",
    "\n",
    "done = False\n",
    "state = env.reset()\n",
    "env.render()\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()  # Exemplo: selecione uma ação aleatória\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    print(state, reward, done)\n",
    "    env.render()\n",
    "    pygame.time.delay(100)\n",
    "\n",
    "print(\"Done!\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonteCarlo:\n",
    "    def __init__(self, env, learning_rate=0.1, discount_factor=0.99, exploration_prob=0.1):\n",
    "        self.env = env\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_prob = exploration_prob\n",
    "        \n",
    "        self.Q_table = {}\n",
    "        for row in range(env.num_rows):\n",
    "            for col in range(env.num_cols):\n",
    "                state = (row, col)    #destino\n",
    "                self.Q_table[state] = np.zeros(env.action_space.n)\n",
    "    \n",
    "        self.mc_acoe = [0,1,2,3]\n",
    "        self.mc_reward = -1\n",
    "        self.VI = np.zeros((env.num_cols, env.num_rows))\n",
    "        self.otimo = []\n",
    "        self.converg = 80\n",
    "        \n",
    "        self.dest = []\n",
    "        self.dest.append(list(state))\n",
    "        self.gamm_arr = {(x, y):list() for x in range(env.num_cols) for y in range(env.num_rows)}\n",
    "        self.vabs = {(x, y):list() for x in range(env.num_cols) for y in range(env.num_rows)}\n",
    "        self.sta = [[x, y] for x in range(env.num_cols) for y in range(env.num_rows)] \n",
    "        \n",
    "    def epocas(self):\n",
    "        begin_state = env.start_pos  \n",
    "        epocas = []\n",
    "        while True:\n",
    "            # Se estado igual destino encerra a epoca\n",
    "            if list(begin_state) in self.dest:\n",
    "                self.env.reset()\n",
    "                return epocas\n",
    "            # Seleciona de forma randomica uma ação (cima , esquerda, ...   )\n",
    "            action = random.choice(self.mc_acoe)\n",
    "            \n",
    "            # Retorna os novos estados que podem ser utilizados\n",
    "            new_state, _, _, _ = self.env.step(action, 0)  # valida se tem parede ou borda\n",
    "\n",
    "            # Cria lista das epocas\n",
    "            epocas.append([list(begin_state), action, self.mc_reward, list(new_state)])\n",
    "\n",
    "            begin_state = new_state\n",
    "        \n",
    "    def MC(self, mc_interactions):\n",
    "        conv_epoc = []\n",
    "        for z in tqdm(range(mc_interactions)):\n",
    "            epoc = self.epocas()\n",
    "            ga = 0\n",
    "            vi_ant = None\n",
    "            count = 0\n",
    "            for i, passo in enumerate(epoc[::-1]):\n",
    "                ga = env.gamma*ga + passo[2]\n",
    "                # Começa pelas epocas proximas ao destino\n",
    "                if passo[0] not in [x[0] for x in epoc[::-1][len(epoc)-i:]]:\n",
    "                    # Pega posicao que vai ser calculada\n",
    "                    pos = (passo[0][0], passo[0][1])\n",
    "                    # Monta uma lista de gamma para posição  \n",
    "                    self.gamm_arr[pos].append(ga)\n",
    "                    # Calcula a media de todos os gammas da posicao\n",
    "                    newValue = np.average(self.gamm_arr[pos])\n",
    "                    # Monta uma lista na posição com o calculo do valor absoluto da diferença entre as posições - antiga e nova\n",
    "                    self.vabs[pos[0], pos[1]].append(np.abs(self.VI[pos[0], pos[1]]-newValue))\n",
    "                    # lista com os valores de interacao \n",
    "                    self.VI[pos[0], pos[1]] = newValue\n",
    "                    \n",
    "            if z >= self.converg:  # convergencia de acordo com o grafico\n",
    "                conv_epoc.append(epoc)\n",
    "            \n",
    "            if z in [100,200,300, 400, mc_interactions-1]:\n",
    "                print(\"Iteration {}\".format(z+1))\n",
    "                print(self.VI)\n",
    "                print(\"\")\n",
    "                        \n",
    "            vabs_ant = self.vabs\n",
    "        \n",
    "        plt.figure(figsize=(20,10))\n",
    "        all_series = [list(x)[:100] for x in self.vabs.values()]\n",
    "        for series in all_series:\n",
    "            plt.plot(series)\n",
    "            \n",
    "        list(map(len, conv_epoc))        \n",
    "    \n",
    "        def get_line_len(conv_epoc):\n",
    "            return len(conv_epoc),conv_epoc\n",
    "        \n",
    "        list_converg = sorted(list(map(get_line_len, conv_epoc)))\n",
    "\n",
    "        print (f'O menos número de passos atingidos em {mc_interactions} interações foram : {list_converg[0][0]}')\n",
    "        \n",
    "        print (f'Ocorreu na epoca  {(self.converg + list_converg[0][0])} e é considerado como o resultado ótimo para este treinamento')\n",
    "        \n",
    "        steps = list_converg[0][1]\n",
    "        \n",
    "        bob_volta_casa = []\n",
    "        \n",
    "        for item in steps:\n",
    "            bob_volta_casa.append((item[3][0], item[3][1]))\n",
    "        \n",
    "        return bob_volta_casa\n",
    "            \n",
    "    def find_optimal_path(self):\n",
    "        state = env.start_pos\n",
    "        optimal_path = [state]\n",
    "\n",
    "        while state != env.goal_pos:\n",
    "            action = np.argmax(self.Q_table[state])\n",
    "            env.move(action)\n",
    "            state = env.current_pos\n",
    "            optimal_path.append(state)\n",
    "\n",
    "        print(self.VI)\n",
    "        return optimal_path\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MazeGameEnv(maze, alpha=0.1, gamma=0.6, epsilon=0.1)\n",
    "conf = MonteCarlo(env)\n",
    "optimal_path = conf.MC(500)\n",
    "\n",
    "# # Encontre o caminho ótimo do estado inicial ao estado de destino\n",
    "print(\"Optimal Path:\", optimal_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self, env, alpha=0.1, gamma=0.99, epsilon=0.1):\n",
    "        self.env = env\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        self.Q = {}\n",
    "        for row in range(env.num_rows):\n",
    "            for col in range(env.num_cols):\n",
    "                self.Q[(row, col)] = {}\n",
    "                for a in range(env.action_space.n):\n",
    "                    self.Q[(row, col)][a] = 0\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        if np.random.uniform(0, 1) < self.epsilon:\n",
    "            return self.env.action_space.sample()  # Explore\n",
    "        else:\n",
    "            return max(self.Q[state], key=self.Q[state].get)  # Exploit\n",
    "    \n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        predict = self.Q[state][action]\n",
    "        target = reward\n",
    "        if next_state:\n",
    "            target = reward + self.gamma * max(self.Q[next_state].values())\n",
    "        \n",
    "        self.Q[state][action] += self.alpha * (target - predict)\n",
    "    \n",
    "    def train(self, episodes, num_obstacles=0, max_steps=10000):\n",
    "        for episode in range(episodes):\n",
    "            path = []  # Lista para armazenar o caminho do episódio atual\n",
    "            state = self.env.reset(num_obstacles)\n",
    "            path.append(state)\n",
    "            done = False\n",
    "            step_count = 0  # Contador de passos\n",
    "            while not done:\n",
    "                if step_count >= max_steps:  # Verifique o limite de passos\n",
    "                    break\n",
    "                action = self.choose_action(state)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                path.append(next_state)  # Adicione o estado ao caminho\n",
    "                self.learn(state, action, reward, None if done else next_state)\n",
    "                state = next_state\n",
    "                self.env.render()\n",
    "                step_count += 1  # Incremente o contador de passos\n",
    "            self.last_path = path  # Armazene o caminho do último episódio\n",
    "        self.env.close()\n",
    "\n",
    "            \n",
    "    def get_last_path(self):\n",
    "        return self.last_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MazeGameEnv(maze)\n",
    "agent = QLearningAgent(env)\n",
    "agent.train(1000)\n",
    "\n",
    "last_path = agent.get_last_path()\n",
    "env.play_path(last_path)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MazeGameEnv(maze)\n",
    "agent = QLearningAgent(env)\n",
    "agent.train(1000, 2)\n",
    "\n",
    "last_path = agent.get_last_path()\n",
    "env.play_path(last_path)\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
